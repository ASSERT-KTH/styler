{
    "project_name": "IQSS-dataverse",
    "error_id": "125",
    "information": {
        "errors": [
            {
                "line": "361",
                "column": "1",
                "severity": "error",
                "message": "File contains tab characters (this is the first instance).",
                "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.FileTabCharacterCheck"
            }
        ]
    },
    "source_code": "        fmd.setDatasetVersion(dataset.getLatestVersion());\n        \n\tString isFilePIDsEnabled = commandEngine.getContext().settings().getValueForKey(SettingsServiceBean.Key.FilePIDsEnabled, \"true\"); //default value for file PIDs is 'true'\n\tif (\"true\".contentEquals( isFilePIDsEnabled )) {\n\t\n        GlobalIdServiceBean idServiceBean = GlobalIdServiceBean.getBean(packageFile.getProtocol(), commandEngine.getContext());",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "361",
                    "column": "1",
                    "severity": "error",
                    "message": "File contains tab characters (this is the first instance).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.FileTabCharacterCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/125/FileRecordWriter.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler/125/FileRecordWriter.java\nindex c82a5bb01eb..fa283f73c53 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/125/FileRecordWriter.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler/125/FileRecordWriter.java\n@@ -357,8 +357,7 @@ public class FileRecordWriter extends AbstractItemWriter {\n         \n         dataset.getLatestVersion().getFileMetadatas().add(fmd);\n         fmd.setDatasetVersion(dataset.getLatestVersion());\n-        \n-\tString isFilePIDsEnabled = commandEngine.getContext().settings().getValueForKey(SettingsServiceBean.Key.FilePIDsEnabled, \"true\"); //default value for file PIDs is 'true'\n+        String isFilePIDsEnabled = commandEngine.getContext().settings().getValueForKey(SettingsServiceBean.Key.FilePIDsEnabled, \"true\"); //default value for file PIDs is 'true'\n \tif (\"true\".contentEquals( isFilePIDsEnabled )) {\n \t\n         GlobalIdServiceBean idServiceBean = GlobalIdServiceBean.getBean(packageFile.getProtocol(), commandEngine.getContext());\n",
            "diff_size": 2
        },
        {
            "tool": "intellij",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/125/FileRecordWriter.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/intellij/125/FileRecordWriter.java\nindex c82a5bb01eb..557bc1c2414 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/125/FileRecordWriter.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/intellij/125/FileRecordWriter.java\n@@ -63,412 +63,429 @@ import edu.harvard.iq.dataverse.GlobalIdServiceBean;\n @Named\n @Dependent\n public class FileRecordWriter extends AbstractItemWriter {\n-    \n-    @Inject\n-    JobContext jobContext;\n-\n-    @Inject\n-    StepContext stepContext;\n-\n-    @Inject\n-    @BatchProperty\n-    String checksumType;\n-    \n-    @Inject\n-    @BatchProperty\n-    String checksumManifest;\n-\n-    @EJB\n-    DatasetServiceBean datasetServiceBean;\n-\n-    @EJB\n-    AuthenticationServiceBean authenticationServiceBean;\n-    \n-    @EJB\n-    SettingsServiceBean settingsService;\n-    \n-    @EJB\n-    DataFileServiceBean dataFileServiceBean;\n-\n-    @EJB\n-    EjbDataverseEngine commandEngine;\n-\n-    Dataset dataset;\n-    AuthenticatedUser user;\n-    int fileCount;\n-    String fileMode; \n-    Long suppliedSize = null;\n-    String uploadFolder; \n-\n-    public static String FILE_MODE_INDIVIDUAL_FILES = \"individual_files\";\n-    public static String FILE_MODE_PACKAGE_FILE = \"package_file\";\n-    \n-    @PostConstruct\n-    public void init() {\n-        JobOperator jobOperator = BatchRuntime.getJobOperator();\n-        Properties jobParams = jobOperator.getParameters(jobContext.getInstanceId());\n-        dataset = datasetServiceBean.find(Long.parseLong(jobParams.getProperty(\"datasetId\")));\n-        user = authenticationServiceBean.getAuthenticatedUser(jobParams.getProperty(\"userId\"));\n-        //jobLogger = Logger.getLogger(\"job-\"+Long.toString(jobContext.getInstanceId()));\n-        fileCount = ((Map<String, String>) jobContext.getTransientUserData()).size();\n-        fileMode = jobParams.getProperty(\"fileMode\");\n-        uploadFolder = jobParams.getProperty(\"uploadFolder\");\n-        if (jobParams.getProperty(\"totalSize\") != null) {\n-            try { \n-                suppliedSize = new Long(jobParams.getProperty(\"totalSize\"));\n-                getJobLogger().log(Level.INFO, \"Size parameter supplied: \"+suppliedSize);\n-            } catch (NumberFormatException ex) {\n-                getJobLogger().log(Level.WARNING, \"Invalid file size supplied (in FileRecordWriter.init()): \"+jobParams.getProperty(\"totalSize\"));\n-                suppliedSize = null; \n+\n+  @Inject\n+  JobContext jobContext;\n+\n+  @Inject\n+  StepContext stepContext;\n+\n+  @Inject\n+  @BatchProperty\n+  String checksumType;\n+\n+  @Inject\n+  @BatchProperty\n+  String checksumManifest;\n+\n+  @EJB\n+  DatasetServiceBean datasetServiceBean;\n+\n+  @EJB\n+  AuthenticationServiceBean authenticationServiceBean;\n+\n+  @EJB\n+  SettingsServiceBean settingsService;\n+\n+  @EJB\n+  DataFileServiceBean dataFileServiceBean;\n+\n+  @EJB\n+  EjbDataverseEngine commandEngine;\n+\n+  Dataset dataset;\n+  AuthenticatedUser user;\n+  int fileCount;\n+  String fileMode;\n+  Long suppliedSize = null;\n+  String uploadFolder;\n+\n+  public static String FILE_MODE_INDIVIDUAL_FILES = \"individual_files\";\n+  public static String FILE_MODE_PACKAGE_FILE = \"package_file\";\n+\n+  @PostConstruct\n+  public void init() {\n+    JobOperator jobOperator = BatchRuntime.getJobOperator();\n+    Properties jobParams = jobOperator.getParameters(jobContext.getInstanceId());\n+    dataset = datasetServiceBean.find(Long.parseLong(jobParams.getProperty(\"datasetId\")));\n+    user = authenticationServiceBean.getAuthenticatedUser(jobParams.getProperty(\"userId\"));\n+    //jobLogger = Logger.getLogger(\"job-\"+Long.toString(jobContext.getInstanceId()));\n+    fileCount = ((Map<String, String>) jobContext.getTransientUserData()).size();\n+    fileMode = jobParams.getProperty(\"fileMode\");\n+    uploadFolder = jobParams.getProperty(\"uploadFolder\");\n+    if (jobParams.getProperty(\"totalSize\") != null) {\n+      try {\n+        suppliedSize = new Long(jobParams.getProperty(\"totalSize\"));\n+        getJobLogger().log(Level.INFO, \"Size parameter supplied: \" + suppliedSize);\n+      } catch (NumberFormatException ex) {\n+        getJobLogger().log(Level.WARNING,\n+          \"Invalid file size supplied (in FileRecordWriter.init()): \" + jobParams.getProperty(\"totalSize\"));\n+        suppliedSize = null;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void open(Serializable checkpoint) throws Exception {\n+    // no-op\n+  }\n+\n+  @Override\n+  public void close() {\n+    // no-op\n+  }\n+\n+  @Override\n+  public void writeItems(List list) {\n+    if (!list.isEmpty()) {\n+      if (FILE_MODE_INDIVIDUAL_FILES.equals(fileMode)) {\n+        List<DataFile> datafiles = dataset.getFiles();\n+        for (Object file : list) {\n+          DataFile df = createDataFile((File) file);\n+          if (df != null) {\n+            // log success if the dataset isn't huge\n+            if (fileCount < 20000) {\n+              getJobLogger().log(Level.INFO, \"Creating DataFile for: \" + ((File) file).getAbsolutePath());\n             }\n+            datafiles.add(df);\n+          } else {\n+            getJobLogger().log(Level.SEVERE, \"Unable to create DataFile for: \" + ((File) file).getAbsolutePath());\n+          }\n+        }\n+        dataset.getLatestVersion().getDataset().setFiles(datafiles);\n+      } else if (FILE_MODE_PACKAGE_FILE.equals(fileMode)) {\n+        DataFile packageFile = createPackageDataFile(list);\n+        if (packageFile == null) {\n+          getJobLogger().log(Level.SEVERE, \"File package import failed.\");\n+          jobContext.setExitStatus(\"FAILED\");\n+          return;\n+        }\n+        DatasetLock dcmLock = dataset.getLockFor(DatasetLock.Reason.DcmUpload);\n+        if (dcmLock == null) {\n+          getJobLogger().log(Level.WARNING, \"Dataset not locked for DCM upload\");\n+        } else {\n+          datasetServiceBean.removeDatasetLocks(dataset, DatasetLock.Reason.DcmUpload);\n+          dataset.removeLock(dcmLock);\n         }\n+        updateDatasetVersion(dataset.getLatestVersion());\n+      } else {\n+        getJobLogger().log(Level.SEVERE, \"File mode \" + fileMode + \" is not supported.\");\n+        jobContext.setExitStatus(\"FAILED\");\n+      }\n+    } else {\n+      getJobLogger().log(Level.SEVERE, \"No items in the writeItems list.\");\n     }\n-    \n-    @Override\n-    public void open(Serializable checkpoint) throws Exception {\n-        // no-op   \n+  }\n+\n+  // utils\n+\n+  /**\n+   * Update the dataset version using the command engine so permissions and constraints are enforced.\n+   * Log errors to both the glassfish log and in the job context, as the exit status \"failed\".\n+   *\n+   * @param version dataset version\n+   */\n+  private void updateDatasetVersion(DatasetVersion version) {\n+\n+    // update version using the command engine to enforce user permissions and constraints\n+    if (dataset.getVersions().size() == 1 && version.getVersionState() == DatasetVersion.VersionState.DRAFT) {\n+      try {\n+        Command<Dataset> cmd;\n+        cmd =\n+          new UpdateDatasetVersionCommand(version.getDataset(), new DataverseRequest(user, (HttpServletRequest) null));\n+        commandEngine.submit(cmd);\n+      } catch (CommandException ex) {\n+        String commandError = \"CommandException updating DatasetVersion from batch job: \" + ex.getMessage();\n+        getJobLogger().log(Level.SEVERE, commandError);\n+        jobContext.setExitStatus(\"FAILED\");\n+      }\n+    } else {\n+      String constraintError = \"ConstraintException updating DatasetVersion form batch job: dataset must be a \"\n+        + \"single version in draft mode.\";\n+      getJobLogger().log(Level.SEVERE, constraintError);\n+      jobContext.setExitStatus(\"FAILED\");\n     }\n \n-    @Override\n-    public void close() {\n-        // no-op\n+  }\n+\n+  /**\n+   * Import the supplied batch of files as a single \"package file\" DataFile\n+   * (basically, a folder/directory, with the single associated DataFile/FileMetadata, etc.)\n+   * and add it to the\n+   * latest dataset version\n+   *\n+   * @param files list of files, already copied to the dataset directory by rsync or otherwise.\n+   * @return datafile\n+   * <p>\n+   * Consider:\n+   * instead of expecting to have an extra top-level directory/folder to be\n+   * present already, generate it here (using the standard code used for generating\n+   * storage identifiers for \"normal\" files), create it as a directory, and move\n+   * all the supplied files there.l\n+   */\n+  private DataFile createPackageDataFile(List<File> files) {\n+    DataFile packageFile = new DataFile(DataFileServiceBean.MIME_TYPE_PACKAGE_FILE);\n+    FileUtil.generateStorageIdentifier(packageFile);\n+\n+    String datasetDirectory = null;\n+    String folderName = null;\n+\n+    long totalSize;\n+\n+    if (suppliedSize != null) {\n+      totalSize = suppliedSize;\n+    } else {\n+      totalSize = 0L;\n     }\n \n-    @Override\n-    public void writeItems(List list) {\n-        if (!list.isEmpty()) {\n-            if (FILE_MODE_INDIVIDUAL_FILES.equals(fileMode)) {\n-                List<DataFile> datafiles = dataset.getFiles();\n-                for (Object file : list) {\n-                    DataFile df = createDataFile((File) file);\n-                    if (df != null) {\n-                        // log success if the dataset isn't huge\n-                        if (fileCount < 20000) {\n-                            getJobLogger().log(Level.INFO, \"Creating DataFile for: \" + ((File) file).getAbsolutePath());\n-                        }\n-                        datafiles.add(df);\n-                    } else {\n-                        getJobLogger().log(Level.SEVERE, \"Unable to create DataFile for: \" + ((File) file).getAbsolutePath());\n-                    }\n-                }\n-                dataset.getLatestVersion().getDataset().setFiles(datafiles);\n-            } else if (FILE_MODE_PACKAGE_FILE.equals(fileMode)) {\n-                DataFile packageFile = createPackageDataFile(list);\n-                if (packageFile == null) {\n-                    getJobLogger().log(Level.SEVERE, \"File package import failed.\");\n-                    jobContext.setExitStatus(\"FAILED\");\n-                    return;\n-                }\n-                DatasetLock dcmLock = dataset.getLockFor(DatasetLock.Reason.DcmUpload);\n-                if (dcmLock == null) {\n-                    getJobLogger().log(Level.WARNING, \"Dataset not locked for DCM upload\");\n-                } else {\n-                    datasetServiceBean.removeDatasetLocks(dataset, DatasetLock.Reason.DcmUpload);\n-                    dataset.removeLock(dcmLock);\n-                }\n-                updateDatasetVersion(dataset.getLatestVersion());\n-            } else {\n-                getJobLogger().log(Level.SEVERE, \"File mode \"+fileMode+\" is not supported.\");\n-                jobContext.setExitStatus(\"FAILED\");\n-            }\n-        } else {\n-            getJobLogger().log(Level.SEVERE, \"No items in the writeItems list.\");\n-        }\n+    String gid = dataset.getAuthority() + \"/\" + dataset.getIdentifier();\n+\n+    packageFile.setChecksumType(DataFile.ChecksumType.SHA1); // initial default\n+\n+    // check system property first, otherwise use the batch job property:\n+    String jobChecksumType;\n+    if (System.getProperty(\"checksumType\") != null) {\n+      jobChecksumType = System.getProperty(\"checksumType\");\n+    } else {\n+      jobChecksumType = checksumType;\n     }\n-    \n-    // utils\n-    /**\n-     * Update the dataset version using the command engine so permissions and constraints are enforced.\n-     * Log errors to both the glassfish log and in the job context, as the exit status \"failed\". \n-     * \n-     * @param version dataset version\n-     *        \n-     */\n-    private void updateDatasetVersion(DatasetVersion version) {\n-    \n-        // update version using the command engine to enforce user permissions and constraints\n-        if (dataset.getVersions().size() == 1 && version.getVersionState() == DatasetVersion.VersionState.DRAFT) {\n-            try {\n-                Command<Dataset> cmd;\n-                cmd = new UpdateDatasetVersionCommand(version.getDataset(), new DataverseRequest(user, (HttpServletRequest) null));\n-                commandEngine.submit(cmd);\n-            } catch (CommandException ex) {\n-                String commandError = \"CommandException updating DatasetVersion from batch job: \" + ex.getMessage();\n-                getJobLogger().log(Level.SEVERE, commandError);\n-                jobContext.setExitStatus(\"FAILED\");\n-            }\n-        } else {\n-            String constraintError = \"ConstraintException updating DatasetVersion form batch job: dataset must be a \"\n-                    + \"single version in draft mode.\";\n-            getJobLogger().log(Level.SEVERE, constraintError);\n-            jobContext.setExitStatus(\"FAILED\");\n-        }\n-       \n+\n+    for (DataFile.ChecksumType type : DataFile.ChecksumType.values()) {\n+      if (jobChecksumType.equalsIgnoreCase(type.name())) {\n+        packageFile.setChecksumType(type);\n+        break;\n+      }\n     }\n-    \n-    /**\n-     * Import the supplied batch of files as a single \"package file\" DataFile \n-     * (basically, a folder/directory, with the single associated DataFile/FileMetadata, etc.)\n-     * and add it to the\n-     * latest dataset version \n-     * @param files list of files, already copied to the dataset directory by rsync or otherwise. \n-     * @return datafile\n-     * \n-     * Consider: \n-     * instead of expecting to have an extra top-level directory/folder to be \n-     * present already, generate it here (using the standard code used for generating\n-     * storage identifiers for \"normal\" files), create it as a directory, and move\n-     * all the supplied files there.l\n-     */\n-    private DataFile createPackageDataFile(List<File> files) {\n-        DataFile packageFile = new DataFile(DataFileServiceBean.MIME_TYPE_PACKAGE_FILE);\n-        FileUtil.generateStorageIdentifier(packageFile);\n-        \n-        String datasetDirectory = null;\n-        String folderName = null; \n-        \n-        long totalSize;\n-\n-        if (suppliedSize != null) {\n-            totalSize = suppliedSize;\n-        } else {\n-            totalSize = 0L;\n-        }\n-        \n-        String gid = dataset.getAuthority() + \"/\" + dataset.getIdentifier();\n-        \n-        packageFile.setChecksumType(DataFile.ChecksumType.SHA1); // initial default\n-\n-        // check system property first, otherwise use the batch job property:\n-        String jobChecksumType;\n-        if (System.getProperty(\"checksumType\") != null) {\n-            jobChecksumType = System.getProperty(\"checksumType\");\n+\n+    for (File file : files) {\n+      String path = file.getAbsolutePath();\n+      String relativePath = path.substring(path.indexOf(gid) + gid.length() + 1);\n+\n+      // All the files have been moved into the same final destination folder by now; so\n+      // the folderName and datasetDirectory need to be initialized only once:\n+      if (datasetDirectory == null && folderName == null) {\n+        datasetDirectory = path.substring(0, path.indexOf(gid) + gid.length() + 1);\n+        if (relativePath != null && relativePath.indexOf(File.separatorChar) > -1) {\n+          folderName = relativePath.substring(0, relativePath.indexOf(File.separatorChar));\n         } else {\n-            jobChecksumType = checksumType;\n+          getJobLogger().log(Level.SEVERE, \"Invalid file package (files are not in a folder)\");\n+          jobContext.setExitStatus(\"FAILED\");\n+          return null;\n         }\n-\n-        for (DataFile.ChecksumType type : DataFile.ChecksumType.values()) {\n-            if (jobChecksumType.equalsIgnoreCase(type.name())) {\n-                packageFile.setChecksumType(type);\n-                break;\n-            }\n+        if (!uploadFolder.equals(folderName)) {\n+          getJobLogger()\n+            .log(Level.SEVERE, \"Folder name mismatch: \" + uploadFolder + \" expected, \" + folderName + \" found.\");\n+          jobContext.setExitStatus(\"FAILED\");\n+          return null;\n         }\n+      }\n \n-        for (File file : files) {\n-            String path = file.getAbsolutePath();\n-            String relativePath = path.substring(path.indexOf(gid) + gid.length() + 1);\n-            \n-            // All the files have been moved into the same final destination folder by now; so \n-            // the folderName and datasetDirectory need to be initialized only once: \n-            if (datasetDirectory == null && folderName == null) {\n-                datasetDirectory = path.substring(0, path.indexOf(gid) + gid.length() + 1);\n-                if (relativePath != null && relativePath.indexOf(File.separatorChar) > -1) {\n-                    folderName = relativePath.substring(0, relativePath.indexOf(File.separatorChar));\n-                } else {\n-                    getJobLogger().log(Level.SEVERE, \"Invalid file package (files are not in a folder)\");\n-                    jobContext.setExitStatus(\"FAILED\");\n-                    return null;\n-                }\n-                if (!uploadFolder.equals(folderName)) {\n-                    getJobLogger().log(Level.SEVERE, \"Folder name mismatch: \"+uploadFolder+\" expected, \"+folderName+\" found.\");\n-                    jobContext.setExitStatus(\"FAILED\");\n-                    return null;\n-                }\n-            }\n+      if (suppliedSize == null) {\n+        totalSize += file.length();\n+      }\n \n-            if (suppliedSize == null) {\n-                totalSize += file.length();\n-            }\n+      String checksumValue;\n \n-            String checksumValue;\n-\n-            // lookup the checksum value in the job's manifest hashmap\n-            if (jobContext.getTransientUserData() != null) {\n-                String manifestPath = relativePath.substring(folderName.length() + 1);\n-                checksumValue = ((Map<String, String>) jobContext.getTransientUserData()).get(manifestPath);\n-                if (checksumValue != null) {\n-                    // remove the key, so we can check for unused checksums when the job is complete\n-                    ((Map<String, String>) jobContext.getTransientUserData()).remove(manifestPath);\n-\n-                } else {\n-                    getJobLogger().log(Level.WARNING, \"Unable to find checksum in manifest for: \" + file.getAbsolutePath());\n-                }\n-            } else {\n-                getJobLogger().log(Level.SEVERE, \"No checksum hashmap found in transientUserData\");\n-                jobContext.setExitStatus(\"FAILED\");\n-                return null;\n-            }\n+      // lookup the checksum value in the job's manifest hashmap\n+      if (jobContext.getTransientUserData() != null) {\n+        String manifestPath = relativePath.substring(folderName.length() + 1);\n+        checksumValue = ((Map<String, String>) jobContext.getTransientUserData()).get(manifestPath);\n+        if (checksumValue != null) {\n+          // remove the key, so we can check for unused checksums when the job is complete\n+          ((Map<String, String>) jobContext.getTransientUserData()).remove(manifestPath);\n \n-        }\n-        \n-        // If the manifest file is present, calculate the checksum of the manifest \n-        // and use it as the checksum of the datafile: \n-        \n-        if (System.getProperty(\"checksumManifest\") != null) {\n-            checksumManifest = System.getProperty(\"checksumManifest\");\n-        }\n-        \n-        File checksumManifestFile = null; \n-        if (checksumManifest != null && !checksumManifest.isEmpty()) {\n-            String checksumManifestPath = datasetDirectory + File.separator + folderName + File.separator + checksumManifest;\n-            checksumManifestFile = new File (checksumManifestPath);\n-        \n-            if (!checksumManifestFile.exists()) {\n-                getJobLogger().log(Level.WARNING, \"Manifest file not found\");\n-                // TODO: \n-                // add code to generate the manifest, if not present? -- L.A. \n-            } else {\n-                try {\n-                    packageFile.setChecksumValue(FileUtil.calculateChecksum(checksumManifestPath, packageFile.getChecksumType()));\n-                } catch (Exception ex) {\n-                    getJobLogger().log(Level.SEVERE, \"Failed to calculate checksum (type \"+packageFile.getChecksumType()+\") \"+ex.getMessage());\n-                    jobContext.setExitStatus(\"FAILED\");\n-                    return null;\n-                }\n-            }\n         } else {\n-            getJobLogger().log(Level.WARNING, \"No checksumManifest property supplied\");\n-        }\n-        \n-        // Move the folder to the final destination: \n-        if (!(new File(datasetDirectory + File.separator + folderName).renameTo(new File(datasetDirectory + File.separator + packageFile.getStorageIdentifier())))) {\n-            getJobLogger().log(Level.SEVERE, \"Could not move the file folder to the final destination (\" + datasetDirectory + File.separator + packageFile.getStorageIdentifier() + \")\");\n-            jobContext.setExitStatus(\"FAILED\");\n-            return null;\n-        }\n-   \n-            \n-        packageFile.setFilesize(totalSize);\n-        packageFile.setModificationTime(new Timestamp(new Date().getTime()));\n-        packageFile.setCreateDate(new Timestamp(new Date().getTime()));\n-        packageFile.setPermissionModificationTime(new Timestamp(new Date().getTime()));\n-        packageFile.setOwner(dataset);\n-        dataset.getFiles().add(packageFile);\n-\n-        packageFile.setIngestDone();\n-\n-        // set metadata and add to latest version\n-        FileMetadata fmd = new FileMetadata();\n-        fmd.setLabel(folderName);\n-        \n-        fmd.setDataFile(packageFile);\n-        packageFile.getFileMetadatas().add(fmd);\n-        if (dataset.getLatestVersion().getFileMetadatas() == null) dataset.getLatestVersion().setFileMetadatas(new ArrayList<>());\n-        \n-        dataset.getLatestVersion().getFileMetadatas().add(fmd);\n-        fmd.setDatasetVersion(dataset.getLatestVersion());\n-        \n-\tString isFilePIDsEnabled = commandEngine.getContext().settings().getValueForKey(SettingsServiceBean.Key.FilePIDsEnabled, \"true\"); //default value for file PIDs is 'true'\n-\tif (\"true\".contentEquals( isFilePIDsEnabled )) {\n-\t\n-        GlobalIdServiceBean idServiceBean = GlobalIdServiceBean.getBean(packageFile.getProtocol(), commandEngine.getContext());\n-        if (packageFile.getIdentifier() == null || packageFile.getIdentifier().isEmpty()) {\n-            packageFile.setIdentifier(dataFileServiceBean.generateDataFileIdentifier(packageFile, idServiceBean));\n-        }\n-        String nonNullDefaultIfKeyNotFound = \"\";\n-        String protocol = commandEngine.getContext().settings().getValueForKey(SettingsServiceBean.Key.Protocol, nonNullDefaultIfKeyNotFound);\n-        String authority = commandEngine.getContext().settings().getValueForKey(SettingsServiceBean.Key.Authority, nonNullDefaultIfKeyNotFound);\n-        if (packageFile.getProtocol() == null) {\n-            packageFile.setProtocol(protocol);\n-        }\n-        if (packageFile.getAuthority() == null) {\n-            packageFile.setAuthority(authority);\n+          getJobLogger().log(Level.WARNING, \"Unable to find checksum in manifest for: \" + file.getAbsolutePath());\n         }\n+      } else {\n+        getJobLogger().log(Level.SEVERE, \"No checksum hashmap found in transientUserData\");\n+        jobContext.setExitStatus(\"FAILED\");\n+        return null;\n+      }\n \n-        if (!packageFile.isIdentifierRegistered()) {\n-            String doiRetString = \"\";\n-            idServiceBean = GlobalIdServiceBean.getBean(commandEngine.getContext());\n-            try {\n-                doiRetString = idServiceBean.createIdentifier(packageFile);\n-            } catch (Throwable e) {\n-                \n-            }\n+    }\n \n-            // Check return value to make sure registration succeeded\n-            if (!idServiceBean.registerWhenPublished() && doiRetString.contains(packageFile.getIdentifier())) {\n-                packageFile.setIdentifierRegistered(true);\n-                packageFile.setGlobalIdCreateTime(new Date());\n-            }\n-        }\n-\t}\n+    // If the manifest file is present, calculate the checksum of the manifest\n+    // and use it as the checksum of the datafile:\n \n-        getJobLogger().log(Level.INFO, \"Successfully created a file of type package\");\n-        \n-        return packageFile;\n+    if (System.getProperty(\"checksumManifest\") != null) {\n+      checksumManifest = System.getProperty(\"checksumManifest\");\n     }\n-    \n-    /**\n-     * Create a DatasetFile and corresponding FileMetadata for a file on the filesystem and add it to the\n-     * latest dataset version (if the user has AddDataset permissions for the dataset).\n-     * @param file file to create dataFile from\n-     * @return datafile\n-     */\n-    private DataFile createDataFile(File file) {\n-        \n-        DatasetVersion version = dataset.getLatestVersion();\n-        String path = file.getAbsolutePath();\n-        String gid = dataset.getAuthority() + \"/\" + dataset.getIdentifier();\n-        String relativePath = path.substring(path.indexOf(gid) + gid.length() + 1);\n-        \n-        DataFile datafile = new DataFile(\"application/octet-stream\"); // we don't determine mime type\n-        datafile.setStorageIdentifier(relativePath);\n-        datafile.setFilesize(file.length());\n-        datafile.setModificationTime(new Timestamp(new Date().getTime()));\n-        datafile.setCreateDate(new Timestamp(new Date().getTime()));\n-        datafile.setPermissionModificationTime(new Timestamp(new Date().getTime()));\n-        datafile.setOwner(dataset);\n-        datafile.setIngestDone();\n-\n-        // check system property first, otherwise use the batch job property\n-        String jobChecksumType;\n-        if (System.getProperty(\"checksumType\") != null) {\n-            jobChecksumType = System.getProperty(\"checksumType\");\n-        } else {\n-            jobChecksumType = checksumType;\n-        }\n-        datafile.setChecksumType(DataFile.ChecksumType.SHA1); // initial default\n-        for (DataFile.ChecksumType type : DataFile.ChecksumType.values()) {\n-            if (jobChecksumType.equalsIgnoreCase(type.name())) {\n-                datafile.setChecksumType(type);\n-                break;\n-            }\n+\n+    File checksumManifestFile = null;\n+    if (checksumManifest != null && !checksumManifest.isEmpty()) {\n+      String checksumManifestPath = datasetDirectory + File.separator + folderName + File.separator + checksumManifest;\n+      checksumManifestFile = new File(checksumManifestPath);\n+\n+      if (!checksumManifestFile.exists()) {\n+        getJobLogger().log(Level.WARNING, \"Manifest file not found\");\n+        // TODO:\n+        // add code to generate the manifest, if not present? -- L.A.\n+      } else {\n+        try {\n+          packageFile.setChecksumValue(FileUtil.calculateChecksum(checksumManifestPath, packageFile.getChecksumType()));\n+        } catch (Exception ex) {\n+          getJobLogger().log(Level.SEVERE,\n+            \"Failed to calculate checksum (type \" + packageFile.getChecksumType() + \") \" + ex.getMessage());\n+          jobContext.setExitStatus(\"FAILED\");\n+          return null;\n         }\n-        // lookup the checksum value in the job's manifest hashmap\n-        if (jobContext.getTransientUserData() != null) {\n-            String checksumVal = ((Map<String, String>) jobContext.getTransientUserData()).get(relativePath);\n-            if (checksumVal != null) {\n-                datafile.setChecksumValue(checksumVal);\n-                // remove the key, so we can check for unused checksums when the job is complete\n-                ((Map<String, String>) jobContext.getTransientUserData()).remove(relativePath);\n-            } else {\n-                datafile.setChecksumValue(\"Unknown\");\n-                getJobLogger().log(Level.WARNING, \"Unable to find checksum in manifest for: \" + file.getAbsolutePath());\n-            }\n-        } else {\n-            getJobLogger().log(Level.SEVERE, \"No checksum hashmap found in transientUserData\");\n-            jobContext.setExitStatus(\"FAILED\");\n-            return null;\n+      }\n+    } else {\n+      getJobLogger().log(Level.WARNING, \"No checksumManifest property supplied\");\n+    }\n+\n+    // Move the folder to the final destination:\n+    if (!(new File(datasetDirectory + File.separator + folderName)\n+      .renameTo(new File(datasetDirectory + File.separator + packageFile.getStorageIdentifier())))) {\n+      getJobLogger().log(Level.SEVERE,\n+        \"Could not move the file folder to the final destination (\" + datasetDirectory + File.separator +\n+          packageFile.getStorageIdentifier() + \")\");\n+      jobContext.setExitStatus(\"FAILED\");\n+      return null;\n+    }\n+\n+\n+    packageFile.setFilesize(totalSize);\n+    packageFile.setModificationTime(new Timestamp(new Date().getTime()));\n+    packageFile.setCreateDate(new Timestamp(new Date().getTime()));\n+    packageFile.setPermissionModificationTime(new Timestamp(new Date().getTime()));\n+    packageFile.setOwner(dataset);\n+    dataset.getFiles().add(packageFile);\n+\n+    packageFile.setIngestDone();\n+\n+    // set metadata and add to latest version\n+    FileMetadata fmd = new FileMetadata();\n+    fmd.setLabel(folderName);\n+\n+    fmd.setDataFile(packageFile);\n+    packageFile.getFileMetadatas().add(fmd);\n+    if (dataset.getLatestVersion().getFileMetadatas() == null) {\n+      dataset.getLatestVersion().setFileMetadatas(new ArrayList<>());\n+    }\n+\n+    dataset.getLatestVersion().getFileMetadatas().add(fmd);\n+    fmd.setDatasetVersion(dataset.getLatestVersion());\n+\n+    String isFilePIDsEnabled = commandEngine.getContext().settings()\n+      .getValueForKey(SettingsServiceBean.Key.FilePIDsEnabled, \"true\"); //default value for file PIDs is 'true'\n+    if (\"true\".contentEquals(isFilePIDsEnabled)) {\n+\n+      GlobalIdServiceBean idServiceBean =\n+        GlobalIdServiceBean.getBean(packageFile.getProtocol(), commandEngine.getContext());\n+      if (packageFile.getIdentifier() == null || packageFile.getIdentifier().isEmpty()) {\n+        packageFile.setIdentifier(dataFileServiceBean.generateDataFileIdentifier(packageFile, idServiceBean));\n+      }\n+      String nonNullDefaultIfKeyNotFound = \"\";\n+      String protocol = commandEngine.getContext().settings()\n+        .getValueForKey(SettingsServiceBean.Key.Protocol, nonNullDefaultIfKeyNotFound);\n+      String authority = commandEngine.getContext().settings()\n+        .getValueForKey(SettingsServiceBean.Key.Authority, nonNullDefaultIfKeyNotFound);\n+      if (packageFile.getProtocol() == null) {\n+        packageFile.setProtocol(protocol);\n+      }\n+      if (packageFile.getAuthority() == null) {\n+        packageFile.setAuthority(authority);\n+      }\n+\n+      if (!packageFile.isIdentifierRegistered()) {\n+        String doiRetString = \"\";\n+        idServiceBean = GlobalIdServiceBean.getBean(commandEngine.getContext());\n+        try {\n+          doiRetString = idServiceBean.createIdentifier(packageFile);\n+        } catch (Throwable e) {\n+\n         }\n \n-        // set metadata and add to latest version\n-        FileMetadata fmd = new FileMetadata();\n-        fmd.setLabel(file.getName());\n-        // set the subdirectory if there is one\n-        if (relativePath.contains(File.separator)) {\n-            fmd.setDirectoryLabel(relativePath.replace(File.separator + file.getName(), \"\"));\n+        // Check return value to make sure registration succeeded\n+        if (!idServiceBean.registerWhenPublished() && doiRetString.contains(packageFile.getIdentifier())) {\n+          packageFile.setIdentifierRegistered(true);\n+          packageFile.setGlobalIdCreateTime(new Date());\n         }\n-        fmd.setDataFile(datafile);\n-        datafile.getFileMetadatas().add(fmd);\n-        if (version.getFileMetadatas() == null) version.setFileMetadatas(new ArrayList<>());\n-        version.getFileMetadatas().add(fmd);\n-        fmd.setDatasetVersion(version);\n-\n-        datafile = dataFileServiceBean.save(datafile);\n-        return datafile;\n+      }\n+    }\n+\n+    getJobLogger().log(Level.INFO, \"Successfully created a file of type package\");\n+\n+    return packageFile;\n+  }\n+\n+  /**\n+   * Create a DatasetFile and corresponding FileMetadata for a file on the filesystem and add it to the\n+   * latest dataset version (if the user has AddDataset permissions for the dataset).\n+   *\n+   * @param file file to create dataFile from\n+   * @return datafile\n+   */\n+  private DataFile createDataFile(File file) {\n+\n+    DatasetVersion version = dataset.getLatestVersion();\n+    String path = file.getAbsolutePath();\n+    String gid = dataset.getAuthority() + \"/\" + dataset.getIdentifier();\n+    String relativePath = path.substring(path.indexOf(gid) + gid.length() + 1);\n+\n+    DataFile datafile = new DataFile(\"application/octet-stream\"); // we don't determine mime type\n+    datafile.setStorageIdentifier(relativePath);\n+    datafile.setFilesize(file.length());\n+    datafile.setModificationTime(new Timestamp(new Date().getTime()));\n+    datafile.setCreateDate(new Timestamp(new Date().getTime()));\n+    datafile.setPermissionModificationTime(new Timestamp(new Date().getTime()));\n+    datafile.setOwner(dataset);\n+    datafile.setIngestDone();\n+\n+    // check system property first, otherwise use the batch job property\n+    String jobChecksumType;\n+    if (System.getProperty(\"checksumType\") != null) {\n+      jobChecksumType = System.getProperty(\"checksumType\");\n+    } else {\n+      jobChecksumType = checksumType;\n     }\n-    \n-    private Logger getJobLogger() {\n-        return Logger.getLogger(\"job-\"+jobContext.getInstanceId());\n+    datafile.setChecksumType(DataFile.ChecksumType.SHA1); // initial default\n+    for (DataFile.ChecksumType type : DataFile.ChecksumType.values()) {\n+      if (jobChecksumType.equalsIgnoreCase(type.name())) {\n+        datafile.setChecksumType(type);\n+        break;\n+      }\n     }\n-    \n+    // lookup the checksum value in the job's manifest hashmap\n+    if (jobContext.getTransientUserData() != null) {\n+      String checksumVal = ((Map<String, String>) jobContext.getTransientUserData()).get(relativePath);\n+      if (checksumVal != null) {\n+        datafile.setChecksumValue(checksumVal);\n+        // remove the key, so we can check for unused checksums when the job is complete\n+        ((Map<String, String>) jobContext.getTransientUserData()).remove(relativePath);\n+      } else {\n+        datafile.setChecksumValue(\"Unknown\");\n+        getJobLogger().log(Level.WARNING, \"Unable to find checksum in manifest for: \" + file.getAbsolutePath());\n+      }\n+    } else {\n+      getJobLogger().log(Level.SEVERE, \"No checksum hashmap found in transientUserData\");\n+      jobContext.setExitStatus(\"FAILED\");\n+      return null;\n+    }\n+\n+    // set metadata and add to latest version\n+    FileMetadata fmd = new FileMetadata();\n+    fmd.setLabel(file.getName());\n+    // set the subdirectory if there is one\n+    if (relativePath.contains(File.separator)) {\n+      fmd.setDirectoryLabel(relativePath.replace(File.separator + file.getName(), \"\"));\n+    }\n+    fmd.setDataFile(datafile);\n+    datafile.getFileMetadatas().add(fmd);\n+    if (version.getFileMetadatas() == null) {\n+      version.setFileMetadatas(new ArrayList<>());\n+    }\n+    version.getFileMetadatas().add(fmd);\n+    fmd.setDatasetVersion(version);\n+\n+    datafile = dataFileServiceBean.save(datafile);\n+    return datafile;\n+  }\n+\n+  private Logger getJobLogger() {\n+    return Logger.getLogger(\"job-\" + jobContext.getInstanceId());\n+  }\n+\n }\n",
            "diff_size": 614
        },
        {
            "tool": "naturalize",
            "errors": null,
            "diff": null
        },
        {
            "tool": "codebuff",
            "errors": null,
            "diff": null
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "361",
                    "column": "1",
                    "severity": "error",
                    "message": "File contains tab characters (this is the first instance).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.FileTabCharacterCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/125/FileRecordWriter.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_random/125/FileRecordWriter.java\nindex c82a5bb01eb..fa283f73c53 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/125/FileRecordWriter.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_random/125/FileRecordWriter.java\n@@ -357,8 +357,7 @@ public class FileRecordWriter extends AbstractItemWriter {\n         \n         dataset.getLatestVersion().getFileMetadatas().add(fmd);\n         fmd.setDatasetVersion(dataset.getLatestVersion());\n-        \n-\tString isFilePIDsEnabled = commandEngine.getContext().settings().getValueForKey(SettingsServiceBean.Key.FilePIDsEnabled, \"true\"); //default value for file PIDs is 'true'\n+        String isFilePIDsEnabled = commandEngine.getContext().settings().getValueForKey(SettingsServiceBean.Key.FilePIDsEnabled, \"true\"); //default value for file PIDs is 'true'\n \tif (\"true\".contentEquals( isFilePIDsEnabled )) {\n \t\n         GlobalIdServiceBean idServiceBean = GlobalIdServiceBean.getBean(packageFile.getProtocol(), commandEngine.getContext());\n",
            "diff_size": 2
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "361",
                    "column": "1",
                    "severity": "error",
                    "message": "File contains tab characters (this is the first instance).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.FileTabCharacterCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/125/FileRecordWriter.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_three_grams/125/FileRecordWriter.java\nindex c82a5bb01eb..fa283f73c53 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/125/FileRecordWriter.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_three_grams/125/FileRecordWriter.java\n@@ -357,8 +357,7 @@ public class FileRecordWriter extends AbstractItemWriter {\n         \n         dataset.getLatestVersion().getFileMetadatas().add(fmd);\n         fmd.setDatasetVersion(dataset.getLatestVersion());\n-        \n-\tString isFilePIDsEnabled = commandEngine.getContext().settings().getValueForKey(SettingsServiceBean.Key.FilePIDsEnabled, \"true\"); //default value for file PIDs is 'true'\n+        String isFilePIDsEnabled = commandEngine.getContext().settings().getValueForKey(SettingsServiceBean.Key.FilePIDsEnabled, \"true\"); //default value for file PIDs is 'true'\n \tif (\"true\".contentEquals( isFilePIDsEnabled )) {\n \t\n         GlobalIdServiceBean idServiceBean = GlobalIdServiceBean.getBean(packageFile.getProtocol(), commandEngine.getContext());\n",
            "diff_size": 2
        }
    ],
    "repaired_by": [
        "intellij"
    ],
    "not_repaired_by": [
        "styler",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ]
}