{
    "project_name": "NationalSecurityAgency-datawave",
    "error_id": "105",
    "information": {
        "errors": [
            {
                "line": "89",
                "severity": "error",
                "message": "Accumulo non-public classes imported",
                "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
            }
        ]
    },
    "source_code": "import org.apache.accumulo.core.iterators.YieldCallback;\nimport org.apache.accumulo.core.iterators.YieldingKeyValueIterator;\nimport org.apache.accumulo.tserver.tablet.TabletClosedException;\nimport org.apache.commons.collections4.iterators.EmptyIterator;\nimport org.apache.commons.jexl2.JexlArithmetic;\nimport org.apache.commons.jexl2.parser.ASTJexlScript;",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "89",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "intellij",
            "errors": [
                {
                    "line": "89",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/105/QueryIterator.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/intellij/105/QueryIterator.java\nindex 01eec229579..7f9668a71e7 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/105/QueryIterator.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/intellij/105/QueryIterator.java\n@@ -124,7 +124,7 @@ import static org.apache.commons.pool.impl.GenericObjectPool.WHEN_EXHAUSTED_BLOC\n  * applies a series of transformations and predicates to satisfy the Datawave query requirements.\n  *\n  * <br>\n- * \n+ *\n  * <h1>Document Keys</h1>\n  * <p>\n  * The source of Document Keys is one of the following:\n@@ -136,7 +136,7 @@ import static org.apache.commons.pool.impl.GenericObjectPool.WHEN_EXHAUSTED_BLOC\n  * {@link Entry}&lt;Key,Value&gt;\n  *\n  * <br>\n- * \n+ *\n  * <h1>Transformations/Predicates</h1>\n  * <p>\n  * The following transformations/predicates are applied (order sensitive):\n@@ -151,1395 +151,1484 @@ import static org.apache.commons.pool.impl.GenericObjectPool.WHEN_EXHAUSTED_BLOC\n  * <li>PostProcessing Enrichment - Variable enrichment, e.g. term frequency enrichment</li>\n  * <li>Serialize Document to a Value, e.g. Kryo, Writable, etc</li>\n  * </ol>\n- *\n  */\n-public class QueryIterator extends QueryOptions implements YieldingKeyValueIterator<Key,Value>, JexlContextCreator.JexlContextValueComparator,\n-                SourceFactory<Key,Value>, SortedKeyValueIterator<Key,Value> {\n-    \n-    private static final Logger log = Logger.getLogger(QueryIterator.class);\n-    \n-    protected SortedKeyValueIterator<Key,Value> source;\n-    protected SortedKeyValueIterator<Key,Value> sourceForDeepCopies;\n-    protected Map<String,String> documentOptions;\n-    protected NestedIterator<Key> initKeySource, seekKeySource;\n-    protected Iterator<Entry<Key,Value>> serializedDocuments;\n-    protected boolean fieldIndexSatisfiesQuery = false;\n-    \n-    protected Range range;\n-    protected Range originalRange;\n-    \n-    protected Key key;\n-    protected Value value;\n-    protected YieldCallback<Key> yield;\n-    \n-    protected IteratorEnvironment myEnvironment;\n-    \n-    protected ASTJexlScript script = null;\n-    \n-    protected JexlEvaluation myEvaluationFunction = null;\n-    \n-    protected QuerySpan trackingSpan = null;\n-    \n-    protected QuerySpanCollector querySpanCollector = new QuerySpanCollector();\n-    \n-    protected UniqueTransform uniqueTransform = null;\n-    \n-    protected GroupingTransform groupingTransform;\n-    \n-    protected boolean groupingContextAddedByMe = false;\n-    \n-    protected TypeMetadata typeMetadataWithNonIndexed = null;\n-    \n-    protected Map<String,Object> exceededOrEvaluationCache = null;\n-    \n-    public QueryIterator() {}\n-    \n-    public QueryIterator(QueryIterator other, IteratorEnvironment env) {\n-        // Need to copy all members instantiated/modified during init()\n-        this.source = other.source.deepCopy(env);\n-        this.sourceForDeepCopies = source.deepCopy(env);\n-        this.initKeySource = other.initKeySource;\n-        this.seekKeySource = other.seekKeySource;\n-        this.myEnvironment = other.myEnvironment;\n-        this.myEvaluationFunction = other.myEvaluationFunction;\n-        this.script = other.script;\n-        this.documentOptions = other.documentOptions;\n-        this.fieldIndexSatisfiesQuery = other.fieldIndexSatisfiesQuery;\n-        this.groupingContextAddedByMe = other.groupingContextAddedByMe;\n-        this.typeMetadataWithNonIndexed = other.typeMetadataWithNonIndexed;\n-        this.typeMetadata = other.typeMetadata;\n-        this.exceededOrEvaluationCache = other.exceededOrEvaluationCache;\n-        this.trackingSpan = other.trackingSpan;\n-        // Defer to QueryOptions to re-set all of the query options\n-        super.deepCopy(other);\n+public class QueryIterator extends QueryOptions\n+    implements YieldingKeyValueIterator<Key, Value>, JexlContextCreator.JexlContextValueComparator,\n+    SourceFactory<Key, Value>, SortedKeyValueIterator<Key, Value> {\n+\n+  private static final Logger log = Logger.getLogger(QueryIterator.class);\n+\n+  protected SortedKeyValueIterator<Key, Value> source;\n+  protected SortedKeyValueIterator<Key, Value> sourceForDeepCopies;\n+  protected Map<String, String> documentOptions;\n+  protected NestedIterator<Key> initKeySource, seekKeySource;\n+  protected Iterator<Entry<Key, Value>> serializedDocuments;\n+  protected boolean fieldIndexSatisfiesQuery = false;\n+\n+  protected Range range;\n+  protected Range originalRange;\n+\n+  protected Key key;\n+  protected Value value;\n+  protected YieldCallback<Key> yield;\n+\n+  protected IteratorEnvironment myEnvironment;\n+\n+  protected ASTJexlScript script = null;\n+\n+  protected JexlEvaluation myEvaluationFunction = null;\n+\n+  protected QuerySpan trackingSpan = null;\n+\n+  protected QuerySpanCollector querySpanCollector = new QuerySpanCollector();\n+\n+  protected UniqueTransform uniqueTransform = null;\n+\n+  protected GroupingTransform groupingTransform;\n+\n+  protected boolean groupingContextAddedByMe = false;\n+\n+  protected TypeMetadata typeMetadataWithNonIndexed = null;\n+\n+  protected Map<String, Object> exceededOrEvaluationCache = null;\n+\n+  public QueryIterator() {\n+  }\n+\n+  public QueryIterator(QueryIterator other, IteratorEnvironment env) {\n+    // Need to copy all members instantiated/modified during init()\n+    this.source = other.source.deepCopy(env);\n+    this.sourceForDeepCopies = source.deepCopy(env);\n+    this.initKeySource = other.initKeySource;\n+    this.seekKeySource = other.seekKeySource;\n+    this.myEnvironment = other.myEnvironment;\n+    this.myEvaluationFunction = other.myEvaluationFunction;\n+    this.script = other.script;\n+    this.documentOptions = other.documentOptions;\n+    this.fieldIndexSatisfiesQuery = other.fieldIndexSatisfiesQuery;\n+    this.groupingContextAddedByMe = other.groupingContextAddedByMe;\n+    this.typeMetadataWithNonIndexed = other.typeMetadataWithNonIndexed;\n+    this.typeMetadata = other.typeMetadata;\n+    this.exceededOrEvaluationCache = other.exceededOrEvaluationCache;\n+    this.trackingSpan = other.trackingSpan;\n+    // Defer to QueryOptions to re-set all of the query options\n+    super.deepCopy(other);\n+  }\n+\n+  private boolean gatherTimingDetails() {\n+    return collectTimingDetails || (statsdHostAndPort != null);\n+  }\n+\n+  @Override\n+  public void init(SortedKeyValueIterator<Key, Value> source, Map<String, String> options, IteratorEnvironment env)\n+      throws IOException {\n+    if (log.isTraceEnabled()) {\n+      log.trace(\"QueryIterator init()\");\n     }\n-    \n-    private boolean gatherTimingDetails() {\n-        return collectTimingDetails || (statsdHostAndPort != null);\n+\n+    if (!validateOptions(new SourcedOptions<>(source, env, options))) {\n+      throw new IllegalArgumentException(\"Could not initialize QueryIterator with \" + options);\n     }\n-    \n-    @Override\n-    public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options, IteratorEnvironment env) throws IOException {\n-        if (log.isTraceEnabled()) {\n-            log.trace(\"QueryIterator init()\");\n+\n+    // We want to add in spoofed dataTypes for Aggregation/Evaluation to\n+    // ensure proper numeric evaluation.\n+    this.typeMetadata = new TypeMetadata(this.getTypeMetadata());\n+    this.typeMetadataWithNonIndexed = new TypeMetadata(this.typeMetadata);\n+    this.typeMetadataWithNonIndexed.addForAllIngestTypes(this.getNonIndexedDataTypeMap());\n+\n+    this.exceededOrEvaluationCache = new HashMap<>();\n+\n+    // Parse the query\n+    try {\n+      this.script = JexlASTHelper.parseJexlQuery(this.getQuery());\n+      this.myEvaluationFunction = new JexlEvaluation(this.getQuery(), arithmetic);\n+\n+    } catch (Exception e) {\n+      throw new IOException(\"Could not parse the JEXL query: '\" + this.getQuery() + \"'\", e);\n+    }\n+\n+    this.documentOptions = options;\n+    this.myEnvironment = env;\n+\n+    if (gatherTimingDetails()) {\n+      this.trackingSpan = new MultiThreadedQuerySpan(getStatsdClient());\n+      this.source = new SourceTrackingIterator(trackingSpan, source);\n+    } else {\n+      this.source = source;\n+    }\n+\n+    this.fiAggregator = new IdentityAggregator(getAllIndexOnlyFields(), getEvaluationFilter(),\n+        getEvaluationFilter() != null ? getEvaluationFilter()\n+            .getMaxNextCount() : -1);\n+\n+    if (isDebugMultithreadedSources()) {\n+      this.source = new SourceThreadTrackingIterator(this.source);\n+    }\n+\n+    this.sourceForDeepCopies = this.source.deepCopy(this.myEnvironment);\n+\n+    // update ActiveQueryLog with (potentially) updated config\n+    if (env != null) {\n+      ActiveQueryLog.setConfig(env.getConfig());\n+    }\n+\n+    DatawaveFieldIndexListIteratorJexl.FSTManager.setHdfsFileSystem(this.getFileSystemCache());\n+    DatawaveFieldIndexListIteratorJexl.FSTManager.setHdfsFileCompressionCodec(this.getHdfsFileCompressionCodec());\n+\n+    pruneIvaratorCacheDirs();\n+  }\n+\n+  // this method will prune any ivarator cache directories that are not available on this node\n+  private void pruneIvaratorCacheDirs() throws IOException {\n+    ivaratorCacheDirConfigs.removeIf(this::pruneIvaratorCacheDir);\n+  }\n+\n+  private boolean pruneIvaratorCacheDir(IvaratorCacheDirConfig config) {\n+    boolean fsExists = false;\n+\n+    // first, make sure the cache configuration is valid\n+    if (config.isValid()) {\n+      Path basePath = new Path(config.getBasePathURI());\n+\n+      try {\n+        FileSystem fs = this.getFileSystemCache().getFileSystem(basePath.toUri());\n+        fsExists = fs.mkdirs(basePath) || fs.exists(basePath);\n+      } catch (Exception e) {\n+        log.debug(\"Ivarator Cache Dir does not exist: \" + basePath);\n+      }\n+    }\n+\n+    return !fsExists;\n+  }\n+\n+  @Override\n+  public boolean hasTop() {\n+    boolean yielded = (this.yield != null) && this.yield.hasYielded();\n+    boolean result = (!yielded) && (this.key != null) && (this.value != null);\n+    if (log.isTraceEnabled()) {\n+      log.trace(\"hasTop() \" + result);\n+    }\n+    return result;\n+  }\n+\n+  @Override\n+  public void enableYielding(YieldCallback<Key> yieldCallback) {\n+    this.yield = yieldCallback;\n+  }\n+\n+  @Override\n+  public void next() throws IOException {\n+    ActiveQueryLog.getInstance().get(getQueryId()).beginCall(this.originalRange, ActiveQuery.CallType.NEXT);\n+\n+    try (TraceScope s = Trace.startSpan(\"QueryIterator.next()\")) {\n+      if (log.isTraceEnabled()) {\n+        log.trace(\"next\");\n+      }\n+      prepareKeyValue(s);\n+    } catch (Exception e) {\n+      handleException(e);\n+    } finally {\n+      QueryStatsDClient client = getStatsdClient();\n+      if (client != null) {\n+        client.flush();\n+      }\n+      ActiveQueryLog.getInstance().get(getQueryId()).endCall(this.originalRange, ActiveQuery.CallType.NEXT);\n+      if (this.key == null && this.value == null) {\n+        // no entries to return\n+        ActiveQueryLog.getInstance().remove(getQueryId(), this.originalRange);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void seek(Range range, Collection<ByteSequence> columnFamilies, boolean inclusive) throws IOException {\n+    // preserve the original range for use with the Final Document tracking iterator because it is placed after the ResultCountingIterator\n+    // so the FinalDocumentTracking iterator needs the start key with the count already appended\n+    originalRange = range;\n+    ActiveQueryLog.getInstance().get(getQueryId()).beginCall(this.originalRange, ActiveQuery.CallType.SEEK);\n+\n+    try (TraceScope span = Trace.startSpan(\"QueryIterator.seek\")) {\n+      if (this.isIncludeGroupingContext() == false\n+          && (this.query.contains(\"grouping:\") || this.query.contains(\"matchesInGroup\") ||\n+          this.query.contains(\"MatchesInGroup\") || this.query\n+          .contains(\"atomValuesMatch\"))) {\n+        this.setIncludeGroupingContext(true);\n+        this.groupingContextAddedByMe = true;\n+      } else {\n+        this.groupingContextAddedByMe = false;\n+      }\n+      if (log.isDebugEnabled()) {\n+        log.debug(\"Seek range: \" + range + \" \" + query);\n+      }\n+      this.range = range;\n+\n+      // determine whether this is a teardown/rebuild range\n+      long resultCount = 0;\n+      if (!range.isStartKeyInclusive()) {\n+        // see if we can fail fast. If we were rebuilt with the FinalDocument key, then we are already completely done\n+        if (collectTimingDetails && FinalDocumentTrackingIterator.isFinalDocumentKey(range.getStartKey())) {\n+          this.seekKeySource = new EmptyTreeIterable();\n+          this.serializedDocuments = EmptyIterator.emptyIterator();\n+          prepareKeyValue(span);\n+          return;\n         }\n-        \n-        if (!validateOptions(new SourcedOptions<>(source, env, options))) {\n-            throw new IllegalArgumentException(\"Could not initialize QueryIterator with \" + options);\n+\n+        // see if we have a count in the cf\n+        Key startKey = range.getStartKey();\n+        String[] parts = StringUtils.split(startKey.getColumnFamily().toString(), '\\0');\n+        if (parts.length == 3) {\n+          resultCount = NumericalEncoder.decode(parts[0]).longValue();\n+          // remove the count from the range\n+          startKey = new Key(startKey.getRow(), new Text(parts[1] + '\\0' + parts[2]), startKey.getColumnQualifier(),\n+              startKey.getColumnVisibility(),\n+              startKey.getTimestamp());\n+          this.range =\n+              range = new Range(startKey, range.isStartKeyInclusive(), range.getEndKey(), range.isEndKeyInclusive());\n         }\n-        \n-        // We want to add in spoofed dataTypes for Aggregation/Evaluation to\n-        // ensure proper numeric evaluation.\n-        this.typeMetadata = new TypeMetadata(this.getTypeMetadata());\n-        this.typeMetadataWithNonIndexed = new TypeMetadata(this.typeMetadata);\n-        this.typeMetadataWithNonIndexed.addForAllIngestTypes(this.getNonIndexedDataTypeMap());\n-        \n-        this.exceededOrEvaluationCache = new HashMap<>();\n-        \n-        // Parse the query\n-        try {\n-            this.script = JexlASTHelper.parseJexlQuery(this.getQuery());\n-            this.myEvaluationFunction = new JexlEvaluation(this.getQuery(), arithmetic);\n-            \n-        } catch (Exception e) {\n-            throw new IOException(\"Could not parse the JEXL query: '\" + this.getQuery() + \"'\", e);\n+      }\n+\n+      // determine whether this is a document specific range\n+      Range documentRange = isDocumentSpecificRange(range) ? range : null;\n+\n+      // if we have a document specific range, but the key is not\n+      // inclusive then we have already returned the document; this scan\n+      // is done\n+      if (documentRange != null && !documentRange.isStartKeyInclusive()) {\n+        if (log.isTraceEnabled()) {\n+          log.trace(\"Received non-inclusive event specific range: \" + documentRange);\n         }\n-        \n-        this.documentOptions = options;\n-        this.myEnvironment = env;\n-        \n         if (gatherTimingDetails()) {\n-            this.trackingSpan = new MultiThreadedQuerySpan(getStatsdClient());\n-            this.source = new SourceTrackingIterator(trackingSpan, source);\n+          this.seekKeySource =\n+              new EvaluationTrackingNestedIterator(QuerySpan.Stage.EmptyTree, trackingSpan, new EmptyTreeIterable(),\n+                  myEnvironment);\n         } else {\n-            this.source = source;\n+          this.seekKeySource = new EmptyTreeIterable();\n         }\n-        \n-        this.fiAggregator = new IdentityAggregator(getAllIndexOnlyFields(), getEvaluationFilter(), getEvaluationFilter() != null ? getEvaluationFilter()\n-                        .getMaxNextCount() : -1);\n-        \n-        if (isDebugMultithreadedSources()) {\n-            this.source = new SourceThreadTrackingIterator(this.source);\n+      }\n+\n+      // if the Range is for a single document and the query doesn't reference any index-only or tokenized fields\n+      else if (documentRange != null &&\n+          (!this.isContainsIndexOnlyTerms() && this.getTermFrequencyFields().isEmpty() && !super.mustUseFieldIndex)) {\n+        if (log.isTraceEnabled()) {\n+          log.trace(\"Received event specific range: \" + documentRange);\n         }\n-        \n-        this.sourceForDeepCopies = this.source.deepCopy(this.myEnvironment);\n-        \n-        // update ActiveQueryLog with (potentially) updated config\n-        if (env != null) {\n-            ActiveQueryLog.setConfig(env.getConfig());\n+        // We can take a shortcut to the directly to the event\n+        Entry<Key, Document> documentKey =\n+            Maps.immutableEntry(super.getDocumentKey.apply(documentRange), new Document());\n+        if (log.isTraceEnabled()) {\n+          log.trace(\"Transformed document key: \" + documentKey);\n         }\n-        \n-        DatawaveFieldIndexListIteratorJexl.FSTManager.setHdfsFileSystem(this.getFileSystemCache());\n-        DatawaveFieldIndexListIteratorJexl.FSTManager.setHdfsFileCompressionCodec(this.getHdfsFileCompressionCodec());\n-        \n-        pruneIvaratorCacheDirs();\n-    }\n-    \n-    // this method will prune any ivarator cache directories that are not available on this node\n-    private void pruneIvaratorCacheDirs() throws IOException {\n-        ivaratorCacheDirConfigs.removeIf(this::pruneIvaratorCacheDir);\n-    }\n-    \n-    private boolean pruneIvaratorCacheDir(IvaratorCacheDirConfig config) {\n-        boolean fsExists = false;\n-        \n-        // first, make sure the cache configuration is valid\n-        if (config.isValid()) {\n-            Path basePath = new Path(config.getBasePathURI());\n-            \n-            try {\n-                FileSystem fs = this.getFileSystemCache().getFileSystem(basePath.toUri());\n-                fsExists = fs.mkdirs(basePath) || fs.exists(basePath);\n-            } catch (Exception e) {\n-                log.debug(\"Ivarator Cache Dir does not exist: \" + basePath);\n-            }\n+        // we can only trim if we're certain that the projected fields\n+        // aren't needed for evaluation\n+\n+        if (gatherTimingDetails()) {\n+          this.seekKeySource = new EvaluationTrackingNestedIterator(QuerySpan.Stage.DocumentSpecificTree, trackingSpan,\n+              new DocumentSpecificNestedIterator(documentKey), myEnvironment);\n+        } else {\n+          this.seekKeySource = new DocumentSpecificNestedIterator(documentKey);\n         }\n-        \n-        return !fsExists;\n-    }\n-    \n-    @Override\n-    public boolean hasTop() {\n-        boolean yielded = (this.yield != null) && this.yield.hasYielded();\n-        boolean result = (!yielded) && (this.key != null) && (this.value != null);\n-        if (log.isTraceEnabled())\n-            log.trace(\"hasTop() \" + result);\n-        return result;\n-    }\n-    \n-    @Override\n-    public void enableYielding(YieldCallback<Key> yieldCallback) {\n-        this.yield = yieldCallback;\n-    }\n-    \n-    @Override\n-    public void next() throws IOException {\n-        ActiveQueryLog.getInstance().get(getQueryId()).beginCall(this.originalRange, ActiveQuery.CallType.NEXT);\n-        \n-        try (TraceScope s = Trace.startSpan(\"QueryIterator.next()\")) {\n-            if (log.isTraceEnabled()) {\n-                log.trace(\"next\");\n-            }\n-            prepareKeyValue(s);\n-        } catch (Exception e) {\n-            handleException(e);\n-        } finally {\n-            QueryStatsDClient client = getStatsdClient();\n-            if (client != null) {\n-                client.flush();\n-            }\n-            ActiveQueryLog.getInstance().get(getQueryId()).endCall(this.originalRange, ActiveQuery.CallType.NEXT);\n-            if (this.key == null && this.value == null) {\n-                // no entries to return\n-                ActiveQueryLog.getInstance().remove(getQueryId(), this.originalRange);\n-            }\n+      } else {\n+        this.seekKeySource = buildDocumentIterator(documentRange, range, columnFamilies, inclusive);\n+      }\n+\n+      // Create the pipeline iterator for document aggregation and\n+      // evaluation within a thread pool\n+      PipelineIterator pipelineIter =\n+          PipelineFactory.createIterator(this.seekKeySource, getMaxEvaluationPipelines(), getMaxPipelineCachedResults(),\n+              getSerialPipelineRequest(), querySpanCollector, trackingSpan, this,\n+              sourceForDeepCopies.deepCopy(myEnvironment), myEnvironment,\n+              yield, yieldThresholdMs, columnFamilies, inclusive);\n+\n+      pipelineIter.setCollectTimingDetails(collectTimingDetails);\n+      // TODO pipelineIter.setStatsdHostAndPort(statsdHostAndPort);\n+\n+      pipelineIter.startPipeline();\n+\n+      // gather Key,Document Entries from the pipelines\n+      Iterator<Entry<Key, Document>> pipelineDocuments = pipelineIter;\n+\n+      if (log.isTraceEnabled()) {\n+        pipelineDocuments = Iterators.filter(pipelineDocuments, keyDocumentEntry -> {\n+          log.trace(\"after pipeline, keyDocumentEntry:\" + keyDocumentEntry);\n+          return true;\n+        });\n+      }\n+\n+      // now apply the unique transform if requested\n+      UniqueTransform uniquify = getUniqueTransform();\n+      if (uniquify != null) {\n+        pipelineDocuments = Iterators.filter(pipelineDocuments, uniquify.getUniquePredicate());\n+      }\n+\n+      // apply the grouping transform if requested and if the batch size is greater than zero\n+      // if the batch size is 0, then grouping is computed only on the web server\n+      GroupingTransform groupify = getGroupingTransform();\n+      if (groupify != null && this.groupFieldsBatchSize > 0) {\n+\n+        pipelineDocuments =\n+            groupingTransform.getGroupingIterator(pipelineDocuments, this.groupFieldsBatchSize, this.yield);\n+\n+        if (log.isTraceEnabled()) {\n+          pipelineDocuments = Iterators.filter(pipelineDocuments, keyDocumentEntry -> {\n+            log.trace(\"after grouping, keyDocumentEntry:\" + keyDocumentEntry);\n+            return true;\n+          });\n         }\n+      }\n+\n+      pipelineDocuments = Iterators.filter(\n+          pipelineDocuments,\n+          keyDocumentEntry -> {\n+            // last chance before the documents are serialized\n+            ActiveQueryLog.getInstance().get(getQueryId())\n+                .recordStats(keyDocumentEntry.getValue(), querySpanCollector.getCombinedQuerySpan(null));\n+            // Always return true since we just want to record data in the ActiveQueryLog\n+            return true;\n+          });\n+\n+      if (this.getReturnType() == ReturnType.kryo) {\n+        // Serialize the Document using Kryo\n+        this.serializedDocuments = Iterators\n+            .transform(pipelineDocuments, new KryoDocumentSerializer(isReducedResponse(), isCompressResults()));\n+      } else if (this.getReturnType() == ReturnType.writable) {\n+        // Use the Writable interface to serialize the Document\n+        this.serializedDocuments =\n+            Iterators.transform(pipelineDocuments, new WritableDocumentSerializer(isReducedResponse()));\n+      } else if (this.getReturnType() == ReturnType.tostring) {\n+        // Just return a toString() representation of the document\n+        this.serializedDocuments =\n+            Iterators.transform(pipelineDocuments, new ToStringDocumentSerializer(isReducedResponse()));\n+      } else {\n+        throw new IllegalArgumentException(\"Unknown return type of: \" + this.getReturnType());\n+      }\n+\n+      if (log.isTraceEnabled()) {\n+        KryoDocumentDeserializer dser = new KryoDocumentDeserializer();\n+        this.serializedDocuments = Iterators.filter(this.serializedDocuments, keyValueEntry -> {\n+          log.trace(\"after serializing, keyValueEntry:\" + dser.apply(keyValueEntry));\n+          return true;\n+        });\n+      }\n+\n+      // now add the result count to the keys (required when not sorting UIDs)\n+      // Cannot do this on document specific ranges as the count would place the keys outside the initial range\n+      if (!sortedUIDs && documentRange == null) {\n+        this.serializedDocuments = new ResultCountingIterator(serializedDocuments, resultCount, yield);\n+      } else if (this.sortedUIDs) {\n+        // we have sorted UIDs, so we can mask out the cq\n+        this.serializedDocuments = new KeyAdjudicator<>(serializedDocuments, yield);\n+      }\n+\n+      // only add the final document tracking iterator which sends stats back to the client if collectTimingDetails is true\n+      if (collectTimingDetails) {\n+        // if there is no document to return, then add an empty document\n+        // to store the timing metadata\n+        this.serializedDocuments =\n+            new FinalDocumentTrackingIterator(querySpanCollector, trackingSpan, originalRange, this.serializedDocuments,\n+                this.getReturnType(), this.isReducedResponse(), this.isCompressResults(), this.yield);\n+      }\n+      if (log.isTraceEnabled()) {\n+        KryoDocumentDeserializer dser = new KryoDocumentDeserializer();\n+        this.serializedDocuments = Iterators.filter(this.serializedDocuments, keyValueEntry -> {\n+          log.debug(\"finally, considering:\" + dser.apply(keyValueEntry));\n+          return true;\n+        });\n+      }\n+\n+      // Determine if we have items to return\n+      prepareKeyValue(span);\n+    } catch (Exception e) {\n+      handleException(e);\n+    } finally {\n+      if (gatherTimingDetails() && trackingSpan != null && querySpanCollector != null) {\n+        querySpanCollector.addQuerySpan(trackingSpan);\n+      }\n+      QueryStatsDClient client = getStatsdClient();\n+      if (client != null) {\n+        client.flush();\n+      }\n+      ActiveQueryLog.getInstance().get(getQueryId()).endCall(this.originalRange, ActiveQuery.CallType.SEEK);\n+      if (this.key == null && this.value == null) {\n+        // no entries to return\n+        ActiveQueryLog.getInstance().remove(getQueryId(), this.originalRange);\n+      }\n     }\n-    \n-    @Override\n-    public void seek(Range range, Collection<ByteSequence> columnFamilies, boolean inclusive) throws IOException {\n-        // preserve the original range for use with the Final Document tracking iterator because it is placed after the ResultCountingIterator\n-        // so the FinalDocumentTracking iterator needs the start key with the count already appended\n-        originalRange = range;\n-        ActiveQueryLog.getInstance().get(getQueryId()).beginCall(this.originalRange, ActiveQuery.CallType.SEEK);\n-        \n-        try (TraceScope span = Trace.startSpan(\"QueryIterator.seek\")) {\n-            if (this.isIncludeGroupingContext() == false\n-                            && (this.query.contains(\"grouping:\") || this.query.contains(\"matchesInGroup\") || this.query.contains(\"MatchesInGroup\") || this.query\n-                                            .contains(\"atomValuesMatch\"))) {\n-                this.setIncludeGroupingContext(true);\n-                this.groupingContextAddedByMe = true;\n-            } else {\n-                this.groupingContextAddedByMe = false;\n-            }\n-            if (log.isDebugEnabled()) {\n-                log.debug(\"Seek range: \" + range + \" \" + query);\n-            }\n-            this.range = range;\n-            \n-            // determine whether this is a teardown/rebuild range\n-            long resultCount = 0;\n-            if (!range.isStartKeyInclusive()) {\n-                // see if we can fail fast. If we were rebuilt with the FinalDocument key, then we are already completely done\n-                if (collectTimingDetails && FinalDocumentTrackingIterator.isFinalDocumentKey(range.getStartKey())) {\n-                    this.seekKeySource = new EmptyTreeIterable();\n-                    this.serializedDocuments = EmptyIterator.emptyIterator();\n-                    prepareKeyValue(span);\n-                    return;\n-                }\n-                \n-                // see if we have a count in the cf\n-                Key startKey = range.getStartKey();\n-                String[] parts = StringUtils.split(startKey.getColumnFamily().toString(), '\\0');\n-                if (parts.length == 3) {\n-                    resultCount = NumericalEncoder.decode(parts[0]).longValue();\n-                    // remove the count from the range\n-                    startKey = new Key(startKey.getRow(), new Text(parts[1] + '\\0' + parts[2]), startKey.getColumnQualifier(), startKey.getColumnVisibility(),\n-                                    startKey.getTimestamp());\n-                    this.range = range = new Range(startKey, range.isStartKeyInclusive(), range.getEndKey(), range.isEndKeyInclusive());\n-                }\n-            }\n-            \n-            // determine whether this is a document specific range\n-            Range documentRange = isDocumentSpecificRange(range) ? range : null;\n-            \n-            // if we have a document specific range, but the key is not\n-            // inclusive then we have already returned the document; this scan\n-            // is done\n-            if (documentRange != null && !documentRange.isStartKeyInclusive()) {\n-                if (log.isTraceEnabled()) {\n-                    log.trace(\"Received non-inclusive event specific range: \" + documentRange);\n-                }\n-                if (gatherTimingDetails()) {\n-                    this.seekKeySource = new EvaluationTrackingNestedIterator(QuerySpan.Stage.EmptyTree, trackingSpan, new EmptyTreeIterable(), myEnvironment);\n-                } else {\n-                    this.seekKeySource = new EmptyTreeIterable();\n-                }\n-            }\n-            \n-            // if the Range is for a single document and the query doesn't reference any index-only or tokenized fields\n-            else if (documentRange != null && (!this.isContainsIndexOnlyTerms() && this.getTermFrequencyFields().isEmpty() && !super.mustUseFieldIndex)) {\n-                if (log.isTraceEnabled())\n-                    log.trace(\"Received event specific range: \" + documentRange);\n-                // We can take a shortcut to the directly to the event\n-                Entry<Key,Document> documentKey = Maps.immutableEntry(super.getDocumentKey.apply(documentRange), new Document());\n-                if (log.isTraceEnabled())\n-                    log.trace(\"Transformed document key: \" + documentKey);\n-                // we can only trim if we're certain that the projected fields\n-                // aren't needed for evaluation\n-                \n-                if (gatherTimingDetails()) {\n-                    this.seekKeySource = new EvaluationTrackingNestedIterator(QuerySpan.Stage.DocumentSpecificTree, trackingSpan,\n-                                    new DocumentSpecificNestedIterator(documentKey), myEnvironment);\n-                } else {\n-                    this.seekKeySource = new DocumentSpecificNestedIterator(documentKey);\n-                }\n-            } else {\n-                this.seekKeySource = buildDocumentIterator(documentRange, range, columnFamilies, inclusive);\n-            }\n-            \n-            // Create the pipeline iterator for document aggregation and\n-            // evaluation within a thread pool\n-            PipelineIterator pipelineIter = PipelineFactory.createIterator(this.seekKeySource, getMaxEvaluationPipelines(), getMaxPipelineCachedResults(),\n-                            getSerialPipelineRequest(), querySpanCollector, trackingSpan, this, sourceForDeepCopies.deepCopy(myEnvironment), myEnvironment,\n-                            yield, yieldThresholdMs, columnFamilies, inclusive);\n-            \n-            pipelineIter.setCollectTimingDetails(collectTimingDetails);\n-            // TODO pipelineIter.setStatsdHostAndPort(statsdHostAndPort);\n-            \n-            pipelineIter.startPipeline();\n-            \n-            // gather Key,Document Entries from the pipelines\n-            Iterator<Entry<Key,Document>> pipelineDocuments = pipelineIter;\n-            \n-            if (log.isTraceEnabled()) {\n-                pipelineDocuments = Iterators.filter(pipelineDocuments, keyDocumentEntry -> {\n-                    log.trace(\"after pipeline, keyDocumentEntry:\" + keyDocumentEntry);\n-                    return true;\n-                });\n-            }\n-            \n-            // now apply the unique transform if requested\n-            UniqueTransform uniquify = getUniqueTransform();\n-            if (uniquify != null) {\n-                pipelineDocuments = Iterators.filter(pipelineDocuments, uniquify.getUniquePredicate());\n-            }\n-            \n-            // apply the grouping transform if requested and if the batch size is greater than zero\n-            // if the batch size is 0, then grouping is computed only on the web server\n-            GroupingTransform groupify = getGroupingTransform();\n-            if (groupify != null && this.groupFieldsBatchSize > 0) {\n-                \n-                pipelineDocuments = groupingTransform.getGroupingIterator(pipelineDocuments, this.groupFieldsBatchSize, this.yield);\n-                \n-                if (log.isTraceEnabled()) {\n-                    pipelineDocuments = Iterators.filter(pipelineDocuments, keyDocumentEntry -> {\n-                        log.trace(\"after grouping, keyDocumentEntry:\" + keyDocumentEntry);\n-                        return true;\n-                    });\n-                }\n-            }\n-            \n-            pipelineDocuments = Iterators.filter(\n-                            pipelineDocuments,\n-                            keyDocumentEntry -> {\n-                                // last chance before the documents are serialized\n-                                ActiveQueryLog.getInstance().get(getQueryId())\n-                                                .recordStats(keyDocumentEntry.getValue(), querySpanCollector.getCombinedQuerySpan(null));\n-                                // Always return true since we just want to record data in the ActiveQueryLog\n-                                return true;\n-                            });\n-            \n-            if (this.getReturnType() == ReturnType.kryo) {\n-                // Serialize the Document using Kryo\n-                this.serializedDocuments = Iterators.transform(pipelineDocuments, new KryoDocumentSerializer(isReducedResponse(), isCompressResults()));\n-            } else if (this.getReturnType() == ReturnType.writable) {\n-                // Use the Writable interface to serialize the Document\n-                this.serializedDocuments = Iterators.transform(pipelineDocuments, new WritableDocumentSerializer(isReducedResponse()));\n-            } else if (this.getReturnType() == ReturnType.tostring) {\n-                // Just return a toString() representation of the document\n-                this.serializedDocuments = Iterators.transform(pipelineDocuments, new ToStringDocumentSerializer(isReducedResponse()));\n-            } else {\n-                throw new IllegalArgumentException(\"Unknown return type of: \" + this.getReturnType());\n-            }\n-            \n-            if (log.isTraceEnabled()) {\n-                KryoDocumentDeserializer dser = new KryoDocumentDeserializer();\n-                this.serializedDocuments = Iterators.filter(this.serializedDocuments, keyValueEntry -> {\n-                    log.trace(\"after serializing, keyValueEntry:\" + dser.apply(keyValueEntry));\n-                    return true;\n-                });\n-            }\n-            \n-            // now add the result count to the keys (required when not sorting UIDs)\n-            // Cannot do this on document specific ranges as the count would place the keys outside the initial range\n-            if (!sortedUIDs && documentRange == null) {\n-                this.serializedDocuments = new ResultCountingIterator(serializedDocuments, resultCount, yield);\n-            } else if (this.sortedUIDs) {\n-                // we have sorted UIDs, so we can mask out the cq\n-                this.serializedDocuments = new KeyAdjudicator<>(serializedDocuments, yield);\n-            }\n-            \n-            // only add the final document tracking iterator which sends stats back to the client if collectTimingDetails is true\n-            if (collectTimingDetails) {\n-                // if there is no document to return, then add an empty document\n-                // to store the timing metadata\n-                this.serializedDocuments = new FinalDocumentTrackingIterator(querySpanCollector, trackingSpan, originalRange, this.serializedDocuments,\n-                                this.getReturnType(), this.isReducedResponse(), this.isCompressResults(), this.yield);\n-            }\n-            if (log.isTraceEnabled()) {\n-                KryoDocumentDeserializer dser = new KryoDocumentDeserializer();\n-                this.serializedDocuments = Iterators.filter(this.serializedDocuments, keyValueEntry -> {\n-                    log.debug(\"finally, considering:\" + dser.apply(keyValueEntry));\n-                    return true;\n-                });\n-            }\n-            \n-            // Determine if we have items to return\n-            prepareKeyValue(span);\n-        } catch (Exception e) {\n-            handleException(e);\n-        } finally {\n-            if (gatherTimingDetails() && trackingSpan != null && querySpanCollector != null) {\n-                querySpanCollector.addQuerySpan(trackingSpan);\n-            }\n-            QueryStatsDClient client = getStatsdClient();\n-            if (client != null) {\n-                client.flush();\n-            }\n-            ActiveQueryLog.getInstance().get(getQueryId()).endCall(this.originalRange, ActiveQuery.CallType.SEEK);\n-            if (this.key == null && this.value == null) {\n-                // no entries to return\n-                ActiveQueryLog.getInstance().remove(getQueryId(), this.originalRange);\n-            }\n-        }\n+  }\n+\n+  /**\n+   * Handle an exception returned from seek or next. This will silently ignore IterationInterruptedException as that happens when the underlying iterator was\n+   * interrupted because the client is no longer listening.\n+   *\n+   * @param e\n+   */\n+  private void handleException(Exception e) throws IOException {\n+    Throwable reason = e;\n+\n+    // We need to pass IOException, IteratorInterruptedException, and TabletClosedExceptions up to the Tablet as they are\n+    // handled specially to ensure that the client will retry the scan elsewhere\n+    IOException ioe = null;\n+    IterationInterruptedException iie = null;\n+    TabletClosedException tce = null;\n+    if (reason instanceof IOException) {\n+      ioe = (IOException) reason;\n     }\n-    \n-    /**\n-     * Handle an exception returned from seek or next. This will silently ignore IterationInterruptedException as that happens when the underlying iterator was\n-     * interrupted because the client is no longer listening.\n-     * \n-     * @param e\n-     */\n-    private void handleException(Exception e) throws IOException {\n-        Throwable reason = e;\n-        \n-        // We need to pass IOException, IteratorInterruptedException, and TabletClosedExceptions up to the Tablet as they are\n-        // handled specially to ensure that the client will retry the scan elsewhere\n-        IOException ioe = null;\n-        IterationInterruptedException iie = null;\n-        TabletClosedException tce = null;\n-        if (reason instanceof IOException)\n-            ioe = (IOException) reason;\n-        if (reason instanceof IterationInterruptedException)\n-            iie = (IterationInterruptedException) reason;\n-        if (reason instanceof TabletClosedException)\n-            tce = (TabletClosedException) reason;\n-        \n-        int depth = 1;\n-        while (iie == null && reason.getCause() != null && reason.getCause() != reason && depth < 100) {\n-            reason = reason.getCause();\n-            if (reason instanceof IOException)\n-                ioe = (IOException) reason;\n-            if (reason instanceof IterationInterruptedException)\n-                iie = (IterationInterruptedException) reason;\n-            if (reason instanceof TabletClosedException)\n-                tce = (TabletClosedException) reason;\n-            depth++;\n-        }\n-        \n-        // NOTE: Only logging debug here because the Tablet/LookupTask will log the exception as a WARN if we actually have an problem here\n-        if (iie != null) {\n-            // exit gracefully if we are yielding as an iie is expected in this case\n-            if ((this.yield != null) && this.yield.hasYielded()) {\n-                log.debug(\"Query yielded \" + queryId);\n-            } else {\n-                log.debug(\"Query interrupted \" + queryId, e);\n-                throw iie;\n-            }\n-        } else if (tce != null) {\n-            log.debug(\"Query tablet closed \" + queryId, e);\n-            throw tce;\n-        } else if (ioe != null) {\n-            log.debug(\"Query io exception \" + queryId, e);\n-            throw ioe;\n-        } else {\n-            log.error(\"Failure for query \" + queryId, e);\n-            throw new RuntimeException(\"Failure for query \" + queryId, e);\n-        }\n+    if (reason instanceof IterationInterruptedException) {\n+      iie = (IterationInterruptedException) reason;\n     }\n-    \n-    /**\n-     * Build the document iterator\n-     * \n-     * @param documentRange\n-     * @param seekRange\n-     * @param columnFamilies\n-     * @param inclusive\n-     * @return\n-     * @throws IOException\n-     */\n-    protected NestedIterator<Key> buildDocumentIterator(Range documentRange, Range seekRange, Collection<ByteSequence> columnFamilies, boolean inclusive)\n-                    throws IOException, ConfigException, InstantiationException, IllegalAccessException {\n-        NestedIterator<Key> docIter = null;\n-        if (log.isTraceEnabled())\n-            log.trace(\"Batched queries is \" + batchedQueries);\n-        if (batchedQueries >= 1) {\n-            List<NestedQuery<Key>> nests = Lists.newArrayList();\n-            \n-            for (Entry<Range,String> queries : batchStack) {\n-                \n-                Range myRange = queries.getKey();\n-                \n-                if (log.isTraceEnabled())\n-                    log.trace(\"Adding \" + myRange + \" from seekrange \" + seekRange);\n-                \n-                /*\n-                 * Only perform the following checks if start key is not infinite and document range is specified\n-                 */\n-                if (null != seekRange && !seekRange.isInfiniteStartKey()) {\n-                    Key seekStartKey = seekRange.getStartKey();\n-                    Key myStartKey = myRange.getStartKey();\n-                    \n-                    /*\n-                     * if our seek key is greater than our start key we can skip this batched query. myStartKey.compareTo(seekStartKey) must be <= 0, which\n-                     * means that startKey must be greater than or equal be seekStartKey\n-                     */\n-                    if (null != myStartKey && null != seekStartKey && !seekRange.contains(myStartKey)) {\n-                        \n-                        if (log.isTraceEnabled()) {\n-                            log.trace(\"skipping \" + myRange);\n-                        }\n-                        \n-                        continue;\n-                    }\n-                }\n-                \n-                JexlArithmetic myArithmetic;\n-                if (arithmetic instanceof StatefulArithmetic) {\n-                    myArithmetic = ((StatefulArithmetic) arithmetic).clone();\n-                } else {\n-                    myArithmetic = new DefaultArithmetic();\n-                }\n-                \n-                // Parse the query\n-                ASTJexlScript myScript = null;\n-                JexlEvaluation eval = null;\n-                try {\n-                    \n-                    myScript = JexlASTHelper.parseJexlQuery(queries.getValue());\n-                    eval = new JexlEvaluation(queries.getValue(), myArithmetic);\n-                    \n-                } catch (Exception e) {\n-                    throw new IOException(\"Could not parse the JEXL query: '\" + this.getQuery() + \"'\", e);\n-                }\n-                // If we had an event-specific range previously, we need to\n-                // reset it back\n-                // to the source we created during init\n-                NestedIterator<Key> subDocIter = getOrSetKeySource(myRange, myScript);\n-                \n-                if (log.isTraceEnabled()) {\n-                    log.trace(\"Using init()'ialized source: \" + subDocIter.getClass().getName());\n-                }\n-                \n-                if (gatherTimingDetails()) {\n-                    subDocIter = new EvaluationTrackingNestedIterator(QuerySpan.Stage.FieldIndexTree, trackingSpan, subDocIter, myEnvironment);\n-                }\n-                \n-                // Seek() the boolean logic stuff\n-                ((SeekableIterator) subDocIter).seek(myRange, columnFamilies, inclusive);\n-                \n-                NestedQuery<Key> nestedQueryObj = new NestedQuery<>();\n-                nestedQueryObj.setQuery(queries.getValue());\n-                nestedQueryObj.setIterator(subDocIter);\n-                nestedQueryObj.setQueryScript(myScript);\n-                nestedQueryObj.setEvaluation(eval);\n-                nestedQueryObj.setRange(queries.getKey());\n-                nests.add(nestedQueryObj);\n-            }\n-            \n-            docIter = new NestedQueryIterator<>(nests);\n-            \n-            // now lets start off the nested iterator\n-            docIter.initialize();\n-            \n-            initKeySource = docIter;\n-            \n-        } else {\n-            // If we had an event-specific range previously, we need to reset it back\n-            // to the source we created during init\n-            docIter = getOrSetKeySource(documentRange, script);\n-            \n-            initKeySource = docIter;\n-            \n-            if (log.isTraceEnabled()) {\n-                log.trace(\"Using init()'ialized source: \" + this.initKeySource.getClass().getName());\n-            }\n-            \n-            if (gatherTimingDetails()) {\n-                docIter = new EvaluationTrackingNestedIterator(QuerySpan.Stage.FieldIndexTree, trackingSpan, docIter, myEnvironment);\n-            }\n-            \n-            // Seek() the boolean logic stuff\n-            ((SeekableIterator) docIter).seek(range, columnFamilies, inclusive);\n-            \n-            // now lets start off the nested iterator\n-            docIter.initialize();\n-        }\n-        \n-        return docIter;\n+    if (reason instanceof TabletClosedException) {\n+      tce = (TabletClosedException) reason;\n     }\n-    \n-    /**\n-     * There was a request to create a serial pipeline. The factory may not choose to honor this.\n-     * \n-     * @return\n-     */\n-    private boolean getSerialPipelineRequest() {\n-        return serialEvaluationPipeline;\n+\n+    int depth = 1;\n+    while (iie == null && reason.getCause() != null && reason.getCause() != reason && depth < 100) {\n+      reason = reason.getCause();\n+      if (reason instanceof IOException) {\n+        ioe = (IOException) reason;\n+      }\n+      if (reason instanceof IterationInterruptedException) {\n+        iie = (IterationInterruptedException) reason;\n+      }\n+      if (reason instanceof TabletClosedException) {\n+        tce = (TabletClosedException) reason;\n+      }\n+      depth++;\n     }\n-    \n-    /**\n-     * A routine which should always be used to create deep copies of the source. This ensures that we are thread safe when doing these copies.\n-     * \n-     * @return\n-     */\n-    public SortedKeyValueIterator<Key,Value> getSourceDeepCopy() {\n-        SortedKeyValueIterator<Key,Value> sourceDeepCopy;\n-        synchronized (sourceForDeepCopies) {\n-            sourceDeepCopy = sourceForDeepCopies.deepCopy(this.myEnvironment);\n-        }\n-        return sourceDeepCopy;\n+\n+    // NOTE: Only logging debug here because the Tablet/LookupTask will log the exception as a WARN if we actually have an problem here\n+    if (iie != null) {\n+      // exit gracefully if we are yielding as an iie is expected in this case\n+      if ((this.yield != null) && this.yield.hasYielded()) {\n+        log.debug(\"Query yielded \" + queryId);\n+      } else {\n+        log.debug(\"Query interrupted \" + queryId, e);\n+        throw iie;\n+      }\n+    } else if (tce != null) {\n+      log.debug(\"Query tablet closed \" + queryId, e);\n+      throw tce;\n+    } else if (ioe != null) {\n+      log.debug(\"Query io exception \" + queryId, e);\n+      throw ioe;\n+    } else {\n+      log.error(\"Failure for query \" + queryId, e);\n+      throw new RuntimeException(\"Failure for query \" + queryId, e);\n     }\n-    \n-    /**\n-     * Returns the elements of {@code unfiltered} that satisfy a predicate. This is used instead of the google commons Iterators.filter to create a non-stateful\n-     * filtering iterator.\n-     */\n-    public static <T> UnmodifiableIterator<T> statelessFilter(final Iterator<T> unfiltered, final Predicate<? super T> predicate) {\n-        checkNotNull(unfiltered);\n-        checkNotNull(predicate);\n-        return new UnmodifiableIterator<T>() {\n-            private T next;\n-            \n-            protected T computeNext() {\n-                while (unfiltered.hasNext()) {\n-                    T element = unfiltered.next();\n-                    if (predicate.apply(element)) {\n-                        return element;\n-                    }\n-                }\n-                return null;\n-            }\n-            \n-            @Override\n-            public final boolean hasNext() {\n-                if (next == null) {\n-                    next = computeNext();\n-                }\n-                return (next != null);\n-            }\n-            \n-            @Override\n-            public T next() {\n-                T next = this.next;\n-                this.next = null;\n-                return next;\n-            }\n-        };\n+  }\n+\n+  /**\n+   * Build the document iterator\n+   *\n+   * @param documentRange\n+   * @param seekRange\n+   * @param columnFamilies\n+   * @param inclusive\n+   * @return\n+   * @throws IOException\n+   */\n+  protected NestedIterator<Key> buildDocumentIterator(Range documentRange, Range seekRange,\n+                                                      Collection<ByteSequence> columnFamilies, boolean inclusive)\n+      throws IOException, ConfigException, InstantiationException, IllegalAccessException {\n+    NestedIterator<Key> docIter = null;\n+    if (log.isTraceEnabled()) {\n+      log.trace(\"Batched queries is \" + batchedQueries);\n     }\n-    \n-    /**\n-     * Create the pipeline. It is very important that this pipeline can handle resetting the bottom iterator with a new value. This means that hasNext() needs\n-     * to call the next iterator. The only state that can be maintained is the next value ready after hasNext() has been called. Once next returns the value,\n-     * the next hasNext() call must call the next iterator again. So for example Iterators.filter() cannot be used as it uses a google commons AbstractIterator\n-     * that maintains an iterator state (failed, ready, done); use statelessFilter above instead.\n-     *\n-     * @param deepSourceCopy\n-     * @param documentSpecificSource\n-     * @return iterator of keys and values\n-     */\n-    public Iterator<Entry<Key,Document>> createDocumentPipeline(SortedKeyValueIterator<Key,Value> deepSourceCopy,\n-                    final NestedQueryIterator<Key> documentSpecificSource, Collection<ByteSequence> columnFamilies, boolean inclusive,\n-                    QuerySpanCollector querySpanCollector) {\n-        \n-        QuerySpan trackingSpan = null;\n-        if (gatherTimingDetails()) {\n-            trackingSpan = new QuerySpan(getStatsdClient());\n-        }\n+    if (batchedQueries >= 1) {\n+      List<NestedQuery<Key>> nests = Lists.newArrayList();\n+\n+      for (Entry<Range, String> queries : batchStack) {\n+\n+        Range myRange = queries.getKey();\n+\n         if (log.isTraceEnabled()) {\n-            log.trace(\"createDocumentPipeline\");\n+          log.trace(\"Adding \" + myRange + \" from seekrange \" + seekRange);\n         }\n-        final Function<Entry<Key,Document>,Entry<DocumentData,Document>> docMapper;\n-        \n-        if (isFieldIndexSatisfyingQuery()) {\n+\n+        /*\n+         * Only perform the following checks if start key is not infinite and document range is specified\n+         */\n+        if (null != seekRange && !seekRange.isInfiniteStartKey()) {\n+          Key seekStartKey = seekRange.getStartKey();\n+          Key myStartKey = myRange.getStartKey();\n+\n+          /*\n+           * if our seek key is greater than our start key we can skip this batched query. myStartKey.compareTo(seekStartKey) must be <= 0, which\n+           * means that startKey must be greater than or equal be seekStartKey\n+           */\n+          if (null != myStartKey && null != seekStartKey && !seekRange.contains(myStartKey)) {\n+\n             if (log.isTraceEnabled()) {\n-                log.trace(\"isFieldIndexSatisfyingQuery\");\n-            }\n-            docMapper = new Function<Entry<Key,Document>,Entry<DocumentData,Document>>() {\n-                @Nullable\n-                @Override\n-                public Entry<DocumentData,Document> apply(@Nullable Entry<Key,Document> input) {\n-                    \n-                    Entry<DocumentData,Document> entry = null;\n-                    if (input != null) {\n-                        entry = Maps.immutableEntry(new DocumentData(input.getKey(), Collections.singleton(input.getKey()), Collections.EMPTY_LIST),\n-                                        input.getValue());\n-                    }\n-                    return entry;\n-                }\n-            };\n-        } else {\n-            docMapper = new KeyToDocumentData(deepSourceCopy, myEnvironment, documentOptions, super.equality, getEvaluationFilter(),\n-                            this.includeHierarchyFields, this.includeHierarchyFields);\n-        }\n-        \n-        Iterator<Entry<DocumentData,Document>> sourceIterator = Iterators.transform(documentSpecificSource, from -> {\n-            Entry<Key,Document> entry = Maps.immutableEntry(from, documentSpecificSource.document());\n-            return docMapper.apply(entry);\n-        });\n-        \n-        // Take the document Keys and transform it into Entry<Key,Document>,\n-        // removing Attributes for this Document\n-        // which do not fall within the expected time range\n-        Iterator<Entry<Key,Document>> documents = null;\n-        Aggregation a = new Aggregation(this.getTimeFilter(), this.typeMetadataWithNonIndexed, compositeMetadata, this.isIncludeGroupingContext(),\n-                        this.includeRecordId, this.disableIndexOnlyDocuments(), getEvaluationFilter(), isTrackSizes());\n-        if (gatherTimingDetails()) {\n-            documents = Iterators.transform(sourceIterator, new EvaluationTrackingFunction<>(QuerySpan.Stage.Aggregation, trackingSpan, a));\n-        } else {\n-            documents = Iterators.transform(sourceIterator, a);\n-        }\n-        \n-        // Inject the data type as a field if the user requested it\n-        if (this.includeDatatype) {\n-            if (gatherTimingDetails()) {\n-                documents = Iterators.transform(documents, new EvaluationTrackingFunction<>(QuerySpan.Stage.DataTypeAsField, trackingSpan, new DataTypeAsField(\n-                                this.datatypeKey)));\n-            } else {\n-                documents = Iterators.transform(documents, new DataTypeAsField(this.datatypeKey));\n-            }\n-        }\n-        \n-        // Inject the document permutations if required\n-        if (!this.getDocumentPermutations().isEmpty()) {\n-            if (gatherTimingDetails()) {\n-                documents = Iterators.transform(documents, new EvaluationTrackingFunction<>(QuerySpan.Stage.DocumentPermutation, trackingSpan,\n-                                new DocumentPermutation.DocumentPermutationAggregation(this.getDocumentPermutations())));\n-            } else {\n-                documents = Iterators.transform(documents, new DocumentPermutation.DocumentPermutationAggregation(this.getDocumentPermutations()));\n+              log.trace(\"skipping \" + myRange);\n             }\n+\n+            continue;\n+          }\n         }\n-        \n-        if (gatherTimingDetails()) {\n-            documents = new EvaluationTrackingIterator(QuerySpan.Stage.DocumentEvaluation, trackingSpan, getEvaluation(documentSpecificSource, deepSourceCopy,\n-                            documents, compositeMetadata, typeMetadataWithNonIndexed, columnFamilies, inclusive));\n+\n+        JexlArithmetic myArithmetic;\n+        if (arithmetic instanceof StatefulArithmetic) {\n+          myArithmetic = ((StatefulArithmetic) arithmetic).clone();\n         } else {\n-            documents = getEvaluation(documentSpecificSource, deepSourceCopy, documents, compositeMetadata, typeMetadataWithNonIndexed, columnFamilies,\n-                            inclusive);\n-        }\n-        \n-        // a hook to allow mapping the document such as with the TLD or Parent\n-        // query logics\n-        // or if the document was not aggregated in the first place because the\n-        // field index fields completely satisfied the query\n-        documents = mapDocument(deepSourceCopy, documents, compositeMetadata);\n-        \n-        // apply any configured post processing\n-        documents = getPostProcessingChain(documents);\n-        if (gatherTimingDetails()) {\n-            documents = new EvaluationTrackingIterator(QuerySpan.Stage.PostProcessing, trackingSpan, documents);\n-        }\n-        \n-        // Filter out masked values if requested\n-        if (this.filterMaskedValues) {\n-            MaskedValueFilterInterface mvfi = MaskedValueFilterFactory.get(this.isIncludeGroupingContext(), this.isReducedResponse());\n-            if (gatherTimingDetails()) {\n-                documents = Iterators.transform(documents, new EvaluationTrackingFunction<>(QuerySpan.Stage.MaskedValueFilter, trackingSpan, mvfi));\n-            } else {\n-                documents = Iterators.transform(documents, mvfi);\n-            }\n+          myArithmetic = new DefaultArithmetic();\n         }\n-        \n-        // now filter the attributes to those with the keep flag set true\n-        if (gatherTimingDetails()) {\n-            documents = Iterators.transform(documents, new EvaluationTrackingFunction<>(QuerySpan.Stage.AttributeKeepFilter, trackingSpan,\n-                            new AttributeKeepFilter<>()));\n-        } else {\n-            documents = Iterators.transform(documents, new AttributeKeepFilter<>());\n+\n+        // Parse the query\n+        ASTJexlScript myScript = null;\n+        JexlEvaluation eval = null;\n+        try {\n+\n+          myScript = JexlASTHelper.parseJexlQuery(queries.getValue());\n+          eval = new JexlEvaluation(queries.getValue(), myArithmetic);\n+\n+        } catch (Exception e) {\n+          throw new IOException(\"Could not parse the JEXL query: '\" + this.getQuery() + \"'\", e);\n         }\n-        \n-        // Project fields using a whitelist or a blacklist before serialization\n-        if (this.projectResults) {\n-            if (gatherTimingDetails()) {\n-                documents = Iterators.transform(documents, new EvaluationTrackingFunction<>(QuerySpan.Stage.DocumentProjection, trackingSpan, getProjection()));\n-            } else {\n-                documents = Iterators.transform(documents, getProjection());\n-            }\n+        // If we had an event-specific range previously, we need to\n+        // reset it back\n+        // to the source we created during init\n+        NestedIterator<Key> subDocIter = getOrSetKeySource(myRange, myScript);\n+\n+        if (log.isTraceEnabled()) {\n+          log.trace(\"Using init()'ialized source: \" + subDocIter.getClass().getName());\n         }\n-        \n-        // remove the composite entries\n-        documents = Iterators.transform(documents, this.getCompositeProjection());\n-        \n-        // Filter out any Documents which are empty (e.g. due to attribute\n-        // projection or visibility filtering)\n+\n         if (gatherTimingDetails()) {\n-            documents = statelessFilter(documents, new EvaluationTrackingPredicate<>(QuerySpan.Stage.EmptyDocumentFilter, trackingSpan,\n-                            new EmptyDocumentFilter()));\n-            documents = Iterators\n-                            .transform(documents, new EvaluationTrackingFunction<>(QuerySpan.Stage.DocumentMetadata, trackingSpan, new DocumentMetadata()));\n-        } else {\n-            documents = statelessFilter(documents, new EmptyDocumentFilter());\n-            documents = Iterators.transform(documents, new DocumentMetadata());\n-        }\n-        \n-        if (!this.limitFieldsMap.isEmpty()) {\n-            if (gatherTimingDetails()) {\n-                documents = Iterators.transform(documents,\n-                                new EvaluationTrackingFunction<>(QuerySpan.Stage.LimitFields, trackingSpan, new LimitFields(this.getLimitFieldsMap())));\n-            } else {\n-                documents = Iterators.transform(documents, new LimitFields(this.getLimitFieldsMap()));\n-            }\n+          subDocIter = new EvaluationTrackingNestedIterator(QuerySpan.Stage.FieldIndexTree, trackingSpan, subDocIter,\n+              myEnvironment);\n         }\n-        \n-        // do I need to remove the grouping context I added above?\n-        if (groupingContextAddedByMe) {\n-            if (gatherTimingDetails()) {\n-                documents = Iterators.transform(documents, new EvaluationTrackingFunction<>(QuerySpan.Stage.RemoveGroupingContext, trackingSpan,\n-                                new RemoveGroupingContext()));\n-            } else {\n-                documents = Iterators.transform(documents, new RemoveGroupingContext());\n-            }\n+\n+        // Seek() the boolean logic stuff\n+        ((SeekableIterator) subDocIter).seek(myRange, columnFamilies, inclusive);\n+\n+        NestedQuery<Key> nestedQueryObj = new NestedQuery<>();\n+        nestedQueryObj.setQuery(queries.getValue());\n+        nestedQueryObj.setIterator(subDocIter);\n+        nestedQueryObj.setQueryScript(myScript);\n+        nestedQueryObj.setEvaluation(eval);\n+        nestedQueryObj.setRange(queries.getKey());\n+        nests.add(nestedQueryObj);\n+      }\n+\n+      docIter = new NestedQueryIterator<>(nests);\n+\n+      // now lets start off the nested iterator\n+      docIter.initialize();\n+\n+      initKeySource = docIter;\n+\n+    } else {\n+      // If we had an event-specific range previously, we need to reset it back\n+      // to the source we created during init\n+      docIter = getOrSetKeySource(documentRange, script);\n+\n+      initKeySource = docIter;\n+\n+      if (log.isTraceEnabled()) {\n+        log.trace(\"Using init()'ialized source: \" + this.initKeySource.getClass().getName());\n+      }\n+\n+      if (gatherTimingDetails()) {\n+        docIter =\n+            new EvaluationTrackingNestedIterator(QuerySpan.Stage.FieldIndexTree, trackingSpan, docIter, myEnvironment);\n+      }\n+\n+      // Seek() the boolean logic stuff\n+      ((SeekableIterator) docIter).seek(range, columnFamilies, inclusive);\n+\n+      // now lets start off the nested iterator\n+      docIter.initialize();\n+    }\n+\n+    return docIter;\n+  }\n+\n+  /**\n+   * There was a request to create a serial pipeline. The factory may not choose to honor this.\n+   *\n+   * @return\n+   */\n+  private boolean getSerialPipelineRequest() {\n+    return serialEvaluationPipeline;\n+  }\n+\n+  /**\n+   * A routine which should always be used to create deep copies of the source. This ensures that we are thread safe when doing these copies.\n+   *\n+   * @return\n+   */\n+  public SortedKeyValueIterator<Key, Value> getSourceDeepCopy() {\n+    SortedKeyValueIterator<Key, Value> sourceDeepCopy;\n+    synchronized (sourceForDeepCopies) {\n+      sourceDeepCopy = sourceForDeepCopies.deepCopy(this.myEnvironment);\n+    }\n+    return sourceDeepCopy;\n+  }\n+\n+  /**\n+   * Returns the elements of {@code unfiltered} that satisfy a predicate. This is used instead of the google commons Iterators.filter to create a non-stateful\n+   * filtering iterator.\n+   */\n+  public static <T> UnmodifiableIterator<T> statelessFilter(final Iterator<T> unfiltered,\n+                                                            final Predicate<? super T> predicate) {\n+    checkNotNull(unfiltered);\n+    checkNotNull(predicate);\n+    return new UnmodifiableIterator<T>() {\n+      private T next;\n+\n+      protected T computeNext() {\n+        while (unfiltered.hasNext()) {\n+          T element = unfiltered.next();\n+          if (predicate.apply(element)) {\n+            return element;\n+          }\n         }\n-        \n-        // only add the pipeline query span collection iterator which will cache metrics with each document if collectTimingDetails is true\n-        if (collectTimingDetails) {\n-            // if there is not a result, then add the trackingSpan to the\n-            // QuerySpanCollector\n-            // if there was a result, then the metrics from the trackingSpan\n-            // will be added here\n-            documents = new PipelineQuerySpanCollectionIterator(querySpanCollector, trackingSpan, documents);\n+        return null;\n+      }\n+\n+      @Override\n+      public final boolean hasNext() {\n+        if (next == null) {\n+          next = computeNext();\n         }\n-        \n-        return documents;\n+        return (next != null);\n+      }\n+\n+      @Override\n+      public T next() {\n+        T next = this.next;\n+        this.next = null;\n+        return next;\n+      }\n+    };\n+  }\n+\n+  /**\n+   * Create the pipeline. It is very important that this pipeline can handle resetting the bottom iterator with a new value. This means that hasNext() needs\n+   * to call the next iterator. The only state that can be maintained is the next value ready after hasNext() has been called. Once next returns the value,\n+   * the next hasNext() call must call the next iterator again. So for example Iterators.filter() cannot be used as it uses a google commons AbstractIterator\n+   * that maintains an iterator state (failed, ready, done); use statelessFilter above instead.\n+   *\n+   * @param deepSourceCopy\n+   * @param documentSpecificSource\n+   * @return iterator of keys and values\n+   */\n+  public Iterator<Entry<Key, Document>> createDocumentPipeline(SortedKeyValueIterator<Key, Value> deepSourceCopy,\n+                                                               final NestedQueryIterator<Key> documentSpecificSource,\n+                                                               Collection<ByteSequence> columnFamilies,\n+                                                               boolean inclusive,\n+                                                               QuerySpanCollector querySpanCollector) {\n+\n+    QuerySpan trackingSpan = null;\n+    if (gatherTimingDetails()) {\n+      trackingSpan = new QuerySpan(getStatsdClient());\n     }\n-    \n-    protected Iterator<Entry<Key,Document>> getEvaluation(SortedKeyValueIterator<Key,Value> sourceDeepCopy, Iterator<Entry<Key,Document>> documents,\n-                    CompositeMetadata compositeMetadata, TypeMetadata typeMetadataForEval, Collection<ByteSequence> columnFamilies, boolean inclusive) {\n-        return getEvaluation(null, sourceDeepCopy, documents, compositeMetadata, typeMetadataForEval, columnFamilies, inclusive);\n+    if (log.isTraceEnabled()) {\n+      log.trace(\"createDocumentPipeline\");\n     }\n-    \n-    protected Iterator<Entry<Key,Document>> getEvaluation(NestedQueryIterator<Key> documentSource, SortedKeyValueIterator<Key,Value> sourceDeepCopy,\n-                    Iterator<Entry<Key,Document>> documents, CompositeMetadata compositeMetadata, TypeMetadata typeMetadataForEval,\n-                    Collection<ByteSequence> columnFamilies, boolean inclusive) {\n-        // Filter the Documents by testing them against the JEXL query\n-        if (!this.disableEvaluation) {\n-            \n-            JexlEvaluation jexlEvaluationFunction = getJexlEvaluation(documentSource);\n-            Collection<String> variables = null;\n-            if (null != documentSource && null != documentSource.getQuery()) {\n-                \n-                variables = VariableNameVisitor.parseQuery(jexlEvaluationFunction.parse(documentSource.getQuery()));\n-            } else {\n-                variables = VariableNameVisitor.parseQuery(jexlEvaluationFunction.parse(query));\n-            }\n-            \n-            final Iterator<Tuple2<Key,Document>> tupleItr = Iterators.transform(documents, new EntryToTuple<>());\n-            \n-            // get the function we use for the tf functionality. Note we are\n-            // getting an additional source deep copy for this function\n-            final Iterator<Tuple3<Key,Document,Map<String,Object>>> itrWithContext;\n-            if (this.isTermFrequenciesRequired()) {\n-                Function<Tuple2<Key,Document>,Tuple3<Key,Document,Map<String,Object>>> tfFunction;\n-                tfFunction = TFFactory.getFunction(getScript(documentSource), getContentExpansionFields(), getTermFrequencyFields(), this.getTypeMetadata(),\n-                                super.equality, getEvaluationFilter(), sourceDeepCopy.deepCopy(myEnvironment));\n-                \n-                itrWithContext = TraceIterators.transform(tupleItr, tfFunction, \"Term Frequency Lookup\");\n-            } else {\n-                itrWithContext = Iterators.transform(tupleItr, new EmptyContext<>());\n-            }\n-            \n-            try {\n-                IteratorBuildingVisitor iteratorBuildingVisitor = createIteratorBuildingVisitor(getDocumentRange(documentSource), false, this.sortedUIDs);\n-                Multimap<String,JexlNode> delayedNonEventFieldMap = DelayedNonEventSubTreeVisitor.getDelayedNonEventFieldMap(iteratorBuildingVisitor, script,\n-                                getNonEventFields());\n-                \n-                IndexOnlyContextCreatorBuilder contextCreatorBuilder = new IndexOnlyContextCreatorBuilder().setSource(sourceDeepCopy)\n-                                .setRange(getDocumentRange(documentSource)).setTypeMetadata(typeMetadataForEval).setCompositeMetadata(compositeMetadata)\n-                                .setOptions(this).setVariables(variables).setIteratorBuildingVisitor(iteratorBuildingVisitor)\n-                                .setDelayedNonEventFieldMap(delayedNonEventFieldMap).setEquality(equality).setColumnFamilies(columnFamilies)\n-                                .setInclusive(inclusive).setComparatorFactory(this);\n-                final IndexOnlyContextCreator contextCreator = contextCreatorBuilder.build();\n-                \n-                if (exceededOrEvaluationCache != null) {\n-                    contextCreator.addAdditionalEntries(exceededOrEvaluationCache);\n-                }\n-                \n-                final Iterator<Tuple3<Key,Document,DatawaveJexlContext>> itrWithDatawaveJexlContext = Iterators.transform(itrWithContext, contextCreator);\n-                Iterator<Tuple3<Key,Document,DatawaveJexlContext>> matchedDocuments = statelessFilter(itrWithDatawaveJexlContext, jexlEvaluationFunction);\n-                if (log.isTraceEnabled()) {\n-                    log.trace(\"arithmetic:\" + arithmetic + \" range:\" + getDocumentRange(documentSource) + \", thread:\" + Thread.currentThread());\n-                }\n-                return Iterators.transform(matchedDocuments, new TupleToEntry<>());\n-            } catch (InstantiationException | MalformedURLException | IllegalAccessException | ConfigException e) {\n-                throw new IllegalStateException(\"Could not perform delayed index only evaluation\", e);\n-            }\n-        } else if (log.isTraceEnabled()) {\n-            log.trace(\"Evaluation is disabled, not instantiating Jexl evaluation logic\");\n+    final Function<Entry<Key, Document>, Entry<DocumentData, Document>> docMapper;\n+\n+    if (isFieldIndexSatisfyingQuery()) {\n+      if (log.isTraceEnabled()) {\n+        log.trace(\"isFieldIndexSatisfyingQuery\");\n+      }\n+      docMapper = new Function<Entry<Key, Document>, Entry<DocumentData, Document>>() {\n+        @Nullable\n+        @Override\n+        public Entry<DocumentData, Document> apply(@Nullable Entry<Key, Document> input) {\n+\n+          Entry<DocumentData, Document> entry = null;\n+          if (input != null) {\n+            entry = Maps.immutableEntry(\n+                new DocumentData(input.getKey(), Collections.singleton(input.getKey()), Collections.EMPTY_LIST),\n+                input.getValue());\n+          }\n+          return entry;\n         }\n-        return documents;\n+      };\n+    } else {\n+      docMapper =\n+          new KeyToDocumentData(deepSourceCopy, myEnvironment, documentOptions, super.equality, getEvaluationFilter(),\n+              this.includeHierarchyFields, this.includeHierarchyFields);\n     }\n-    \n-    private Range getDocumentRange(NestedQueryIterator<Key> documentSource) {\n-        if (null == documentSource) {\n-            return range;\n-        }\n-        NestedQuery<Key> nestedQuery = documentSource.getNestedQuery();\n-        if (null == nestedQuery) {\n-            return range;\n-        } else {\n-            return nestedQuery.getRange();\n-        }\n+\n+    Iterator<Entry<DocumentData, Document>> sourceIterator = Iterators.transform(documentSpecificSource, from -> {\n+      Entry<Key, Document> entry = Maps.immutableEntry(from, documentSpecificSource.document());\n+      return docMapper.apply(entry);\n+    });\n+\n+    // Take the document Keys and transform it into Entry<Key,Document>,\n+    // removing Attributes for this Document\n+    // which do not fall within the expected time range\n+    Iterator<Entry<Key, Document>> documents = null;\n+    Aggregation a = new Aggregation(this.getTimeFilter(), this.typeMetadataWithNonIndexed, compositeMetadata,\n+        this.isIncludeGroupingContext(),\n+        this.includeRecordId, this.disableIndexOnlyDocuments(), getEvaluationFilter(), isTrackSizes());\n+    if (gatherTimingDetails()) {\n+      documents = Iterators\n+          .transform(sourceIterator, new EvaluationTrackingFunction<>(QuerySpan.Stage.Aggregation, trackingSpan, a));\n+    } else {\n+      documents = Iterators.transform(sourceIterator, a);\n     }\n-    \n-    protected JexlEvaluation getJexlEvaluation(NestedQueryIterator<Key> documentSource) {\n-        \n-        if (null == documentSource) {\n-            return new JexlEvaluation(query, getArithmetic());\n-        }\n-        JexlEvaluation jexlEvaluationFunction = null;\n-        NestedQuery<Key> nestedQuery = documentSource.getNestedQuery();\n-        if (null == nestedQuery) {\n-            jexlEvaluationFunction = new JexlEvaluation(query, getArithmetic());\n-        } else {\n-            jexlEvaluationFunction = nestedQuery.getEvaluation();\n-            if (null == jexlEvaluationFunction) {\n-                return new JexlEvaluation(query, getArithmetic());\n-            }\n-        }\n-        return jexlEvaluationFunction;\n+\n+    // Inject the data type as a field if the user requested it\n+    if (this.includeDatatype) {\n+      if (gatherTimingDetails()) {\n+        documents = Iterators.transform(documents,\n+            new EvaluationTrackingFunction<>(QuerySpan.Stage.DataTypeAsField, trackingSpan, new DataTypeAsField(\n+                this.datatypeKey)));\n+      } else {\n+        documents = Iterators.transform(documents, new DataTypeAsField(this.datatypeKey));\n+      }\n     }\n-    \n-    @Override\n-    public JexlArithmetic getArithmetic() {\n-        JexlArithmetic myArithmetic = this.arithmetic;\n-        if (myArithmetic instanceof StatefulArithmetic) {\n-            myArithmetic = ((StatefulArithmetic) arithmetic).clone();\n-        }\n-        return myArithmetic;\n+\n+    // Inject the document permutations if required\n+    if (!this.getDocumentPermutations().isEmpty()) {\n+      if (gatherTimingDetails()) {\n+        documents = Iterators\n+            .transform(documents, new EvaluationTrackingFunction<>(QuerySpan.Stage.DocumentPermutation, trackingSpan,\n+                new DocumentPermutation.DocumentPermutationAggregation(this.getDocumentPermutations())));\n+      } else {\n+        documents = Iterators.transform(documents,\n+            new DocumentPermutation.DocumentPermutationAggregation(this.getDocumentPermutations()));\n+      }\n     }\n-    \n-    protected ASTJexlScript getScript(NestedQueryIterator<Key> documentSource) {\n-        if (null == documentSource) {\n-            return script;\n-        }\n-        NestedQuery<Key> query = documentSource.getNestedQuery();\n-        if (null == query) {\n-            return script;\n-        } else {\n-            ASTJexlScript rangeScript = query.getScript();\n-            if (null == rangeScript) {\n-                return script;\n-            }\n-            return rangeScript;\n-        }\n+\n+    if (gatherTimingDetails()) {\n+      documents = new EvaluationTrackingIterator(QuerySpan.Stage.DocumentEvaluation, trackingSpan,\n+          getEvaluation(documentSpecificSource, deepSourceCopy,\n+              documents, compositeMetadata, typeMetadataWithNonIndexed, columnFamilies, inclusive));\n+    } else {\n+      documents = getEvaluation(documentSpecificSource, deepSourceCopy, documents, compositeMetadata,\n+          typeMetadataWithNonIndexed, columnFamilies,\n+          inclusive);\n     }\n-    \n-    protected Iterator<Entry<Key,Document>> mapDocument(SortedKeyValueIterator<Key,Value> deepSourceCopy, Iterator<Entry<Key,Document>> documents,\n-                    CompositeMetadata compositeMetadata) {\n-        // now lets pull the data if we need to\n-        if (log.isTraceEnabled()) {\n-            log.trace(\"mapDocument \" + fieldIndexSatisfiesQuery);\n-        }\n-        if (fieldIndexSatisfiesQuery) {\n-            final KeyToDocumentData docMapper = new KeyToDocumentData(deepSourceCopy, this.myEnvironment, this.documentOptions, super.equality,\n-                            getEvaluationFilter(), this.includeHierarchyFields, this.includeHierarchyFields);\n-            Iterator<Tuple2<Key,Document>> mappedDocuments = Iterators.transform(\n-                            documents,\n-                            new GetDocument(docMapper, new Aggregation(this.getTimeFilter(), typeMetadataWithNonIndexed, compositeMetadata, this\n-                                            .isIncludeGroupingContext(), this.includeRecordId, this.disableIndexOnlyDocuments(), getEvaluationFilter(),\n-                                            isTrackSizes())));\n-            \n-            Iterator<Entry<Key,Document>> retDocuments = Iterators.transform(mappedDocuments, new TupleToEntry<>());\n-            \n-            // Inject the document permutations if required\n-            if (!this.getDocumentPermutations().isEmpty()) {\n-                if (gatherTimingDetails()) {\n-                    retDocuments = Iterators.transform(retDocuments, new EvaluationTrackingFunction<>(QuerySpan.Stage.DocumentPermutation, trackingSpan,\n-                                    new DocumentPermutation.DocumentPermutationAggregation(this.getDocumentPermutations())));\n-                } else {\n-                    retDocuments = Iterators.transform(retDocuments, new DocumentPermutation.DocumentPermutationAggregation(this.getDocumentPermutations()));\n-                }\n-            }\n-            return retDocuments;\n-        }\n-        return documents;\n+\n+    // a hook to allow mapping the document such as with the TLD or Parent\n+    // query logics\n+    // or if the document was not aggregated in the first place because the\n+    // field index fields completely satisfied the query\n+    documents = mapDocument(deepSourceCopy, documents, compositeMetadata);\n+\n+    // apply any configured post processing\n+    documents = getPostProcessingChain(documents);\n+    if (gatherTimingDetails()) {\n+      documents = new EvaluationTrackingIterator(QuerySpan.Stage.PostProcessing, trackingSpan, documents);\n     }\n-    \n-    public class GetDocument implements Function<Entry<Key,Document>,Tuple2<Key,Document>> {\n-        private final KeyToDocumentData fetchDocData;\n-        private final Aggregation makeDocument;\n-        private final EntryToTuple<Key,Document> convert = new EntryToTuple<>();\n-        \n-        public GetDocument(KeyToDocumentData fetchDocData, Aggregation makeDocument) {\n-            this.fetchDocData = fetchDocData;\n-            this.makeDocument = makeDocument;\n-        }\n-        \n-        public Tuple2<Key,Document> apply(Entry<Key,Document> from) {\n-            from = makeDocument.apply(this.fetchDocData.apply(from));\n-            return convert.apply(from);\n-        }\n+\n+    // Filter out masked values if requested\n+    if (this.filterMaskedValues) {\n+      MaskedValueFilterInterface mvfi =\n+          MaskedValueFilterFactory.get(this.isIncludeGroupingContext(), this.isReducedResponse());\n+      if (gatherTimingDetails()) {\n+        documents = Iterators.transform(documents,\n+            new EvaluationTrackingFunction<>(QuerySpan.Stage.MaskedValueFilter, trackingSpan, mvfi));\n+      } else {\n+        documents = Iterators.transform(documents, mvfi);\n+      }\n     }\n-    \n-    private void prepareKeyValue(TraceScope span) {\n-        if (this.serializedDocuments.hasNext()) {\n-            Entry<Key,Value> entry = this.serializedDocuments.next();\n-            \n-            if (log.isTraceEnabled()) {\n-                log.trace(\"next() returned \" + entry);\n-            }\n-            \n-            this.key = entry.getKey();\n-            this.value = entry.getValue();\n-            \n-            if (Trace.isTracing() && span.getSpan() != null) {\n-                span.getSpan().addKVAnnotation(\"Key\", rowColFamToString(this.key));\n-            }\n-        } else {\n-            if (log.isTraceEnabled()) {\n-                log.trace(\"Exhausted all keys\");\n-            }\n-            this.key = null;\n-            this.value = null;\n-        }\n+\n+    // now filter the attributes to those with the keep flag set true\n+    if (gatherTimingDetails()) {\n+      documents = Iterators\n+          .transform(documents, new EvaluationTrackingFunction<>(QuerySpan.Stage.AttributeKeepFilter, trackingSpan,\n+              new AttributeKeepFilter<>()));\n+    } else {\n+      documents = Iterators.transform(documents, new AttributeKeepFilter<>());\n     }\n-    \n-    @Override\n-    public Key getTopKey() {\n-        return this.key;\n+\n+    // Project fields using a whitelist or a blacklist before serialization\n+    if (this.projectResults) {\n+      if (gatherTimingDetails()) {\n+        documents = Iterators.transform(documents,\n+            new EvaluationTrackingFunction<>(QuerySpan.Stage.DocumentProjection, trackingSpan, getProjection()));\n+      } else {\n+        documents = Iterators.transform(documents, getProjection());\n+      }\n     }\n-    \n-    @Override\n-    public Value getTopValue() {\n-        return this.value;\n+\n+    // remove the composite entries\n+    documents = Iterators.transform(documents, this.getCompositeProjection());\n+\n+    // Filter out any Documents which are empty (e.g. due to attribute\n+    // projection or visibility filtering)\n+    if (gatherTimingDetails()) {\n+      documents = statelessFilter(documents,\n+          new EvaluationTrackingPredicate<>(QuerySpan.Stage.EmptyDocumentFilter, trackingSpan,\n+              new EmptyDocumentFilter()));\n+      documents = Iterators\n+          .transform(documents,\n+              new EvaluationTrackingFunction<>(QuerySpan.Stage.DocumentMetadata, trackingSpan, new DocumentMetadata()));\n+    } else {\n+      documents = statelessFilter(documents, new EmptyDocumentFilter());\n+      documents = Iterators.transform(documents, new DocumentMetadata());\n     }\n-    \n-    @Override\n-    public SortedKeyValueIterator<Key,Value> deepCopy(IteratorEnvironment env) {\n-        return new QueryIterator(this, env);\n+\n+    if (!this.limitFieldsMap.isEmpty()) {\n+      if (gatherTimingDetails()) {\n+        documents = Iterators.transform(documents,\n+            new EvaluationTrackingFunction<>(QuerySpan.Stage.LimitFields, trackingSpan,\n+                new LimitFields(this.getLimitFieldsMap())));\n+      } else {\n+        documents = Iterators.transform(documents, new LimitFields(this.getLimitFieldsMap()));\n+      }\n     }\n-    \n-    /**\n-     * If we are performing evaluation (have a query) and are not performing a full-table scan, then we want to instantiate the boolean logic iterators\n-     * \n-     * @return Whether or not the boolean logic iterators should be used\n-     */\n-    public boolean instantiateBooleanLogic() {\n-        return !this.disableEvaluation && !this.fullTableScanOnly;\n+\n+    // do I need to remove the grouping context I added above?\n+    if (groupingContextAddedByMe) {\n+      if (gatherTimingDetails()) {\n+        documents = Iterators\n+            .transform(documents, new EvaluationTrackingFunction<>(QuerySpan.Stage.RemoveGroupingContext, trackingSpan,\n+                new RemoveGroupingContext()));\n+      } else {\n+        documents = Iterators.transform(documents, new RemoveGroupingContext());\n+      }\n     }\n-    \n-    public void debugBooleanLogicIterators(NestedIterator<Key> root) {\n-        if (log.isDebugEnabled()) {\n-            debugBooleanLogicIterator(root, \"\");\n-        }\n+\n+    // only add the pipeline query span collection iterator which will cache metrics with each document if collectTimingDetails is true\n+    if (collectTimingDetails) {\n+      // if there is not a result, then add the trackingSpan to the\n+      // QuerySpanCollector\n+      // if there was a result, then the metrics from the trackingSpan\n+      // will be added here\n+      documents = new PipelineQuerySpanCollectionIterator(querySpanCollector, trackingSpan, documents);\n     }\n-    \n-    private void debugBooleanLogicIterator(NestedIterator<Key> root, String prefix) {\n-        log.debug(root);\n-        \n-        if (null == root || null == root.children()) {\n-            return;\n+\n+    return documents;\n+  }\n+\n+  protected Iterator<Entry<Key, Document>> getEvaluation(SortedKeyValueIterator<Key, Value> sourceDeepCopy,\n+                                                         Iterator<Entry<Key, Document>> documents,\n+                                                         CompositeMetadata compositeMetadata,\n+                                                         TypeMetadata typeMetadataForEval,\n+                                                         Collection<ByteSequence> columnFamilies, boolean inclusive) {\n+    return getEvaluation(null, sourceDeepCopy, documents, compositeMetadata, typeMetadataForEval, columnFamilies,\n+        inclusive);\n+  }\n+\n+  protected Iterator<Entry<Key, Document>> getEvaluation(NestedQueryIterator<Key> documentSource,\n+                                                         SortedKeyValueIterator<Key, Value> sourceDeepCopy,\n+                                                         Iterator<Entry<Key, Document>> documents,\n+                                                         CompositeMetadata compositeMetadata,\n+                                                         TypeMetadata typeMetadataForEval,\n+                                                         Collection<ByteSequence> columnFamilies, boolean inclusive) {\n+    // Filter the Documents by testing them against the JEXL query\n+    if (!this.disableEvaluation) {\n+\n+      JexlEvaluation jexlEvaluationFunction = getJexlEvaluation(documentSource);\n+      Collection<String> variables = null;\n+      if (null != documentSource && null != documentSource.getQuery()) {\n+\n+        variables = VariableNameVisitor.parseQuery(jexlEvaluationFunction.parse(documentSource.getQuery()));\n+      } else {\n+        variables = VariableNameVisitor.parseQuery(jexlEvaluationFunction.parse(query));\n+      }\n+\n+      final Iterator<Tuple2<Key, Document>> tupleItr = Iterators.transform(documents, new EntryToTuple<>());\n+\n+      // get the function we use for the tf functionality. Note we are\n+      // getting an additional source deep copy for this function\n+      final Iterator<Tuple3<Key, Document, Map<String, Object>>> itrWithContext;\n+      if (this.isTermFrequenciesRequired()) {\n+        Function<Tuple2<Key, Document>, Tuple3<Key, Document, Map<String, Object>>> tfFunction;\n+        tfFunction = TFFactory\n+            .getFunction(getScript(documentSource), getContentExpansionFields(), getTermFrequencyFields(),\n+                this.getTypeMetadata(),\n+                super.equality, getEvaluationFilter(), sourceDeepCopy.deepCopy(myEnvironment));\n+\n+        itrWithContext = TraceIterators.transform(tupleItr, tfFunction, \"Term Frequency Lookup\");\n+      } else {\n+        itrWithContext = Iterators.transform(tupleItr, new EmptyContext<>());\n+      }\n+\n+      try {\n+        IteratorBuildingVisitor iteratorBuildingVisitor =\n+            createIteratorBuildingVisitor(getDocumentRange(documentSource), false, this.sortedUIDs);\n+        Multimap<String, JexlNode> delayedNonEventFieldMap =\n+            DelayedNonEventSubTreeVisitor.getDelayedNonEventFieldMap(iteratorBuildingVisitor, script,\n+                getNonEventFields());\n+\n+        IndexOnlyContextCreatorBuilder contextCreatorBuilder =\n+            new IndexOnlyContextCreatorBuilder().setSource(sourceDeepCopy)\n+                .setRange(getDocumentRange(documentSource)).setTypeMetadata(typeMetadataForEval)\n+                .setCompositeMetadata(compositeMetadata)\n+                .setOptions(this).setVariables(variables).setIteratorBuildingVisitor(iteratorBuildingVisitor)\n+                .setDelayedNonEventFieldMap(delayedNonEventFieldMap).setEquality(equality)\n+                .setColumnFamilies(columnFamilies)\n+                .setInclusive(inclusive).setComparatorFactory(this);\n+        final IndexOnlyContextCreator contextCreator = contextCreatorBuilder.build();\n+\n+        if (exceededOrEvaluationCache != null) {\n+          contextCreator.addAdditionalEntries(exceededOrEvaluationCache);\n         }\n-        \n-        for (NestedIterator<Key> child : root.children()) {\n-            debugBooleanLogicIterator(child, prefix + \"  \");\n+\n+        final Iterator<Tuple3<Key, Document, DatawaveJexlContext>> itrWithDatawaveJexlContext =\n+            Iterators.transform(itrWithContext, contextCreator);\n+        Iterator<Tuple3<Key, Document, DatawaveJexlContext>> matchedDocuments =\n+            statelessFilter(itrWithDatawaveJexlContext, jexlEvaluationFunction);\n+        if (log.isTraceEnabled()) {\n+          log.trace(\"arithmetic:\" + arithmetic + \" range:\" + getDocumentRange(documentSource) + \", thread:\" +\n+              Thread.currentThread());\n         }\n+        return Iterators.transform(matchedDocuments, new TupleToEntry<>());\n+      } catch (InstantiationException | MalformedURLException | IllegalAccessException | ConfigException e) {\n+        throw new IllegalStateException(\"Could not perform delayed index only evaluation\", e);\n+      }\n+    } else if (log.isTraceEnabled()) {\n+      log.trace(\"Evaluation is disabled, not instantiating Jexl evaluation logic\");\n     }\n-    \n-    protected DocumentProjection getProjection() {\n-        DocumentProjection projection = new DocumentProjection(this.isIncludeGroupingContext(), this.isReducedResponse(), isTrackSizes());\n-        \n-        if (this.useWhiteListedFields) {\n-            projection.initializeWhitelist(this.whiteListedFields);\n-            return projection;\n-        } else if (this.useBlackListedFields) {\n-            projection.initializeBlacklist(this.blackListedFields);\n-            return projection;\n-        } else {\n-            String msg = \"Configured to use projection, but no whitelist or blacklist was provided\";\n-            log.error(msg);\n-            throw new IllegalArgumentException(msg);\n-        }\n+    return documents;\n+  }\n+\n+  private Range getDocumentRange(NestedQueryIterator<Key> documentSource) {\n+    if (null == documentSource) {\n+      return range;\n     }\n-    \n-    protected DocumentProjection getCompositeProjection() {\n-        DocumentProjection projection = new DocumentProjection(this.isIncludeGroupingContext(), this.isReducedResponse(), isTrackSizes());\n-        Set<String> composites = Sets.newHashSet();\n-        if (compositeMetadata != null) {\n-            for (Multimap<String,String> val : this.compositeMetadata.getCompositeFieldMapByType().values())\n-                for (String compositeField : val.keySet())\n-                    if (!CompositeIngest.isOverloadedCompositeField(val, compositeField))\n-                        composites.add(compositeField);\n-        }\n-        projection.initializeBlacklist(composites);\n-        return projection;\n+    NestedQuery<Key> nestedQuery = documentSource.getNestedQuery();\n+    if (null == nestedQuery) {\n+      return range;\n+    } else {\n+      return nestedQuery.getRange();\n     }\n-    \n-    /**\n-     * Determines if a range is document specific according to the following criteria\n-     * \n-     * <pre>\n-     *     1. Cannot have a null start or end key\n-     *     2. Cannot span multiple rows\n-     *     3. ColumnFamily must contain a null byte separator\n-     * </pre>\n-     *\n-     * @param r\n-     *            - {@link Range} to be evaluated\n-     * @return - true if this is a document specific range, false if not.\n-     */\n-    public static boolean isDocumentSpecificRange(Range r) {\n-        Preconditions.checkNotNull(r);\n-        \n-        // Also @see datawave.query.index.lookup.TupleToRange\n-        // We have already made the assertion that the client is sending us\n-        // an inclusive start key due to the inability to ascertain the\n-        // difference between and event-specific range and a continueMultiScan.\n-        //\n-        // As such, it is acceptable for us to make the same assertion on the\n-        // inclusivity of the start key.\n-        \n-        // Cannot have a null start or end key\n-        if (r.isInfiniteStartKey() || r.isInfiniteStopKey()) {\n-            return false;\n-        }\n-        \n-        // Cannot span multiple rows.\n-        Key startKey = r.getStartKey();\n-        Key endKey = r.getEndKey();\n-        if (!startKey.getRowData().equals(endKey.getRowData())) {\n-            return false;\n-        }\n-        \n-        // Column Family must contain a null byte separator.\n-        Text startCF = startKey.getColumnFamily();\n-        Text endCF = endKey.getColumnFamily();\n-        if (startCF.find(Constants.NULL) == -1 || endCF.find(Constants.NULL) == -1) {\n-            return false;\n-        }\n-        return true;\n+  }\n+\n+  protected JexlEvaluation getJexlEvaluation(NestedQueryIterator<Key> documentSource) {\n+\n+    if (null == documentSource) {\n+      return new JexlEvaluation(query, getArithmetic());\n     }\n-    \n-    /**\n-     * Convert the given key's row &amp; column family to a string.\n-     *\n-     * @param k\n-     *            - a {@link Key}\n-     * @return - a string representation of the given key's row &amp; column family.\n-     */\n-    public static String rowColFamToString(Key k) {\n-        if (null == k) {\n-            return \"null\";\n-        }\n-        \n-        final Text holder = new Text();\n-        StringBuilder sb = new StringBuilder(40);\n-        \n-        k.getRow(holder);\n-        Key.appendPrintableString(holder.getBytes(), 0, holder.getLength(), org.apache.accumulo.core.Constants.MAX_DATA_TO_PRINT, sb);\n-        sb.append(Constants.SPACE);\n-        \n-        k.getColumnFamily(holder);\n-        Key.appendPrintableString(holder.getBytes(), 0, holder.getLength(), org.apache.accumulo.core.Constants.MAX_DATA_TO_PRINT, sb);\n-        \n-        sb.append(Constants.COLON);\n-        \n-        k.getColumnQualifier(holder);\n-        Key.appendPrintableString(holder.getBytes(), 0, holder.getLength(), org.apache.accumulo.core.Constants.MAX_DATA_TO_PRINT, sb);\n-        \n-        return sb.toString();\n+    JexlEvaluation jexlEvaluationFunction = null;\n+    NestedQuery<Key> nestedQuery = documentSource.getNestedQuery();\n+    if (null == nestedQuery) {\n+      jexlEvaluationFunction = new JexlEvaluation(query, getArithmetic());\n+    } else {\n+      jexlEvaluationFunction = nestedQuery.getEvaluation();\n+      if (null == jexlEvaluationFunction) {\n+        return new JexlEvaluation(query, getArithmetic());\n+      }\n     }\n-    \n-    protected NestedIterator<Key> getOrSetKeySource(final Range documentRange, ASTJexlScript rangeScript) throws IOException, ConfigException,\n-                    IllegalAccessException, InstantiationException {\n-        NestedIterator<Key> sourceIter = null;\n-        // If we're doing field index or a non-fulltable (aka a normal\n-        // query)\n-        if (!this.isFullTableScanOnly()) {\n-            \n-            boolean isQueryFullySatisfiedInitialState = batchedQueries <= 0;\n-            String hitListOptionString = documentOptions.get(QueryOptions.HIT_LIST);\n-            \n-            if (hitListOptionString != null) {\n-                boolean hitListOption = Boolean.parseBoolean(hitListOptionString);\n-                if (hitListOption) {\n-                    isQueryFullySatisfiedInitialState = false; // if hit\n-                                                               // list is\n-                                                               // on, don't\n-                                                               // attempt\n-                                                               // satisfiability\n-                    // don't even make a SatisfactionVisitor.....\n-                }\n-            }\n-            if (isQueryFullySatisfiedInitialState) {\n-                SatisfactionVisitor satisfactionVisitor = this.createSatisfiabilityVisitor(true); // we'll\n-                                                                                                  // charge\n-                                                                                                  // in\n-                                                                                                  // with\n-                                                                                                  // optimism\n-                \n-                // visit() and get the root which is the root of a tree of\n-                // Boolean Logic Iterator<Key>'s\n-                rangeScript.jjtAccept(satisfactionVisitor, null);\n-                \n-                isQueryFullySatisfiedInitialState = satisfactionVisitor.isQueryFullySatisfied();\n-                \n-            }\n-            \n-            IteratorBuildingVisitor visitor = createIteratorBuildingVisitor(documentRange, isQueryFullySatisfiedInitialState, this.sortedUIDs);\n-            \n-            // visit() and get the root which is the root of a tree of\n-            // Boolean Logic Iterator<Key>'s\n-            rangeScript.jjtAccept(visitor, null);\n-            \n-            sourceIter = visitor.root();\n-            \n-            if (visitor.isQueryFullySatisfied()) {\n-                this.fieldIndexSatisfiesQuery = true;\n-            }\n-            \n-            // Print out the boolean logic tree of iterators\n-            debugBooleanLogicIterators(sourceIter);\n-            \n-            if (sourceIter != null) {\n-                sourceIter = new SeekableNestedIterator(sourceIter, this.myEnvironment);\n-            }\n-        }\n-        \n-        // resort to a full table scan otherwise\n-        if (sourceIter == null) {\n-            sourceIter = getEventDataNestedIterator(source);\n-        }\n-        \n-        return sourceIter;\n+    return jexlEvaluationFunction;\n+  }\n+\n+  @Override\n+  public JexlArithmetic getArithmetic() {\n+    JexlArithmetic myArithmetic = this.arithmetic;\n+    if (myArithmetic instanceof StatefulArithmetic) {\n+      myArithmetic = ((StatefulArithmetic) arithmetic).clone();\n     }\n-    \n-    /**\n-     * Determine whether the query can be completely satisfied by the field index\n-     * \n-     * @return true if it can be completely satisfied.\n-     */\n-    protected boolean isFieldIndexSatisfyingQuery() {\n-        return this.fieldIndexSatisfiesQuery;\n+    return myArithmetic;\n+  }\n+\n+  protected ASTJexlScript getScript(NestedQueryIterator<Key> documentSource) {\n+    if (null == documentSource) {\n+      return script;\n     }\n-    \n-    protected NestedIterator<Key> getEventDataNestedIterator(SortedKeyValueIterator<Key,Value> source) {\n-        return new EventDataScanNestedIterator(source, getEventEntryKeyDataTypeFilter());\n+    NestedQuery<Key> query = documentSource.getNestedQuery();\n+    if (null == query) {\n+      return script;\n+    } else {\n+      ASTJexlScript rangeScript = query.getScript();\n+      if (null == rangeScript) {\n+        return script;\n+      }\n+      return rangeScript;\n     }\n-    \n-    protected IteratorBuildingVisitor createIteratorBuildingVisitor(final Range documentRange, boolean isQueryFullySatisfied, boolean sortedUIDs)\n-                    throws ConfigException, MalformedURLException, InstantiationException, IllegalAccessException {\n-        return createIteratorBuildingVisitor(IteratorBuildingVisitor.class, documentRange, isQueryFullySatisfied, sortedUIDs);\n+  }\n+\n+  protected Iterator<Entry<Key, Document>> mapDocument(SortedKeyValueIterator<Key, Value> deepSourceCopy,\n+                                                       Iterator<Entry<Key, Document>> documents,\n+                                                       CompositeMetadata compositeMetadata) {\n+    // now lets pull the data if we need to\n+    if (log.isTraceEnabled()) {\n+      log.trace(\"mapDocument \" + fieldIndexSatisfiesQuery);\n     }\n-    \n-    protected IteratorBuildingVisitor createIteratorBuildingVisitor(Class<? extends IteratorBuildingVisitor> c, final Range documentRange,\n-                    boolean isQueryFullySatisfied, boolean sortedUIDs) throws MalformedURLException, ConfigException, IllegalAccessException,\n-                    InstantiationException {\n-        if (log.isTraceEnabled()) {\n-            log.trace(documentRange);\n+    if (fieldIndexSatisfiesQuery) {\n+      final KeyToDocumentData docMapper =\n+          new KeyToDocumentData(deepSourceCopy, this.myEnvironment, this.documentOptions, super.equality,\n+              getEvaluationFilter(), this.includeHierarchyFields, this.includeHierarchyFields);\n+      Iterator<Tuple2<Key, Document>> mappedDocuments = Iterators.transform(\n+          documents,\n+          new GetDocument(docMapper,\n+              new Aggregation(this.getTimeFilter(), typeMetadataWithNonIndexed, compositeMetadata, this\n+                  .isIncludeGroupingContext(), this.includeRecordId, this.disableIndexOnlyDocuments(),\n+                  getEvaluationFilter(),\n+                  isTrackSizes())));\n+\n+      Iterator<Entry<Key, Document>> retDocuments = Iterators.transform(mappedDocuments, new TupleToEntry<>());\n+\n+      // Inject the document permutations if required\n+      if (!this.getDocumentPermutations().isEmpty()) {\n+        if (gatherTimingDetails()) {\n+          retDocuments = Iterators.transform(retDocuments,\n+              new EvaluationTrackingFunction<>(QuerySpan.Stage.DocumentPermutation, trackingSpan,\n+                  new DocumentPermutation.DocumentPermutationAggregation(this.getDocumentPermutations())));\n+        } else {\n+          retDocuments = Iterators.transform(retDocuments,\n+              new DocumentPermutation.DocumentPermutationAggregation(this.getDocumentPermutations()));\n         }\n-        \n-        // determine the list of indexed fields\n-        Set<String> indexedFields = this.getIndexedFields();\n-        indexedFields.removeAll(this.getNonIndexedDataTypeMap().keySet());\n-        \n-        // @formatter:off\n-        return c.newInstance()\n-                .setSource(this, this.myEnvironment)\n-                .setTimeFilter(this.getTimeFilter())\n-                .setTypeMetadata(this.getTypeMetadata())\n-                .setFieldsToAggregate(this.getNonEventFields())\n-                .setAttrFilter(this.getEvaluationFilter())\n-                .setDatatypeFilter(this.getFieldIndexKeyDataTypeFilter())\n-                .setFiAggregator(this.fiAggregator)\n-                .setHdfsFileSystem(this.getFileSystemCache())\n-                .setQueryLock(this.getQueryLock())\n-                .setIvaratorCacheDirConfigs(this.getIvaratorCacheDirConfigs())\n-                .setQueryId(this.getQueryId())\n-                .setScanId(this.getScanId())\n-                .setIvaratorCacheSubDirPrefix(this.getHdfsCacheSubDirPrefix())\n-                .setHdfsFileCompressionCodec(this.getHdfsFileCompressionCodec())\n-                .setIvaratorCacheBufferSize(this.getIvaratorCacheBufferSize())\n-                .setIvaratorCacheScanPersistThreshold(this.getIvaratorCacheScanPersistThreshold())\n-                .setIvaratorCacheScanTimeout(this.getIvaratorCacheScanTimeout())\n-                .setMaxRangeSplit(this.getMaxIndexRangeSplit())\n-                .setIvaratorMaxOpenFiles(this.getIvaratorMaxOpenFiles())\n-                .setIvaratorNumRetries(this.getIvaratorNumRetries())\n-                .setIvaratorPersistOptions(this.getIvaratorPersistOptions())\n-                .setUnsortedIvaratorSource(this.sourceForDeepCopies)\n-                .setIvaratorSourcePool(createIvaratorSourcePool(this.maxIvaratorSources))\n-                .setMaxIvaratorResults(this.getMaxIvaratorResults())\n-                .setIncludes(indexedFields)\n-                .setTermFrequencyFields(this.getTermFrequencyFields())\n-                .setIsQueryFullySatisfied(isQueryFullySatisfied)\n-                .setSortedUIDs(sortedUIDs)\n-                .limit(documentRange)\n-                .disableIndexOnly(disableFiEval)\n-                .limit(this.sourceLimit)\n-                .setCollectTimingDetails(this.collectTimingDetails)\n-                .setQuerySpanCollector(this.querySpanCollector)\n-                .setIndexOnlyFields(this.getAllIndexOnlyFields())\n-                .setAllowTermFrequencyLookup(this.allowTermFrequencyLookup)\n-                .setCompositeMetadata(compositeMetadata)\n-                .setExceededOrEvaluationCache(exceededOrEvaluationCache);\n-        // @formatter:on\n-        // TODO: .setStatsPort(this.statsdHostAndPort);\n+      }\n+      return retDocuments;\n     }\n-    \n-    protected GenericObjectPool<SortedKeyValueIterator<Key,Value>> createIvaratorSourcePool(int maxIvaratorSources) {\n-        return new GenericObjectPool<>(createIvaratorSourceFactory(this), createIvaratorSourcePoolConfig(maxIvaratorSources));\n+    return documents;\n+  }\n+\n+  public class GetDocument implements Function<Entry<Key, Document>, Tuple2<Key, Document>> {\n+    private final KeyToDocumentData fetchDocData;\n+    private final Aggregation makeDocument;\n+    private final EntryToTuple<Key, Document> convert = new EntryToTuple<>();\n+\n+    public GetDocument(KeyToDocumentData fetchDocData, Aggregation makeDocument) {\n+      this.fetchDocData = fetchDocData;\n+      this.makeDocument = makeDocument;\n     }\n-    \n-    private BasePoolableObjectFactory<SortedKeyValueIterator<Key,Value>> createIvaratorSourceFactory(SourceFactory<Key,Value> sourceFactory) {\n-        return new BasePoolableObjectFactory<SortedKeyValueIterator<Key,Value>>() {\n-            @Override\n-            public SortedKeyValueIterator<Key,Value> makeObject() throws Exception {\n-                return sourceFactory.getSourceDeepCopy();\n-            }\n-        };\n+\n+    public Tuple2<Key, Document> apply(Entry<Key, Document> from) {\n+      from = makeDocument.apply(this.fetchDocData.apply(from));\n+      return convert.apply(from);\n     }\n-    \n-    private GenericObjectPool.Config createIvaratorSourcePoolConfig(int maxIvaratorSources) {\n-        GenericObjectPool.Config poolConfig = new GenericObjectPool.Config();\n-        poolConfig.maxActive = maxIvaratorSources;\n-        poolConfig.maxIdle = maxIvaratorSources;\n-        poolConfig.minIdle = 0;\n-        poolConfig.whenExhaustedAction = WHEN_EXHAUSTED_BLOCK;\n-        return poolConfig;\n+  }\n+\n+  private void prepareKeyValue(TraceScope span) {\n+    if (this.serializedDocuments.hasNext()) {\n+      Entry<Key, Value> entry = this.serializedDocuments.next();\n+\n+      if (log.isTraceEnabled()) {\n+        log.trace(\"next() returned \" + entry);\n+      }\n+\n+      this.key = entry.getKey();\n+      this.value = entry.getValue();\n+\n+      if (Trace.isTracing() && span.getSpan() != null) {\n+        span.getSpan().addKVAnnotation(\"Key\", rowColFamToString(this.key));\n+      }\n+    } else {\n+      if (log.isTraceEnabled()) {\n+        log.trace(\"Exhausted all keys\");\n+      }\n+      this.key = null;\n+      this.value = null;\n     }\n-    \n-    protected String getHdfsCacheSubDirPrefix() {\n-        // if we have a document specific range, or a list of specific doc ids (bundled document specific range per-se), then\n-        // we could have multiple iterators running against this shard for this query at the same time.\n-        // In this case we need to differentiate between the ivarator directories being created. However this is\n-        // a situation we do not want to be in, so we will also display a large warning to be seen by the accumulo monitor.\n-        String hdfsPrefix = null;\n-        if (isDocumentSpecificRange(this.range)) {\n-            hdfsPrefix = range.getStartKey().getColumnFamily().toString().replace('\\0', '_');\n-        } else if (batchedQueries > 0) {\n-            StringBuilder sb = new StringBuilder();\n-            for (Entry<Range,String> queries : batchStack) {\n-                if (sb.length() > 0) {\n-                    sb.append('-');\n-                }\n-                sb.append(queries.getKey().getStartKey().getColumnFamily().toString().replace('\\0', '_'));\n-            }\n-            hdfsPrefix = sb.toString();\n+  }\n+\n+  @Override\n+  public Key getTopKey() {\n+    return this.key;\n+  }\n+\n+  @Override\n+  public Value getTopValue() {\n+    return this.value;\n+  }\n+\n+  @Override\n+  public SortedKeyValueIterator<Key, Value> deepCopy(IteratorEnvironment env) {\n+    return new QueryIterator(this, env);\n+  }\n+\n+  /**\n+   * If we are performing evaluation (have a query) and are not performing a full-table scan, then we want to instantiate the boolean logic iterators\n+   *\n+   * @return Whether or not the boolean logic iterators should be used\n+   */\n+  public boolean instantiateBooleanLogic() {\n+    return !this.disableEvaluation && !this.fullTableScanOnly;\n+  }\n+\n+  public void debugBooleanLogicIterators(NestedIterator<Key> root) {\n+    if (log.isDebugEnabled()) {\n+      debugBooleanLogicIterator(root, \"\");\n+    }\n+  }\n+\n+  private void debugBooleanLogicIterator(NestedIterator<Key> root, String prefix) {\n+    log.debug(root);\n+\n+    if (null == root || null == root.children()) {\n+      return;\n+    }\n+\n+    for (NestedIterator<Key> child : root.children()) {\n+      debugBooleanLogicIterator(child, prefix + \"  \");\n+    }\n+  }\n+\n+  protected DocumentProjection getProjection() {\n+    DocumentProjection projection =\n+        new DocumentProjection(this.isIncludeGroupingContext(), this.isReducedResponse(), isTrackSizes());\n+\n+    if (this.useWhiteListedFields) {\n+      projection.initializeWhitelist(this.whiteListedFields);\n+      return projection;\n+    } else if (this.useBlackListedFields) {\n+      projection.initializeBlacklist(this.blackListedFields);\n+      return projection;\n+    } else {\n+      String msg = \"Configured to use projection, but no whitelist or blacklist was provided\";\n+      log.error(msg);\n+      throw new IllegalArgumentException(msg);\n+    }\n+  }\n+\n+  protected DocumentProjection getCompositeProjection() {\n+    DocumentProjection projection =\n+        new DocumentProjection(this.isIncludeGroupingContext(), this.isReducedResponse(), isTrackSizes());\n+    Set<String> composites = Sets.newHashSet();\n+    if (compositeMetadata != null) {\n+      for (Multimap<String, String> val : this.compositeMetadata.getCompositeFieldMapByType().values()) {\n+        for (String compositeField : val.keySet()) {\n+          if (!CompositeIngest.isOverloadedCompositeField(val, compositeField)) {\n+            composites.add(compositeField);\n+          }\n         }\n-        return hdfsPrefix;\n+      }\n     }\n-    \n-    protected SatisfactionVisitor createSatisfiabilityVisitor(boolean isQueryFullySatisfiedInitialState) {\n-        \n-        // determine the list of indexed fields\n-        Set<String> indexedFields = this.getIndexedFields();\n-        \n-        indexedFields.removeAll(this.getNonIndexedDataTypeMap().keySet());\n-        \n-        return new SatisfactionVisitor(getNonEventFields(), indexedFields, Collections.emptySet(), isQueryFullySatisfiedInitialState);\n+    projection.initializeBlacklist(composites);\n+    return projection;\n+  }\n+\n+  /**\n+   * Determines if a range is document specific according to the following criteria\n+   *\n+   * <pre>\n+   *     1. Cannot have a null start or end key\n+   *     2. Cannot span multiple rows\n+   *     3. ColumnFamily must contain a null byte separator\n+   * </pre>\n+   *\n+   * @param r - {@link Range} to be evaluated\n+   * @return - true if this is a document specific range, false if not.\n+   */\n+  public static boolean isDocumentSpecificRange(Range r) {\n+    Preconditions.checkNotNull(r);\n+\n+    // Also @see datawave.query.index.lookup.TupleToRange\n+    // We have already made the assertion that the client is sending us\n+    // an inclusive start key due to the inability to ascertain the\n+    // difference between and event-specific range and a continueMultiScan.\n+    //\n+    // As such, it is acceptable for us to make the same assertion on the\n+    // inclusivity of the start key.\n+\n+    // Cannot have a null start or end key\n+    if (r.isInfiniteStartKey() || r.isInfiniteStopKey()) {\n+      return false;\n+    }\n+\n+    // Cannot span multiple rows.\n+    Key startKey = r.getStartKey();\n+    Key endKey = r.getEndKey();\n+    if (!startKey.getRowData().equals(endKey.getRowData())) {\n+      return false;\n     }\n-    \n-    public void setQuery(String query) {\n-        this.query = query;\n+\n+    // Column Family must contain a null byte separator.\n+    Text startCF = startKey.getColumnFamily();\n+    Text endCF = endKey.getColumnFamily();\n+    if (startCF.find(Constants.NULL) == -1 || endCF.find(Constants.NULL) == -1) {\n+      return false;\n     }\n-    \n-    public String getQuery() {\n-        return query;\n+    return true;\n+  }\n+\n+  /**\n+   * Convert the given key's row &amp; column family to a string.\n+   *\n+   * @param k - a {@link Key}\n+   * @return - a string representation of the given key's row &amp; column family.\n+   */\n+  public static String rowColFamToString(Key k) {\n+    if (null == k) {\n+      return \"null\";\n     }\n-    \n-    /**\n-     * A comparator which will ensure that values with a grouping context are sorted first. This allows us to not use exhaustive matching within the jexl\n-     * context to determine whether the document of interest is actually a match. Matching on the grouping context one (from the Event, not from the field\n-     * index) means that the hit terms will also have the grouping context on the field, and have the non-normalized value.\n-     */\n-    private static class ValueComparator implements Comparator<Object> {\n-        final Text fi;\n-        \n-        public ValueComparator(Key metadata) {\n-            fi = (metadata == null ? new Text() : metadata.getColumnFamily());\n+\n+    final Text holder = new Text();\n+    StringBuilder sb = new StringBuilder(40);\n+\n+    k.getRow(holder);\n+    Key.appendPrintableString(holder.getBytes(), 0, holder.getLength(),\n+        org.apache.accumulo.core.Constants.MAX_DATA_TO_PRINT, sb);\n+    sb.append(Constants.SPACE);\n+\n+    k.getColumnFamily(holder);\n+    Key.appendPrintableString(holder.getBytes(), 0, holder.getLength(),\n+        org.apache.accumulo.core.Constants.MAX_DATA_TO_PRINT, sb);\n+\n+    sb.append(Constants.COLON);\n+\n+    k.getColumnQualifier(holder);\n+    Key.appendPrintableString(holder.getBytes(), 0, holder.getLength(),\n+        org.apache.accumulo.core.Constants.MAX_DATA_TO_PRINT, sb);\n+\n+    return sb.toString();\n+  }\n+\n+  protected NestedIterator<Key> getOrSetKeySource(final Range documentRange, ASTJexlScript rangeScript)\n+      throws IOException, ConfigException,\n+      IllegalAccessException, InstantiationException {\n+    NestedIterator<Key> sourceIter = null;\n+    // If we're doing field index or a non-fulltable (aka a normal\n+    // query)\n+    if (!this.isFullTableScanOnly()) {\n+\n+      boolean isQueryFullySatisfiedInitialState = batchedQueries <= 0;\n+      String hitListOptionString = documentOptions.get(QueryOptions.HIT_LIST);\n+\n+      if (hitListOptionString != null) {\n+        boolean hitListOption = Boolean.parseBoolean(hitListOptionString);\n+        if (hitListOption) {\n+          isQueryFullySatisfiedInitialState = false; // if hit\n+          // list is\n+          // on, don't\n+          // attempt\n+          // satisfiability\n+          // don't even make a SatisfactionVisitor.....\n         }\n-        \n-        @Override\n-        public int compare(Object o1, Object o2) {\n-            if (fi.getLength() == 0) {\n-                return new CompareToBuilder().append(o1, o2).toComparison();\n-            } else {\n-                boolean o1Matches = (o1 instanceof ValueTuple && (((ValueTuple) o1).first().indexOf('.') != -1));\n-                boolean o2Matches = (o2 instanceof ValueTuple && (((ValueTuple) o2).first().indexOf('.') != -1));\n-                if (o1Matches == o2Matches) {\n-                    return new CompareToBuilder().append(o1, o2).toComparison();\n-                } else if (o1Matches) {\n-                    return -1;\n-                } else {\n-                    return 1;\n-                }\n-            }\n+      }\n+      if (isQueryFullySatisfiedInitialState) {\n+        SatisfactionVisitor satisfactionVisitor = this.createSatisfiabilityVisitor(true); // we'll\n+        // charge\n+        // in\n+        // with\n+        // optimism\n+\n+        // visit() and get the root which is the root of a tree of\n+        // Boolean Logic Iterator<Key>'s\n+        rangeScript.jjtAccept(satisfactionVisitor, null);\n+\n+        isQueryFullySatisfiedInitialState = satisfactionVisitor.isQueryFullySatisfied();\n+\n+      }\n+\n+      IteratorBuildingVisitor visitor =\n+          createIteratorBuildingVisitor(documentRange, isQueryFullySatisfiedInitialState, this.sortedUIDs);\n+\n+      // visit() and get the root which is the root of a tree of\n+      // Boolean Logic Iterator<Key>'s\n+      rangeScript.jjtAccept(visitor, null);\n+\n+      sourceIter = visitor.root();\n+\n+      if (visitor.isQueryFullySatisfied()) {\n+        this.fieldIndexSatisfiesQuery = true;\n+      }\n+\n+      // Print out the boolean logic tree of iterators\n+      debugBooleanLogicIterators(sourceIter);\n+\n+      if (sourceIter != null) {\n+        sourceIter = new SeekableNestedIterator(sourceIter, this.myEnvironment);\n+      }\n+    }\n+\n+    // resort to a full table scan otherwise\n+    if (sourceIter == null) {\n+      sourceIter = getEventDataNestedIterator(source);\n+    }\n+\n+    return sourceIter;\n+  }\n+\n+  /**\n+   * Determine whether the query can be completely satisfied by the field index\n+   *\n+   * @return true if it can be completely satisfied.\n+   */\n+  protected boolean isFieldIndexSatisfyingQuery() {\n+    return this.fieldIndexSatisfiesQuery;\n+  }\n+\n+  protected NestedIterator<Key> getEventDataNestedIterator(SortedKeyValueIterator<Key, Value> source) {\n+    return new EventDataScanNestedIterator(source, getEventEntryKeyDataTypeFilter());\n+  }\n+\n+  protected IteratorBuildingVisitor createIteratorBuildingVisitor(final Range documentRange,\n+                                                                  boolean isQueryFullySatisfied, boolean sortedUIDs)\n+      throws ConfigException, MalformedURLException, InstantiationException, IllegalAccessException {\n+    return createIteratorBuildingVisitor(IteratorBuildingVisitor.class, documentRange, isQueryFullySatisfied,\n+        sortedUIDs);\n+  }\n+\n+  protected IteratorBuildingVisitor createIteratorBuildingVisitor(Class<? extends IteratorBuildingVisitor> c,\n+                                                                  final Range documentRange,\n+                                                                  boolean isQueryFullySatisfied, boolean sortedUIDs)\n+      throws MalformedURLException, ConfigException, IllegalAccessException,\n+      InstantiationException {\n+    if (log.isTraceEnabled()) {\n+      log.trace(documentRange);\n+    }\n+\n+    // determine the list of indexed fields\n+    Set<String> indexedFields = this.getIndexedFields();\n+    indexedFields.removeAll(this.getNonIndexedDataTypeMap().keySet());\n+\n+    // @formatter:off\n+    return c.newInstance()\n+        .setSource(this, this.myEnvironment)\n+        .setTimeFilter(this.getTimeFilter())\n+        .setTypeMetadata(this.getTypeMetadata())\n+        .setFieldsToAggregate(this.getNonEventFields())\n+        .setAttrFilter(this.getEvaluationFilter())\n+        .setDatatypeFilter(this.getFieldIndexKeyDataTypeFilter())\n+        .setFiAggregator(this.fiAggregator)\n+        .setHdfsFileSystem(this.getFileSystemCache())\n+        .setQueryLock(this.getQueryLock())\n+        .setIvaratorCacheDirConfigs(this.getIvaratorCacheDirConfigs())\n+        .setQueryId(this.getQueryId())\n+        .setScanId(this.getScanId())\n+        .setIvaratorCacheSubDirPrefix(this.getHdfsCacheSubDirPrefix())\n+        .setHdfsFileCompressionCodec(this.getHdfsFileCompressionCodec())\n+        .setIvaratorCacheBufferSize(this.getIvaratorCacheBufferSize())\n+        .setIvaratorCacheScanPersistThreshold(this.getIvaratorCacheScanPersistThreshold())\n+        .setIvaratorCacheScanTimeout(this.getIvaratorCacheScanTimeout())\n+        .setMaxRangeSplit(this.getMaxIndexRangeSplit())\n+        .setIvaratorMaxOpenFiles(this.getIvaratorMaxOpenFiles())\n+        .setIvaratorNumRetries(this.getIvaratorNumRetries())\n+        .setIvaratorPersistOptions(this.getIvaratorPersistOptions())\n+        .setUnsortedIvaratorSource(this.sourceForDeepCopies)\n+        .setIvaratorSourcePool(createIvaratorSourcePool(this.maxIvaratorSources))\n+        .setMaxIvaratorResults(this.getMaxIvaratorResults())\n+        .setIncludes(indexedFields)\n+        .setTermFrequencyFields(this.getTermFrequencyFields())\n+        .setIsQueryFullySatisfied(isQueryFullySatisfied)\n+        .setSortedUIDs(sortedUIDs)\n+        .limit(documentRange)\n+        .disableIndexOnly(disableFiEval)\n+        .limit(this.sourceLimit)\n+        .setCollectTimingDetails(this.collectTimingDetails)\n+        .setQuerySpanCollector(this.querySpanCollector)\n+        .setIndexOnlyFields(this.getAllIndexOnlyFields())\n+        .setAllowTermFrequencyLookup(this.allowTermFrequencyLookup)\n+        .setCompositeMetadata(compositeMetadata)\n+        .setExceededOrEvaluationCache(exceededOrEvaluationCache);\n+    // @formatter:on\n+    // TODO: .setStatsPort(this.statsdHostAndPort);\n+  }\n+\n+  protected GenericObjectPool<SortedKeyValueIterator<Key, Value>> createIvaratorSourcePool(int maxIvaratorSources) {\n+    return new GenericObjectPool<>(createIvaratorSourceFactory(this),\n+        createIvaratorSourcePoolConfig(maxIvaratorSources));\n+  }\n+\n+  private BasePoolableObjectFactory<SortedKeyValueIterator<Key, Value>> createIvaratorSourceFactory(\n+      SourceFactory<Key, Value> sourceFactory) {\n+    return new BasePoolableObjectFactory<SortedKeyValueIterator<Key, Value>>() {\n+      @Override\n+      public SortedKeyValueIterator<Key, Value> makeObject() throws Exception {\n+        return sourceFactory.getSourceDeepCopy();\n+      }\n+    };\n+  }\n+\n+  private GenericObjectPool.Config createIvaratorSourcePoolConfig(int maxIvaratorSources) {\n+    GenericObjectPool.Config poolConfig = new GenericObjectPool.Config();\n+    poolConfig.maxActive = maxIvaratorSources;\n+    poolConfig.maxIdle = maxIvaratorSources;\n+    poolConfig.minIdle = 0;\n+    poolConfig.whenExhaustedAction = WHEN_EXHAUSTED_BLOCK;\n+    return poolConfig;\n+  }\n+\n+  protected String getHdfsCacheSubDirPrefix() {\n+    // if we have a document specific range, or a list of specific doc ids (bundled document specific range per-se), then\n+    // we could have multiple iterators running against this shard for this query at the same time.\n+    // In this case we need to differentiate between the ivarator directories being created. However this is\n+    // a situation we do not want to be in, so we will also display a large warning to be seen by the accumulo monitor.\n+    String hdfsPrefix = null;\n+    if (isDocumentSpecificRange(this.range)) {\n+      hdfsPrefix = range.getStartKey().getColumnFamily().toString().replace('\\0', '_');\n+    } else if (batchedQueries > 0) {\n+      StringBuilder sb = new StringBuilder();\n+      for (Entry<Range, String> queries : batchStack) {\n+        if (sb.length() > 0) {\n+          sb.append('-');\n         }\n+        sb.append(queries.getKey().getStartKey().getColumnFamily().toString().replace('\\0', '_'));\n+      }\n+      hdfsPrefix = sb.toString();\n     }\n-    \n-    /**\n-     * This can be overridden to supply a value comparator for use within the jexl context. Useful when using the HitListArithmetic which pulls back which value\n-     * tuples were actually hit upon.\n-     * \n-     * @param from\n-     * @return A comparator for values within the jexl context.\n-     */\n+    return hdfsPrefix;\n+  }\n+\n+  protected SatisfactionVisitor createSatisfiabilityVisitor(boolean isQueryFullySatisfiedInitialState) {\n+\n+    // determine the list of indexed fields\n+    Set<String> indexedFields = this.getIndexedFields();\n+\n+    indexedFields.removeAll(this.getNonIndexedDataTypeMap().keySet());\n+\n+    return new SatisfactionVisitor(getNonEventFields(), indexedFields, Collections.emptySet(),\n+        isQueryFullySatisfiedInitialState);\n+  }\n+\n+  public void setQuery(String query) {\n+    this.query = query;\n+  }\n+\n+  public String getQuery() {\n+    return query;\n+  }\n+\n+  /**\n+   * A comparator which will ensure that values with a grouping context are sorted first. This allows us to not use exhaustive matching within the jexl\n+   * context to determine whether the document of interest is actually a match. Matching on the grouping context one (from the Event, not from the field\n+   * index) means that the hit terms will also have the grouping context on the field, and have the non-normalized value.\n+   */\n+  private static class ValueComparator implements Comparator<Object> {\n+    final Text fi;\n+\n+    public ValueComparator(Key metadata) {\n+      fi = (metadata == null ? new Text() : metadata.getColumnFamily());\n+    }\n+\n     @Override\n-    public Comparator<Object> getValueComparator(Tuple3<Key,Document,Map<String,Object>> from) {\n-        return new ValueComparator(from.second().getMetadata());\n+    public int compare(Object o1, Object o2) {\n+      if (fi.getLength() == 0) {\n+        return new CompareToBuilder().append(o1, o2).toComparison();\n+      } else {\n+        boolean o1Matches = (o1 instanceof ValueTuple && (((ValueTuple) o1).first().indexOf('.') != -1));\n+        boolean o2Matches = (o2 instanceof ValueTuple && (((ValueTuple) o2).first().indexOf('.') != -1));\n+        if (o1Matches == o2Matches) {\n+          return new CompareToBuilder().append(o1, o2).toComparison();\n+        } else if (o1Matches) {\n+          return -1;\n+        } else {\n+          return 1;\n+        }\n+      }\n     }\n-    \n-    protected UniqueTransform getUniqueTransform() {\n-        if (uniqueTransform == null && getUniqueFields() != null && !getUniqueFields().isEmpty()) {\n-            synchronized (getUniqueFields()) {\n-                if (uniqueTransform == null) {\n-                    uniqueTransform = new UniqueTransform(getUniqueFields());\n-                }\n-            }\n+  }\n+\n+  /**\n+   * This can be overridden to supply a value comparator for use within the jexl context. Useful when using the HitListArithmetic which pulls back which value\n+   * tuples were actually hit upon.\n+   *\n+   * @param from\n+   * @return A comparator for values within the jexl context.\n+   */\n+  @Override\n+  public Comparator<Object> getValueComparator(Tuple3<Key, Document, Map<String, Object>> from) {\n+    return new ValueComparator(from.second().getMetadata());\n+  }\n+\n+  protected UniqueTransform getUniqueTransform() {\n+    if (uniqueTransform == null && getUniqueFields() != null && !getUniqueFields().isEmpty()) {\n+      synchronized (getUniqueFields()) {\n+        if (uniqueTransform == null) {\n+          uniqueTransform = new UniqueTransform(getUniqueFields());\n         }\n-        return uniqueTransform;\n+      }\n     }\n-    \n-    protected GroupingTransform getGroupingTransform() {\n-        if (groupingTransform == null && getGroupFields() != null && !getGroupFields().isEmpty()) {\n-            synchronized (getGroupFields()) {\n-                if (groupingTransform == null) {\n-                    groupingTransform = new GroupingTransform(null, getGroupFields(), true);\n-                    groupingTransform.initialize(null, MarkingFunctionsFactory.createMarkingFunctions());\n-                }\n-            }\n+    return uniqueTransform;\n+  }\n+\n+  protected GroupingTransform getGroupingTransform() {\n+    if (groupingTransform == null && getGroupFields() != null && !getGroupFields().isEmpty()) {\n+      synchronized (getGroupFields()) {\n+        if (groupingTransform == null) {\n+          groupingTransform = new GroupingTransform(null, getGroupFields(), true);\n+          groupingTransform.initialize(null, MarkingFunctionsFactory.createMarkingFunctions());\n         }\n-        return groupingTransform;\n+      }\n     }\n+    return groupingTransform;\n+  }\n }\n",
            "diff_size": 2178
        },
        {
            "tool": "naturalize",
            "errors": null,
            "diff": null
        },
        {
            "tool": "codebuff",
            "errors": null,
            "diff": null
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "89",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "89",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        }
    ],
    "repaired_by": [],
    "not_repaired_by": [
        "styler",
        "intellij",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ]
}