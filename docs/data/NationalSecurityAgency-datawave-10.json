{
    "project_name": "NationalSecurityAgency-datawave",
    "error_id": "10",
    "information": {
        "errors": [
            {
                "line": "15",
                "severity": "error",
                "message": "Accumulo non-public classes imported",
                "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
            }
        ]
    },
    "source_code": "import org.apache.accumulo.core.data.Range;\nimport org.apache.accumulo.core.data.TabletId;\nimport org.apache.accumulo.fate.util.UtilWaitThread;\nimport org.apache.commons.lang.time.DateUtils;\nimport org.apache.commons.lang3.mutable.MutableInt;\nimport org.apache.hadoop.conf.Configuration;",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "15",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "intellij",
            "errors": [
                {
                    "line": "15",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/10/ShardedTableMapFile.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/intellij/10/ShardedTableMapFile.java\nindex cd6f89fb549..df9c91e586f 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/10/ShardedTableMapFile.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/intellij/10/ShardedTableMapFile.java\n@@ -38,379 +38,368 @@ import java.util.TreeMap;\n  * Extracted from IngestJob\n  */\n public class ShardedTableMapFile {\n-    public static final String SHARDS_BALANCED_DAYS_TO_VERIFY = \"shards.balanced.days.to.verify\";\n-    private static final String PREFIX = ShardedTableMapFile.class.getName();\n-    private static final int MAX_RETRY_ATTEMPTS = 10;\n-    \n-    private static final Logger log = Logger.getLogger(ShardedTableMapFile.class);\n-    \n-    public static final String TABLE_NAMES = \"job.table.names\";\n-    public static final String SHARD_TSERVER_MAP_FILE = PREFIX + \".shardTServerMapFile\";\n-    public static final String SPLIT_WORK_DIR = \"split.work.dir\";\n-    \n-    public static final String CONFIGURED_SHARDED_TABLE_NAMES = ShardedDataTypeHandler.SHARDED_TNAMES + \".configured\";\n-    public static final String SHARDED_MAP_FILE_PATHS_RAW = \"shardedMap.file.paths.raw\";\n-    public static final String SHARD_VALIDATION_ENABLED = \"shardedMap.validation.enabled\";\n-    public static final String MAX_SHARDS_PER_TSERVER = \"shardedMap.max.shards.per.tserver\";\n-    \n-    public static void setupFile(Configuration conf) throws IOException, URISyntaxException, AccumuloSecurityException, AccumuloException {\n-        // want validation turned off by default\n-        boolean doValidation = conf.getBoolean(ShardedTableMapFile.SHARD_VALIDATION_ENABLED, false);\n-        \n-        Map<String,Path> map = loadMap(conf, doValidation);\n-        if (null == map) {\n-            log.fatal(\"Receieved a null mapping of sharded tables to split files, exiting...\");\n-            throw new RuntimeException(\"Receieved a null mapping of sharded tables to split files, exiting...\");\n-        }\n-        addToConf(conf, map);\n+  public static final String SHARDS_BALANCED_DAYS_TO_VERIFY = \"shards.balanced.days.to.verify\";\n+  private static final String PREFIX = ShardedTableMapFile.class.getName();\n+  private static final int MAX_RETRY_ATTEMPTS = 10;\n+\n+  private static final Logger log = Logger.getLogger(ShardedTableMapFile.class);\n+\n+  public static final String TABLE_NAMES = \"job.table.names\";\n+  public static final String SHARD_TSERVER_MAP_FILE = PREFIX + \".shardTServerMapFile\";\n+  public static final String SPLIT_WORK_DIR = \"split.work.dir\";\n+\n+  public static final String CONFIGURED_SHARDED_TABLE_NAMES = ShardedDataTypeHandler.SHARDED_TNAMES + \".configured\";\n+  public static final String SHARDED_MAP_FILE_PATHS_RAW = \"shardedMap.file.paths.raw\";\n+  public static final String SHARD_VALIDATION_ENABLED = \"shardedMap.validation.enabled\";\n+  public static final String MAX_SHARDS_PER_TSERVER = \"shardedMap.max.shards.per.tserver\";\n+\n+  public static void setupFile(Configuration conf)\n+      throws IOException, URISyntaxException, AccumuloSecurityException, AccumuloException {\n+    // want validation turned off by default\n+    boolean doValidation = conf.getBoolean(ShardedTableMapFile.SHARD_VALIDATION_ENABLED, false);\n+\n+    Map<String, Path> map = loadMap(conf, doValidation);\n+    if (null == map) {\n+      log.fatal(\"Receieved a null mapping of sharded tables to split files, exiting...\");\n+      throw new RuntimeException(\"Receieved a null mapping of sharded tables to split files, exiting...\");\n     }\n-    \n-    private static SequenceFile.Reader getReader(Configuration conf, String tableName) throws IOException {\n-        String shardMapFileName = conf.get(SHARD_TSERVER_MAP_FILE + \".\" + tableName);\n-        try {\n-            return new SequenceFile.Reader(FileSystem.get(conf), new Path(shardMapFileName), conf);\n-        } catch (Exception e) {\n-            throw new IOException(\"Failed to create sequence file reader for \" + shardMapFileName, e);\n-        }\n+    addToConf(conf, map);\n+  }\n+\n+  private static SequenceFile.Reader getReader(Configuration conf, String tableName) throws IOException {\n+    String shardMapFileName = conf.get(SHARD_TSERVER_MAP_FILE + \".\" + tableName);\n+    try {\n+      return new SequenceFile.Reader(FileSystem.get(conf), new Path(shardMapFileName), conf);\n+    } catch (Exception e) {\n+      throw new IOException(\"Failed to create sequence file reader for \" + shardMapFileName, e);\n     }\n-    \n-    public static TreeMap<Text,String> getShardIdToLocations(Configuration conf, String tableName) throws IOException {\n-        TreeMap<Text,String> locations = new TreeMap<>();\n-        \n-        SequenceFile.Reader reader = ShardedTableMapFile.getReader(conf, tableName);\n-        \n-        Text shardID = new Text();\n-        Text location = new Text();\n-        \n-        try {\n-            while (reader.next(shardID, location)) {\n-                locations.put(new Text(shardID), location.toString());\n-            }\n-        } finally {\n-            reader.close();\n-        }\n-        return locations;\n+  }\n+\n+  public static TreeMap<Text, String> getShardIdToLocations(Configuration conf, String tableName) throws IOException {\n+    TreeMap<Text, String> locations = new TreeMap<>();\n+\n+    SequenceFile.Reader reader = ShardedTableMapFile.getReader(conf, tableName);\n+\n+    Text shardID = new Text();\n+    Text location = new Text();\n+\n+    try {\n+      while (reader.next(shardID, location)) {\n+        locations.put(new Text(shardID), location.toString());\n+      }\n+    } finally {\n+      reader.close();\n     }\n-    \n-    public static void validateShardIdLocations(Configuration conf, String tableName, int daysToVerify, Map<Text,String> shardIdToLocation) {\n-        ShardIdFactory shardIdFactory = new ShardIdFactory(conf);\n-        // assume true unless proven otherwise\n-        boolean isValid = true;\n-        int maxShardsPerTserver = conf.getInt(MAX_SHARDS_PER_TSERVER, 1);\n-        \n-        for (int daysAgo = 0; daysAgo <= daysToVerify; daysAgo++) {\n-            long inMillis = System.currentTimeMillis() - (daysAgo * DateUtils.MILLIS_PER_DAY);\n-            String datePrefix = DateHelper.format(inMillis);\n-            int expectedNumberOfShards = shardIdFactory.getNumShards(datePrefix);\n-            boolean shardsExist = shardsExistForDate(shardIdToLocation, datePrefix, expectedNumberOfShards);\n-            if (!shardsExist) {\n-                log.error(\"Shards for \" + datePrefix + \" for table \" + tableName + \" do not exist!\");\n-                isValid = false;\n-                continue;\n-            }\n-            boolean shardsAreBalanced = shardsAreBalanced(shardIdToLocation, datePrefix, maxShardsPerTserver);\n-            if (!shardsAreBalanced) {\n-                log.error(\"Shards for \" + datePrefix + \" for table \" + tableName + \" are not balanced!\");\n-                isValid = false;\n-            }\n-        }\n-        if (!isValid) {\n-            throw new IllegalStateException(\"Shard validation failed for \" + tableName + \". Please ensure that \"\n-                            + \"shards have been generated. Check log for details about the dates in question\");\n-        }\n+    return locations;\n+  }\n+\n+  public static void validateShardIdLocations(Configuration conf, String tableName, int daysToVerify,\n+                                              Map<Text, String> shardIdToLocation) {\n+    ShardIdFactory shardIdFactory = new ShardIdFactory(conf);\n+    // assume true unless proven otherwise\n+    boolean isValid = true;\n+    int maxShardsPerTserver = conf.getInt(MAX_SHARDS_PER_TSERVER, 1);\n+\n+    for (int daysAgo = 0; daysAgo <= daysToVerify; daysAgo++) {\n+      long inMillis = System.currentTimeMillis() - (daysAgo * DateUtils.MILLIS_PER_DAY);\n+      String datePrefix = DateHelper.format(inMillis);\n+      int expectedNumberOfShards = shardIdFactory.getNumShards(datePrefix);\n+      boolean shardsExist = shardsExistForDate(shardIdToLocation, datePrefix, expectedNumberOfShards);\n+      if (!shardsExist) {\n+        log.error(\"Shards for \" + datePrefix + \" for table \" + tableName + \" do not exist!\");\n+        isValid = false;\n+        continue;\n+      }\n+      boolean shardsAreBalanced = shardsAreBalanced(shardIdToLocation, datePrefix, maxShardsPerTserver);\n+      if (!shardsAreBalanced) {\n+        log.error(\"Shards for \" + datePrefix + \" for table \" + tableName + \" are not balanced!\");\n+        isValid = false;\n+      }\n     }\n-    \n-    /**\n-     * Existence check for the shard splits for the specified date\n-     *\n-     * @param locations\n-     *            mapping of shard to tablet\n-     * @param datePrefix\n-     *            to check\n-     * @param expectedNumberOfShards\n-     *            that should exist\n-     * @return if the number of shards for the given date are as expected\n-     */\n-    private static boolean shardsExistForDate(Map<Text,String> locations, String datePrefix, int expectedNumberOfShards) {\n-        int count = 0;\n-        byte[] prefixBytes = datePrefix.getBytes();\n-        for (Text key : locations.keySet()) {\n-            if (prefixMatches(prefixBytes, key.getBytes(), key.getLength())) {\n-                count++;\n-            }\n-        }\n-        return count == expectedNumberOfShards;\n+    if (!isValid) {\n+      throw new IllegalStateException(\"Shard validation failed for \" + tableName + \". Please ensure that \"\n+          + \"shards have been generated. Check log for details about the dates in question\");\n     }\n-    \n-    /**\n-     * Checks that the shard splits for the given date have been assigned to unique tablets.\n-     *\n-     * @param locations\n-     *            mapping of shard to tablet\n-     * @param datePrefix\n-     *            to check\n-     * @return if the shards are distributed in a balanced fashion\n-     */\n-    private static boolean shardsAreBalanced(Map<Text,String> locations, String datePrefix, int maxShardsPerTserver) {\n-        // assume true unless proven wrong\n-        boolean dateIsBalanced = true;\n-        \n-        Map<String,MutableInt> tabletsSeenForDate = new HashMap<>();\n-        byte[] prefixBytes = datePrefix.getBytes();\n-        \n-        for (Entry<Text,String> entry : locations.entrySet()) {\n-            Text key = entry.getKey();\n-            // only check entries for specified date\n-            if (prefixMatches(prefixBytes, key.getBytes(), key.getLength())) {\n-                String value = entry.getValue();\n-                MutableInt cnt = tabletsSeenForDate.get(value);\n-                if (null == cnt) {\n-                    cnt = new MutableInt(0);\n-                }\n-                // increment here before checking\n-                cnt.increment();\n-                \n-                // if shard is assigned to more tservers than allowed, then the shards are not balanced\n-                if (cnt.intValue() > maxShardsPerTserver) {\n-                    log.warn(cnt.toInteger() + \" Shards for \" + datePrefix + \" assigned to tablet \" + value);\n-                    dateIsBalanced = false;\n-                }\n-                \n-                tabletsSeenForDate.put(value, cnt);\n-            }\n-        }\n-        \n-        return dateIsBalanced;\n+  }\n+\n+  /**\n+   * Existence check for the shard splits for the specified date\n+   *\n+   * @param locations              mapping of shard to tablet\n+   * @param datePrefix             to check\n+   * @param expectedNumberOfShards that should exist\n+   * @return if the number of shards for the given date are as expected\n+   */\n+  private static boolean shardsExistForDate(Map<Text, String> locations, String datePrefix,\n+                                            int expectedNumberOfShards) {\n+    int count = 0;\n+    byte[] prefixBytes = datePrefix.getBytes();\n+    for (Text key : locations.keySet()) {\n+      if (prefixMatches(prefixBytes, key.getBytes(), key.getLength())) {\n+        count++;\n+      }\n     }\n-    \n-    private static boolean prefixMatches(byte[] prefixBytes, byte[] keyBytes, int keyLen) {\n-        // if key length is less than prefix size, no use comparing\n-        if (prefixBytes.length > keyLen) {\n-            return false;\n+    return count == expectedNumberOfShards;\n+  }\n+\n+  /**\n+   * Checks that the shard splits for the given date have been assigned to unique tablets.\n+   *\n+   * @param locations  mapping of shard to tablet\n+   * @param datePrefix to check\n+   * @return if the shards are distributed in a balanced fashion\n+   */\n+  private static boolean shardsAreBalanced(Map<Text, String> locations, String datePrefix, int maxShardsPerTserver) {\n+    // assume true unless proven wrong\n+    boolean dateIsBalanced = true;\n+\n+    Map<String, MutableInt> tabletsSeenForDate = new HashMap<>();\n+    byte[] prefixBytes = datePrefix.getBytes();\n+\n+    for (Entry<Text, String> entry : locations.entrySet()) {\n+      Text key = entry.getKey();\n+      // only check entries for specified date\n+      if (prefixMatches(prefixBytes, key.getBytes(), key.getLength())) {\n+        String value = entry.getValue();\n+        MutableInt cnt = tabletsSeenForDate.get(value);\n+        if (null == cnt) {\n+          cnt = new MutableInt(0);\n         }\n-        for (int i = 0; i < prefixBytes.length; i++) {\n-            if (prefixBytes[i] != keyBytes[i]) {\n-                return false;\n-            }\n+        // increment here before checking\n+        cnt.increment();\n+\n+        // if shard is assigned to more tservers than allowed, then the shards are not balanced\n+        if (cnt.intValue() > maxShardsPerTserver) {\n+          log.warn(cnt.toInteger() + \" Shards for \" + datePrefix + \" assigned to tablet \" + value);\n+          dateIsBalanced = false;\n         }\n-        // at this point didn't fail match, so should be good\n-        return true;\n+\n+        tabletsSeenForDate.put(value, cnt);\n+      }\n     }\n-    \n-    public static void addToConf(Configuration conf, Map<String,Path> map) {\n-        for (Map.Entry<String,Path> entry : map.entrySet()) {\n-            log.info(\"Loading sharded partitioner for table '\" + entry.getKey() + \"' with shardedMapFile '\" + entry.getValue() + \"'\");\n-            conf.set(SHARD_TSERVER_MAP_FILE + \".\" + entry.getKey(), entry.getValue().toString());\n-        }\n-        Set<String> var = map.keySet();\n-        conf.setStrings(CONFIGURED_SHARDED_TABLE_NAMES, var.toArray(new String[var.size()]));\n+\n+    return dateIsBalanced;\n+  }\n+\n+  private static boolean prefixMatches(byte[] prefixBytes, byte[] keyBytes, int keyLen) {\n+    // if key length is less than prefix size, no use comparing\n+    if (prefixBytes.length > keyLen) {\n+      return false;\n     }\n-    \n-    private static Map<String,Path> loadMap(Configuration conf, boolean doValidation) throws IOException, URISyntaxException, AccumuloSecurityException,\n-                    AccumuloException {\n-        AccumuloHelper accumuloHelper = null;\n-        Path workDir = new Path(conf.get(SPLIT_WORK_DIR));// todo make sure this is set in ingest job\n-        String[] tableNames = StringUtils.split(conf.get(TABLE_NAMES), \",\");// todo make sure this is set in ingest job\n-        \n-        Map<String,String> shardedTableMapFilePaths = extractShardedTableMapFilePaths(conf);\n-        // Get a list of \"sharded\" tables\n-        String[] shardedTableNames = ConfigurationHelper.isNull(conf, ShardedDataTypeHandler.SHARDED_TNAMES, String[].class);\n-        Set<String> configuredShardedTableNames = new HashSet<>(Arrays.asList(shardedTableNames));\n-        \n-        // Remove all \"sharded\" tables that we aren't actually outputting to\n-        configuredShardedTableNames.retainAll(Arrays.asList(tableNames));\n-        \n-        Map<String,Path> shardedTableMapFiles = new HashMap<>();\n-        \n-        // Pull the list of table that we \"shard\":\n-        // Use the sequence file of splits for the current table, or pull them off of the configured Instance\n-        for (String shardedTableName : configuredShardedTableNames) {\n-            Path shardedMapFile;\n-            \n-            // If an existing splits file was provided on the command line, use it.\n-            // Otherwise, calculate one from the Accumulo instance\n-            if (shardedTableMapFilePaths.containsKey(shardedTableName) && null != shardedTableMapFilePaths.get(shardedTableName)) {\n-                shardedMapFile = new Path(shardedTableMapFilePaths.get(shardedTableName));\n-            } else {\n-                if (null == accumuloHelper) {\n-                    accumuloHelper = new AccumuloHelper();\n-                    accumuloHelper.setup(conf);\n-                }\n-                shardedMapFile = createShardedMapFile(log, conf, workDir, accumuloHelper, shardedTableName, doValidation);\n-            }\n-            \n-            // Ensure that we either computed, or were given, a valid path to the shard mappings\n-            if (!shardedMapFile.getFileSystem(conf).exists(shardedMapFile)) {\n-                log.fatal(\"Could not find the supplied shard map file: \" + shardedMapFile);\n-                return null;\n-            } else {\n-                shardedTableMapFiles.put(shardedTableName, shardedMapFile);\n-            }\n-        }\n-        \n-        return shardedTableMapFiles;\n+    for (int i = 0; i < prefixBytes.length; i++) {\n+      if (prefixBytes[i] != keyBytes[i]) {\n+        return false;\n+      }\n     }\n-    \n-    static Map<String,String> extractShardedTableMapFilePaths(Configuration conf) {\n-        Map<String,String> shardedTableMapFilePaths = new HashMap<>();\n-        String commaSeparatedFileNamesByTable = conf.get(SHARDED_MAP_FILE_PATHS_RAW);\n-        if (null == commaSeparatedFileNamesByTable) {\n-            return shardedTableMapFilePaths;\n-        }\n-        \n-        String[] pairs = StringUtils.split(commaSeparatedFileNamesByTable, ',');\n-        \n-        for (String pair : pairs) {\n-            int index = pair.indexOf('=');\n-            if (index < 0) {\n-                log.warn(\"WARN: Skipping bad tableName=/path/to/tableNameSplits.seq property: \" + pair);\n-            } else {\n-                String tableName = pair.substring(0, index), splitsFile = pair.substring(index + 1);\n-                \n-                log.info(\"Using splits file '\" + splitsFile + \"' for table '\" + tableName + \"'\");\n-                \n-                shardedTableMapFilePaths.put(tableName, splitsFile);\n-            }\n-        }\n-        return shardedTableMapFilePaths;\n+    // at this point didn't fail match, so should be good\n+    return true;\n+  }\n+\n+  public static void addToConf(Configuration conf, Map<String, Path> map) {\n+    for (Map.Entry<String, Path> entry : map.entrySet()) {\n+      log.info(\n+          \"Loading sharded partitioner for table '\" + entry.getKey() + \"' with shardedMapFile '\" + entry.getValue() +\n+              \"'\");\n+      conf.set(SHARD_TSERVER_MAP_FILE + \".\" + entry.getKey(), entry.getValue().toString());\n     }\n-    \n-    /**\n-     * Build a file that maps shard IDs (row keys in the sharded table) to the tablet server where they are currently stored.\n-     *\n-     * @param log\n-     *            logger for reporting errors\n-     * @param conf\n-     *            hadoop configuration\n-     * @param workDir\n-     *            base dir in HDFS where the file is written\n-     * @param accumuloHelper\n-     *            Accumulo helper to query shard locations\n-     * @param shardedTableName\n-     *            name of the shard table--the table whose locations we are querying\n-     * @param validateShardLocations\n-     *            if validation of shards mappings should be performed\n-     * @return the path to the sharded table map file\n-     * @throws IOException\n-     * @throws URISyntaxException\n-     */\n-    public static Path createShardedMapFile(Logger log, Configuration conf, Path workDir, AccumuloHelper accumuloHelper, String shardedTableName,\n-                    boolean validateShardLocations) throws IOException, URISyntaxException {\n-        Path shardedMapFile = null;\n-        // minus one to make zero based indexed\n-        int daysToVerify = conf.getInt(SHARDS_BALANCED_DAYS_TO_VERIFY, 2) - 1;\n-        \n-        if (null != shardedTableName) {\n-            // Read all the metadata entries for the sharded table so that we can\n-            // get the mapping of shard IDs to tablet locations.\n-            log.info(\"Reading metadata entries for \" + shardedTableName);\n-            \n-            Map<Text,String> splitToLocations = getLocations(log, accumuloHelper, shardedTableName);\n-            if (validateShardLocations) {\n-                validateShardIdLocations(conf, shardedTableName, daysToVerify, splitToLocations);\n-            }\n-            \n-            // Now write all of the assignments out to a file stored in HDFS\n-            // we're ok with putting the sharded table file in the hdfs workdir. why is that not good enough for the non sharded splits?\n-            shardedMapFile = new Path(workDir, shardedTableName + \"_shards.lst\");\n-            log.info(\"Writing shard assignments to \" + shardedMapFile);\n-            long count = writeSplitsFile(splitToLocations, shardedMapFile, conf);\n-            log.info(\"Wrote \" + count + \" shard assignments to \" + shardedMapFile);\n+    Set<String> var = map.keySet();\n+    conf.setStrings(CONFIGURED_SHARDED_TABLE_NAMES, var.toArray(new String[var.size()]));\n+  }\n+\n+  private static Map<String, Path> loadMap(Configuration conf, boolean doValidation)\n+      throws IOException, URISyntaxException, AccumuloSecurityException,\n+      AccumuloException {\n+    AccumuloHelper accumuloHelper = null;\n+    Path workDir = new Path(conf.get(SPLIT_WORK_DIR));// todo make sure this is set in ingest job\n+    String[] tableNames = StringUtils.split(conf.get(TABLE_NAMES), \",\");// todo make sure this is set in ingest job\n+\n+    Map<String, String> shardedTableMapFilePaths = extractShardedTableMapFilePaths(conf);\n+    // Get a list of \"sharded\" tables\n+    String[] shardedTableNames =\n+        ConfigurationHelper.isNull(conf, ShardedDataTypeHandler.SHARDED_TNAMES, String[].class);\n+    Set<String> configuredShardedTableNames = new HashSet<>(Arrays.asList(shardedTableNames));\n+\n+    // Remove all \"sharded\" tables that we aren't actually outputting to\n+    configuredShardedTableNames.retainAll(Arrays.asList(tableNames));\n+\n+    Map<String, Path> shardedTableMapFiles = new HashMap<>();\n+\n+    // Pull the list of table that we \"shard\":\n+    // Use the sequence file of splits for the current table, or pull them off of the configured Instance\n+    for (String shardedTableName : configuredShardedTableNames) {\n+      Path shardedMapFile;\n+\n+      // If an existing splits file was provided on the command line, use it.\n+      // Otherwise, calculate one from the Accumulo instance\n+      if (shardedTableMapFilePaths.containsKey(shardedTableName) &&\n+          null != shardedTableMapFilePaths.get(shardedTableName)) {\n+        shardedMapFile = new Path(shardedTableMapFilePaths.get(shardedTableName));\n+      } else {\n+        if (null == accumuloHelper) {\n+          accumuloHelper = new AccumuloHelper();\n+          accumuloHelper.setup(conf);\n         }\n-        \n-        return shardedMapFile;\n+        shardedMapFile = createShardedMapFile(log, conf, workDir, accumuloHelper, shardedTableName, doValidation);\n+      }\n+\n+      // Ensure that we either computed, or were given, a valid path to the shard mappings\n+      if (!shardedMapFile.getFileSystem(conf).exists(shardedMapFile)) {\n+        log.fatal(\"Could not find the supplied shard map file: \" + shardedMapFile);\n+        return null;\n+      } else {\n+        shardedTableMapFiles.put(shardedTableName, shardedMapFile);\n+      }\n     }\n-    \n-    /**\n-     * Continually scans the metdata table attempting to get the split locations for the shard table.\n-     *\n-     * @param log\n-     *            logger for reporting errors\n-     * @param accumuloHelper\n-     *            Accumulo helper to query shard locations\n-     * @param shardedTableName\n-     *            name of the shard table--the table whose locations we are querying\n-     * @return a map of split (endRow) to the location of those tablets in accumulo\n-     */\n-    public static Map<Text,String> getLocations(Logger log, AccumuloHelper accumuloHelper, String shardedTableName) {\n-        // split (endRow) -> String location mapping\n-        Map<Text,String> splitToLocation = new TreeMap<>();\n-        \n-        boolean keepRetrying = true;\n-        int attempts = 0;\n-        while (keepRetrying && attempts < MAX_RETRY_ATTEMPTS) {\n-            try {\n-                TableOperations tableOps = accumuloHelper.getConnector().tableOperations();\n-                attempts++;\n-                // if table does not exist don't want to catch the errors and end up in infinite loop\n-                if (!tableOps.exists(shardedTableName)) {\n-                    log.error(\"Table \" + shardedTableName + \" not found, skipping split locations for missing table\");\n-                } else {\n-                    Range range = new Range();\n-                    Locations locations = tableOps.locate(shardedTableName, Collections.singletonList(range));\n-                    List<TabletId> tabletIds = locations.groupByRange().get(range);\n-                    \n-                    tabletIds.stream().filter(tId -> tId.getEndRow() != null)\n-                                    .forEach(tId -> splitToLocation.put(tId.getEndRow(), locations.getTabletLocation(tId)));\n-                }\n-                // made it here, no errors so break out\n-                keepRetrying = false;\n-            } catch (Exception e) {\n-                log.warn(e.getMessage() + \" ... retrying ...\");\n-                UtilWaitThread.sleep(3000);\n-            }\n-        }\n-        \n-        return splitToLocation;\n+\n+    return shardedTableMapFiles;\n+  }\n+\n+  static Map<String, String> extractShardedTableMapFilePaths(Configuration conf) {\n+    Map<String, String> shardedTableMapFilePaths = new HashMap<>();\n+    String commaSeparatedFileNamesByTable = conf.get(SHARDED_MAP_FILE_PATHS_RAW);\n+    if (null == commaSeparatedFileNamesByTable) {\n+      return shardedTableMapFilePaths;\n+    }\n+\n+    String[] pairs = StringUtils.split(commaSeparatedFileNamesByTable, ',');\n+\n+    for (String pair : pairs) {\n+      int index = pair.indexOf('=');\n+      if (index < 0) {\n+        log.warn(\"WARN: Skipping bad tableName=/path/to/tableNameSplits.seq property: \" + pair);\n+      } else {\n+        String tableName = pair.substring(0, index), splitsFile = pair.substring(index + 1);\n+\n+        log.info(\"Using splits file '\" + splitsFile + \"' for table '\" + tableName + \"'\");\n+\n+        shardedTableMapFilePaths.put(tableName, splitsFile);\n+      }\n     }\n-    \n-    /**\n-     * Writes the contents of splits out to a sequence file on the given FileSystem.\n-     *\n-     * @param splits\n-     *            map of split points for a table\n-     * @param file\n-     *            the file to which the splits should be written\n-     * @param conf\n-     *            hadoop configuration\n-     * @return the number of entries written to the splits file\n-     * @throws IOException\n-     *             if file system interaction fails\n-     */\n-    public static long writeSplitsFile(Map<Text,String> splits, Path file, Configuration conf) throws IOException {\n-        FileSystem fs = file.getFileSystem(conf);\n-        if (fs.exists(file))\n-            fs.delete(file, false);\n-        \n-        long count = 0;\n-        // reusable value for writing\n-        Text value = new Text();\n-        SequenceFile.Writer writer = SequenceFile.createWriter(conf, SequenceFile.Writer.file(file), SequenceFile.Writer.keyClass(Text.class),\n-                        SequenceFile.Writer.valueClass(Text.class));\n-        for (Entry<Text,String> entry : splits.entrySet()) {\n-            count++;\n-            value.set(entry.getValue());\n-            writer.append(entry.getKey(), value);\n+    return shardedTableMapFilePaths;\n+  }\n+\n+  /**\n+   * Build a file that maps shard IDs (row keys in the sharded table) to the tablet server where they are currently stored.\n+   *\n+   * @param log                    logger for reporting errors\n+   * @param conf                   hadoop configuration\n+   * @param workDir                base dir in HDFS where the file is written\n+   * @param accumuloHelper         Accumulo helper to query shard locations\n+   * @param shardedTableName       name of the shard table--the table whose locations we are querying\n+   * @param validateShardLocations if validation of shards mappings should be performed\n+   * @return the path to the sharded table map file\n+   * @throws IOException\n+   * @throws URISyntaxException\n+   */\n+  public static Path createShardedMapFile(Logger log, Configuration conf, Path workDir, AccumuloHelper accumuloHelper,\n+                                          String shardedTableName,\n+                                          boolean validateShardLocations) throws IOException, URISyntaxException {\n+    Path shardedMapFile = null;\n+    // minus one to make zero based indexed\n+    int daysToVerify = conf.getInt(SHARDS_BALANCED_DAYS_TO_VERIFY, 2) - 1;\n+\n+    if (null != shardedTableName) {\n+      // Read all the metadata entries for the sharded table so that we can\n+      // get the mapping of shard IDs to tablet locations.\n+      log.info(\"Reading metadata entries for \" + shardedTableName);\n+\n+      Map<Text, String> splitToLocations = getLocations(log, accumuloHelper, shardedTableName);\n+      if (validateShardLocations) {\n+        validateShardIdLocations(conf, shardedTableName, daysToVerify, splitToLocations);\n+      }\n+\n+      // Now write all of the assignments out to a file stored in HDFS\n+      // we're ok with putting the sharded table file in the hdfs workdir. why is that not good enough for the non sharded splits?\n+      shardedMapFile = new Path(workDir, shardedTableName + \"_shards.lst\");\n+      log.info(\"Writing shard assignments to \" + shardedMapFile);\n+      long count = writeSplitsFile(splitToLocations, shardedMapFile, conf);\n+      log.info(\"Wrote \" + count + \" shard assignments to \" + shardedMapFile);\n+    }\n+\n+    return shardedMapFile;\n+  }\n+\n+  /**\n+   * Continually scans the metdata table attempting to get the split locations for the shard table.\n+   *\n+   * @param log              logger for reporting errors\n+   * @param accumuloHelper   Accumulo helper to query shard locations\n+   * @param shardedTableName name of the shard table--the table whose locations we are querying\n+   * @return a map of split (endRow) to the location of those tablets in accumulo\n+   */\n+  public static Map<Text, String> getLocations(Logger log, AccumuloHelper accumuloHelper, String shardedTableName) {\n+    // split (endRow) -> String location mapping\n+    Map<Text, String> splitToLocation = new TreeMap<>();\n+\n+    boolean keepRetrying = true;\n+    int attempts = 0;\n+    while (keepRetrying && attempts < MAX_RETRY_ATTEMPTS) {\n+      try {\n+        TableOperations tableOps = accumuloHelper.getConnector().tableOperations();\n+        attempts++;\n+        // if table does not exist don't want to catch the errors and end up in infinite loop\n+        if (!tableOps.exists(shardedTableName)) {\n+          log.error(\"Table \" + shardedTableName + \" not found, skipping split locations for missing table\");\n+        } else {\n+          Range range = new Range();\n+          Locations locations = tableOps.locate(shardedTableName, Collections.singletonList(range));\n+          List<TabletId> tabletIds = locations.groupByRange().get(range);\n+\n+          tabletIds.stream().filter(tId -> tId.getEndRow() != null)\n+              .forEach(tId -> splitToLocation.put(tId.getEndRow(), locations.getTabletLocation(tId)));\n         }\n-        writer.close();\n-        return count;\n+        // made it here, no errors so break out\n+        keepRetrying = false;\n+      } catch (Exception e) {\n+        log.warn(e.getMessage() + \" ... retrying ...\");\n+        UtilWaitThread.sleep(3000);\n+      }\n     }\n-    \n-    /**\n-     * Writes the contents of splits out to a sequence file on the given FileSystem.\n-     *\n-     * @param splits\n-     *            map of split points for a table\n-     * @param file\n-     *            the file to which the splits should be written\n-     * @param conf\n-     *            hadoop configuration\n-     * @return the number of entries written to the splits file\n-     * @throws IOException\n-     *             if file system interaction fails\n-     */\n-    public static long writeSplitsFileLegacy(Map<Text,String> splits, Path file, Configuration conf) throws IOException {\n-        return writeSplitsFile(splits, file, conf);\n+\n+    return splitToLocation;\n+  }\n+\n+  /**\n+   * Writes the contents of splits out to a sequence file on the given FileSystem.\n+   *\n+   * @param splits map of split points for a table\n+   * @param file   the file to which the splits should be written\n+   * @param conf   hadoop configuration\n+   * @return the number of entries written to the splits file\n+   * @throws IOException if file system interaction fails\n+   */\n+  public static long writeSplitsFile(Map<Text, String> splits, Path file, Configuration conf) throws IOException {\n+    FileSystem fs = file.getFileSystem(conf);\n+    if (fs.exists(file)) {\n+      fs.delete(file, false);\n     }\n-    \n+\n+    long count = 0;\n+    // reusable value for writing\n+    Text value = new Text();\n+    SequenceFile.Writer writer =\n+        SequenceFile.createWriter(conf, SequenceFile.Writer.file(file), SequenceFile.Writer.keyClass(Text.class),\n+            SequenceFile.Writer.valueClass(Text.class));\n+    for (Entry<Text, String> entry : splits.entrySet()) {\n+      count++;\n+      value.set(entry.getValue());\n+      writer.append(entry.getKey(), value);\n+    }\n+    writer.close();\n+    return count;\n+  }\n+\n+  /**\n+   * Writes the contents of splits out to a sequence file on the given FileSystem.\n+   *\n+   * @param splits map of split points for a table\n+   * @param file   the file to which the splits should be written\n+   * @param conf   hadoop configuration\n+   * @return the number of entries written to the splits file\n+   * @throws IOException if file system interaction fails\n+   */\n+  public static long writeSplitsFileLegacy(Map<Text, String> splits, Path file, Configuration conf) throws IOException {\n+    return writeSplitsFile(splits, file, conf);\n+  }\n+\n }\n",
            "diff_size": 467
        },
        {
            "tool": "naturalize",
            "errors": null,
            "diff": null
        },
        {
            "tool": "codebuff",
            "errors": [
                {
                    "line": "15",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/10/ShardedTableMapFile.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/codebuff/10/ShardedTableMapFile.java\nindex cd6f89fb549..b5157f46e36 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/10/ShardedTableMapFile.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/codebuff/10/ShardedTableMapFile.java\n@@ -21,7 +21,6 @@ import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.SequenceFile;\n import org.apache.hadoop.io.Text;\n import org.apache.log4j.Logger;\n-\n import java.io.IOException;\n import java.net.URISyntaxException;\n import java.util.Arrays;\n@@ -37,26 +36,31 @@ import java.util.TreeMap;\n /**\n  * Extracted from IngestJob\n  */\n+\n+\n public class ShardedTableMapFile {\n     public static final String SHARDS_BALANCED_DAYS_TO_VERIFY = \"shards.balanced.days.to.verify\";\n     private static final String PREFIX = ShardedTableMapFile.class.getName();\n     private static final int MAX_RETRY_ATTEMPTS = 10;\n-    \n     private static final Logger log = Logger.getLogger(ShardedTableMapFile.class);\n-    \n+\n     public static final String TABLE_NAMES = \"job.table.names\";\n+\n     public static final String SHARD_TSERVER_MAP_FILE = PREFIX + \".shardTServerMapFile\";\n+\n     public static final String SPLIT_WORK_DIR = \"split.work.dir\";\n-    \n+\n     public static final String CONFIGURED_SHARDED_TABLE_NAMES = ShardedDataTypeHandler.SHARDED_TNAMES + \".configured\";\n+\n     public static final String SHARDED_MAP_FILE_PATHS_RAW = \"shardedMap.file.paths.raw\";\n+\n     public static final String SHARD_VALIDATION_ENABLED = \"shardedMap.validation.enabled\";\n+\n     public static final String MAX_SHARDS_PER_TSERVER = \"shardedMap.max.shards.per.tserver\";\n-    \n+\n     public static void setupFile(Configuration conf) throws IOException, URISyntaxException, AccumuloSecurityException, AccumuloException {\n         // want validation turned off by default\n         boolean doValidation = conf.getBoolean(ShardedTableMapFile.SHARD_VALIDATION_ENABLED, false);\n-        \n         Map<String,Path> map = loadMap(conf, doValidation);\n         if (null == map) {\n             log.fatal(\"Receieved a null mapping of sharded tables to split files, exiting...\");\n@@ -64,7 +68,7 @@ public class ShardedTableMapFile {\n         }\n         addToConf(conf, map);\n     }\n-    \n+\n     private static SequenceFile.Reader getReader(Configuration conf, String tableName) throws IOException {\n         String shardMapFileName = conf.get(SHARD_TSERVER_MAP_FILE + \".\" + tableName);\n         try {\n@@ -73,15 +77,12 @@ public class ShardedTableMapFile {\n             throw new IOException(\"Failed to create sequence file reader for \" + shardMapFileName, e);\n         }\n     }\n-    \n+\n     public static TreeMap<Text,String> getShardIdToLocations(Configuration conf, String tableName) throws IOException {\n         TreeMap<Text,String> locations = new TreeMap<>();\n-        \n         SequenceFile.Reader reader = ShardedTableMapFile.getReader(conf, tableName);\n-        \n         Text shardID = new Text();\n         Text location = new Text();\n-        \n         try {\n             while (reader.next(shardID, location)) {\n                 locations.put(new Text(shardID), location.toString());\n@@ -91,13 +92,12 @@ public class ShardedTableMapFile {\n         }\n         return locations;\n     }\n-    \n+\n     public static void validateShardIdLocations(Configuration conf, String tableName, int daysToVerify, Map<Text,String> shardIdToLocation) {\n         ShardIdFactory shardIdFactory = new ShardIdFactory(conf);\n         // assume true unless proven otherwise\n         boolean isValid = true;\n         int maxShardsPerTserver = conf.getInt(MAX_SHARDS_PER_TSERVER, 1);\n-        \n         for (int daysAgo = 0; daysAgo <= daysToVerify; daysAgo++) {\n             long inMillis = System.currentTimeMillis() - (daysAgo * DateUtils.MILLIS_PER_DAY);\n             String datePrefix = DateHelper.format(inMillis);\n@@ -108,15 +108,17 @@ public class ShardedTableMapFile {\n                 isValid = false;\n                 continue;\n             }\n+\n             boolean shardsAreBalanced = shardsAreBalanced(shardIdToLocation, datePrefix, maxShardsPerTserver);\n             if (!shardsAreBalanced) {\n                 log.error(\"Shards for \" + datePrefix + \" for table \" + tableName + \" are not balanced!\");\n                 isValid = false;\n             }\n         }\n+\n+\n         if (!isValid) {\n-            throw new IllegalStateException(\"Shard validation failed for \" + tableName + \". Please ensure that \"\n-                            + \"shards have been generated. Check log for details about the dates in question\");\n+            throw new IllegalStateException(\"Shard validation failed for \" + tableName + \". Please ensure that \" + \"shards have been generated. Check log for details about the dates in question\");\n         }\n     }\n     \n@@ -131,6 +133,7 @@ public class ShardedTableMapFile {\n      *            that should exist\n      * @return if the number of shards for the given date are as expected\n      */\n+\n     private static boolean shardsExistForDate(Map<Text,String> locations, String datePrefix, int expectedNumberOfShards) {\n         int count = 0;\n         byte[] prefixBytes = datePrefix.getBytes();\n@@ -151,13 +154,12 @@ public class ShardedTableMapFile {\n      *            to check\n      * @return if the shards are distributed in a balanced fashion\n      */\n+\n     private static boolean shardsAreBalanced(Map<Text,String> locations, String datePrefix, int maxShardsPerTserver) {\n         // assume true unless proven wrong\n         boolean dateIsBalanced = true;\n-        \n         Map<String,MutableInt> tabletsSeenForDate = new HashMap<>();\n         byte[] prefixBytes = datePrefix.getBytes();\n-        \n         for (Entry<Text,String> entry : locations.entrySet()) {\n             Text key = entry.getKey();\n             // only check entries for specified date\n@@ -175,14 +177,12 @@ public class ShardedTableMapFile {\n                     log.warn(cnt.toInteger() + \" Shards for \" + datePrefix + \" assigned to tablet \" + value);\n                     dateIsBalanced = false;\n                 }\n-                \n                 tabletsSeenForDate.put(value, cnt);\n             }\n         }\n-        \n         return dateIsBalanced;\n     }\n-    \n+\n     private static boolean prefixMatches(byte[] prefixBytes, byte[] keyBytes, int keyLen) {\n         // if key length is less than prefix size, no use comparing\n         if (prefixBytes.length > keyLen) {\n@@ -196,22 +196,21 @@ public class ShardedTableMapFile {\n         // at this point didn't fail match, so should be good\n         return true;\n     }\n-    \n+\n     public static void addToConf(Configuration conf, Map<String,Path> map) {\n         for (Map.Entry<String,Path> entry : map.entrySet()) {\n             log.info(\"Loading sharded partitioner for table '\" + entry.getKey() + \"' with shardedMapFile '\" + entry.getValue() + \"'\");\n             conf.set(SHARD_TSERVER_MAP_FILE + \".\" + entry.getKey(), entry.getValue().toString());\n         }\n+\n         Set<String> var = map.keySet();\n         conf.setStrings(CONFIGURED_SHARDED_TABLE_NAMES, var.toArray(new String[var.size()]));\n     }\n-    \n-    private static Map<String,Path> loadMap(Configuration conf, boolean doValidation) throws IOException, URISyntaxException, AccumuloSecurityException,\n-                    AccumuloException {\n+\n+    private static Map<String,Path> loadMap(Configuration conf, boolean doValidation) throws IOException, URISyntaxException, AccumuloSecurityException, AccumuloException {\n         AccumuloHelper accumuloHelper = null;\n         Path workDir = new Path(conf.get(SPLIT_WORK_DIR));// todo make sure this is set in ingest job\n         String[] tableNames = StringUtils.split(conf.get(TABLE_NAMES), \",\");// todo make sure this is set in ingest job\n-        \n         Map<String,String> shardedTableMapFilePaths = extractShardedTableMapFilePaths(conf);\n         // Get a list of \"sharded\" tables\n         String[] shardedTableNames = ConfigurationHelper.isNull(conf, ShardedDataTypeHandler.SHARDED_TNAMES, String[].class);\n@@ -219,7 +218,7 @@ public class ShardedTableMapFile {\n         \n         // Remove all \"sharded\" tables that we aren't actually outputting to\n         configuredShardedTableNames.retainAll(Arrays.asList(tableNames));\n-        \n+\n         Map<String,Path> shardedTableMapFiles = new HashMap<>();\n         \n         // Pull the list of table that we \"shard\":\n@@ -240,6 +239,8 @@ public class ShardedTableMapFile {\n             }\n             \n             // Ensure that we either computed, or were given, a valid path to the shard mappings\n+\n+\n             if (!shardedMapFile.getFileSystem(conf).exists(shardedMapFile)) {\n                 log.fatal(\"Could not find the supplied shard map file: \" + shardedMapFile);\n                 return null;\n@@ -247,28 +248,24 @@ public class ShardedTableMapFile {\n                 shardedTableMapFiles.put(shardedTableName, shardedMapFile);\n             }\n         }\n-        \n         return shardedTableMapFiles;\n     }\n-    \n+\n     static Map<String,String> extractShardedTableMapFilePaths(Configuration conf) {\n         Map<String,String> shardedTableMapFilePaths = new HashMap<>();\n         String commaSeparatedFileNamesByTable = conf.get(SHARDED_MAP_FILE_PATHS_RAW);\n         if (null == commaSeparatedFileNamesByTable) {\n             return shardedTableMapFilePaths;\n         }\n-        \n+\n         String[] pairs = StringUtils.split(commaSeparatedFileNamesByTable, ',');\n-        \n         for (String pair : pairs) {\n             int index = pair.indexOf('=');\n             if (index < 0) {\n                 log.warn(\"WARN: Skipping bad tableName=/path/to/tableNameSplits.seq property: \" + pair);\n             } else {\n                 String tableName = pair.substring(0, index), splitsFile = pair.substring(index + 1);\n-                \n                 log.info(\"Using splits file '\" + splitsFile + \"' for table '\" + tableName + \"'\");\n-                \n                 shardedTableMapFilePaths.put(tableName, splitsFile);\n             }\n         }\n@@ -294,17 +291,16 @@ public class ShardedTableMapFile {\n      * @throws IOException\n      * @throws URISyntaxException\n      */\n-    public static Path createShardedMapFile(Logger log, Configuration conf, Path workDir, AccumuloHelper accumuloHelper, String shardedTableName,\n-                    boolean validateShardLocations) throws IOException, URISyntaxException {\n+\n+    public static Path createShardedMapFile(Logger log, Configuration conf, Path workDir, AccumuloHelper accumuloHelper, String shardedTableName, boolean validateShardLocations) throws IOException, URISyntaxException {\n         Path shardedMapFile = null;\n         // minus one to make zero based indexed\n         int daysToVerify = conf.getInt(SHARDS_BALANCED_DAYS_TO_VERIFY, 2) - 1;\n-        \n         if (null != shardedTableName) {\n             // Read all the metadata entries for the sharded table so that we can\n             // get the mapping of shard IDs to tablet locations.\n             log.info(\"Reading metadata entries for \" + shardedTableName);\n-            \n+\n             Map<Text,String> splitToLocations = getLocations(log, accumuloHelper, shardedTableName);\n             if (validateShardLocations) {\n                 validateShardIdLocations(conf, shardedTableName, daysToVerify, splitToLocations);\n@@ -317,7 +313,6 @@ public class ShardedTableMapFile {\n             long count = writeSplitsFile(splitToLocations, shardedMapFile, conf);\n             log.info(\"Wrote \" + count + \" shard assignments to \" + shardedMapFile);\n         }\n-        \n         return shardedMapFile;\n     }\n     \n@@ -332,10 +327,10 @@ public class ShardedTableMapFile {\n      *            name of the shard table--the table whose locations we are querying\n      * @return a map of split (endRow) to the location of those tablets in accumulo\n      */\n+\n     public static Map<Text,String> getLocations(Logger log, AccumuloHelper accumuloHelper, String shardedTableName) {\n         // split (endRow) -> String location mapping\n         Map<Text,String> splitToLocation = new TreeMap<>();\n-        \n         boolean keepRetrying = true;\n         int attempts = 0;\n         while (keepRetrying && attempts < MAX_RETRY_ATTEMPTS) {\n@@ -349,9 +344,7 @@ public class ShardedTableMapFile {\n                     Range range = new Range();\n                     Locations locations = tableOps.locate(shardedTableName, Collections.singletonList(range));\n                     List<TabletId> tabletIds = locations.groupByRange().get(range);\n-                    \n-                    tabletIds.stream().filter(tId -> tId.getEndRow() != null)\n-                                    .forEach(tId -> splitToLocation.put(tId.getEndRow(), locations.getTabletLocation(tId)));\n+                    tabletIds.stream().filter(tId -> tId.getEndRow() != null).forEach(tId -> splitToLocation.put(tId.getEndRow(), locations.getTabletLocation(tId)));\n                 }\n                 // made it here, no errors so break out\n                 keepRetrying = false;\n@@ -360,7 +353,6 @@ public class ShardedTableMapFile {\n                 UtilWaitThread.sleep(3000);\n             }\n         }\n-        \n         return splitToLocation;\n     }\n     \n@@ -377,16 +369,18 @@ public class ShardedTableMapFile {\n      * @throws IOException\n      *             if file system interaction fails\n      */\n+\n     public static long writeSplitsFile(Map<Text,String> splits, Path file, Configuration conf) throws IOException {\n         FileSystem fs = file.getFileSystem(conf);\n         if (fs.exists(file))\n             fs.delete(file, false);\n-        \n         long count = 0;\n         // reusable value for writing\n         Text value = new Text();\n-        SequenceFile.Writer writer = SequenceFile.createWriter(conf, SequenceFile.Writer.file(file), SequenceFile.Writer.keyClass(Text.class),\n-                        SequenceFile.Writer.valueClass(Text.class));\n+        SequenceFile.Writer writer = SequenceFile.createWriter(conf,\n+            SequenceFile.Writer.file(file),\n+                SequenceFile.Writer.keyClass(Text.class),\n+                    SequenceFile.Writer.valueClass(Text.class));\n         for (Entry<Text,String> entry : splits.entrySet()) {\n             count++;\n             value.set(entry.getValue());\n@@ -409,8 +403,9 @@ public class ShardedTableMapFile {\n      * @throws IOException\n      *             if file system interaction fails\n      */\n+\n     public static long writeSplitsFileLegacy(Map<Text,String> splits, Path file, Configuration conf) throws IOException {\n         return writeSplitsFile(splits, file, conf);\n     }\n-    \n-}\n+\n+}\n\\ No newline at end of file\n",
            "diff_size": 67
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "15",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "15",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        }
    ],
    "repaired_by": [],
    "not_repaired_by": [
        "styler",
        "intellij",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ]
}