{
    "project_name": "IQSS-dataverse",
    "error_id": "0",
    "information": {
        "errors": [
            {
                "line": "1563",
                "column": "1",
                "severity": "error",
                "message": "File contains tab characters (this is the first instance).",
                "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.FileTabCharacterCheck"
            }
        ]
    },
    "source_code": "@Deprecated\npublic Response getUploadUrl(@PathParam(\"id\") String idSupplied) {\n\ttry {\n\t\tDataset dataset = findDatasetOrDie(idSupplied);\n\n\t\tboolean canUpdateDataset = false;",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "1564",
                    "column": "1",
                    "severity": "error",
                    "message": "File contains tab characters (this is the first instance).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.FileTabCharacterCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/0/Datasets.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler/0/Datasets.java\nindex 226b4092078..8f9b71dc210 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/0/Datasets.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler/0/Datasets.java\n@@ -1560,7 +1560,7 @@ public class Datasets extends AbstractApiBean {\n @Path(\"{id}/uploadsid\")\n @Deprecated\n public Response getUploadUrl(@PathParam(\"id\") String idSupplied) {\n-\ttry {\n+try {\n \t\tDataset dataset = findDatasetOrDie(idSupplied);\n \n \t\tboolean canUpdateDataset = false;\n",
            "diff_size": 1
        },
        {
            "tool": "intellij",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/0/Datasets.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/intellij/0/Datasets.java\nindex 226b4092078..9cd294d76da 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/0/Datasets.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/intellij/0/Datasets.java\n@@ -165,2278 +165,2416 @@ import java.util.Map.Entry;\n @Path(\"datasets\")\n public class Datasets extends AbstractApiBean {\n \n-    private static final Logger logger = Logger.getLogger(Datasets.class.getCanonicalName());\n-    \n-    @Inject DataverseSession session;    \n-\n-    @EJB\n-    DatasetServiceBean datasetService;\n-\n-    @EJB\n-    DataverseServiceBean dataverseService;\n-    \n-    @EJB\n-    UserNotificationServiceBean userNotificationService;\n-    \n-    @EJB\n-    PermissionServiceBean permissionService;\n-    \n-    @EJB\n-    AuthenticationServiceBean authenticationServiceBean;\n-    \n-    @EJB\n-    DDIExportServiceBean ddiExportService;\n-    \n-    @EJB\n-    DatasetFieldServiceBean datasetfieldService;\n-\n-    @EJB\n-    MetadataBlockServiceBean metadataBlockService;\n-    \n-    @EJB\n-    DataFileServiceBean fileService;\n-\n-    @EJB\n-    IngestServiceBean ingestService;\n-\n-    @EJB\n-    EjbDataverseEngine commandEngine;\n-    \n-    @EJB\n-    IndexServiceBean indexService;\n-\n-    @EJB\n-    S3PackageImporter s3PackageImporter;\n-     \n-    @EJB\n-    SettingsServiceBean settingsService;\n-\n-    // TODO: Move to AbstractApiBean\n-    @EJB\n-    DatasetMetricsServiceBean datasetMetricsSvc;\n-    \n-    @EJB\n-    DatasetExternalCitationsServiceBean datasetExternalCitationsService;\n-    \n-    @Inject\n-    MakeDataCountLoggingServiceBean mdcLogService;\n-    \n-    @Inject\n-    DataverseRequestServiceBean dvRequestService;\n-\n-    /**\n-     * Used to consolidate the way we parse and handle dataset versions.\n-     * @param <T> \n-     */\n-    public interface DsVersionHandler<T> {\n-        T handleLatest();\n-        T handleDraft();\n-        T handleSpecific( long major, long minor );\n-        T handleLatestPublished();\n-    }\n-    \n-    @GET\n-    @Path(\"{id}\")\n-    public Response getDataset(@PathParam(\"id\") String id, @Context UriInfo uriInfo, @Context HttpHeaders headers, @Context HttpServletResponse response) {\n-        return response( req -> {\n-            final Dataset retrieved = execCommand(new GetDatasetCommand(req, findDatasetOrDie(id)));\n-            final DatasetVersion latest = execCommand(new GetLatestAccessibleDatasetVersionCommand(req, retrieved));\n-            final JsonObjectBuilder jsonbuilder = json(retrieved);\n-            //Report MDC if this is a released version (could be draft if user has access, or user may not have access at all and is not getting metadata beyond the minimum)\n-            if((latest != null) && latest.isReleased()) {\n-                MakeDataCountLoggingServiceBean.MakeDataCountEntry entry = new MakeDataCountEntry(uriInfo, headers, dvRequestService, retrieved);\n-                mdcLogService.logEntry(entry);\n-            }\n-            return ok(jsonbuilder.add(\"latestVersion\", (latest != null) ? json(latest) : null));\n-        });\n-    }\n-    \n-    // TODO: \n-    // This API call should, ideally, call findUserOrDie() and the GetDatasetCommand \n-    // to obtain the dataset that we are trying to export - which would handle\n-    // Auth in the process... For now, Auth isn't necessary - since export ONLY \n-    // WORKS on published datasets, which are open to the world. -- L.A. 4.5\n-    \n-    @GET\n-    @Path(\"/export\")\n-    @Produces({\"application/xml\", \"application/json\", \"application/html\" })\n-    public Response exportDataset(@QueryParam(\"persistentId\") String persistentId, @QueryParam(\"exporter\") String exporter, @Context UriInfo uriInfo, @Context HttpHeaders headers, @Context HttpServletResponse response) {\n+  private static final Logger logger = Logger.getLogger(Datasets.class.getCanonicalName());\n \n-        try {\n-            Dataset dataset = datasetService.findByGlobalId(persistentId);\n-            if (dataset == null) {\n-                return error(Response.Status.NOT_FOUND, \"A dataset with the persistentId \" + persistentId + \" could not be found.\");\n-            }\n-            \n-            ExportService instance = ExportService.getInstance();\n-            \n-            InputStream is = instance.getExport(dataset, exporter);\n-           \n-            String mediaType = instance.getMediaType(exporter);\n-            //Export is only possible for released (non-draft) dataset versions so we can log without checking to see if this is a request for a draft \n-            MakeDataCountLoggingServiceBean.MakeDataCountEntry entry = new MakeDataCountEntry(uriInfo, headers, dvRequestService, dataset);\n-            mdcLogService.logEntry(entry);\n-            \n-            return Response.ok()\n-                    .entity(is)\n-                    .type(mediaType).\n-                    build();\n-        } catch (Exception wr) {\n-            return error(Response.Status.FORBIDDEN, \"Export Failed\");\n-        }\n-    }\n+  @Inject\n+  DataverseSession session;\n \n-    @DELETE\n-    @Path(\"{id}\")\n-    public Response deleteDataset( @PathParam(\"id\") String id) {\n-        // Internally, \"DeleteDatasetCommand\" simply redirects to \"DeleteDatasetVersionCommand\"\n-        // (and there's a comment that says \"TODO: remove this command\")\n-        // do we need an exposed API call for it? \n-        // And DeleteDatasetVersionCommand further redirects to DestroyDatasetCommand, \n-        // if the dataset only has 1 version... In other words, the functionality \n-        // currently provided by this API is covered between the \"deleteDraftVersion\" and\n-        // \"destroyDataset\" API calls.  \n-        // (The logic below follows the current implementation of the underlying \n-        // commands!)\n-        \n-        return response( req -> {\n-            Dataset doomed = findDatasetOrDie(id);\n-            DatasetVersion doomedVersion = doomed.getLatestVersion();\n-            User u = findUserOrDie();\n-            boolean destroy = false;\n-            \n-            if (doomed.getVersions().size() == 1) {\n-                if (doomed.isReleased() && (!(u instanceof AuthenticatedUser) || !u.isSuperuser())) {\n-                    throw new WrappedResponse(error(Response.Status.UNAUTHORIZED, \"Only superusers can delete published datasets\"));\n-                }\n-                destroy = true;\n-            } else {\n-                if (!doomedVersion.isDraft()) {\n-                    throw new WrappedResponse(error(Response.Status.UNAUTHORIZED, \"This is a published dataset with multiple versions. This API can only delete the latest version if it is a DRAFT\"));\n-                }\n-            }\n-            \n-            // Gather the locations of the physical files that will need to be \n-            // deleted once the destroy command execution has been finalized:\n-            Map<Long, String> deleteStorageLocations = fileService.getPhysicalFilesToDelete(doomedVersion, destroy);\n-            \n-            execCommand( new DeleteDatasetCommand(req, findDatasetOrDie(id)));\n-            \n-            // If we have gotten this far, the destroy command has succeeded, \n-            // so we can finalize it by permanently deleting the physical files:\n-            // (DataFileService will double-check that the datafiles no \n-            // longer exist in the database, before attempting to delete \n-            // the physical files)\n-            if (!deleteStorageLocations.isEmpty()) {\n-                fileService.finalizeFileDeletes(deleteStorageLocations);\n-            }\n-            \n-            return ok(\"Dataset \" + id + \" deleted\");\n-        });\n-    }\n-        \n-    @DELETE\n-    @Path(\"{id}/destroy\")\n-    public Response destroyDataset(@PathParam(\"id\") String id) {\n-\n-        return response(req -> {\n-            // first check if dataset is released, and if so, if user is a superuser\n-            Dataset doomed = findDatasetOrDie(id);\n-            User u = findUserOrDie();\n-\n-            if (doomed.isReleased() && (!(u instanceof AuthenticatedUser) || !u.isSuperuser())) {\n-                throw new WrappedResponse(error(Response.Status.UNAUTHORIZED, \"Destroy can only be called by superusers.\"));\n-            }\n+  @EJB\n+  DatasetServiceBean datasetService;\n \n-            // Gather the locations of the physical files that will need to be \n-            // deleted once the destroy command execution has been finalized:\n-            Map<Long, String> deleteStorageLocations = fileService.getPhysicalFilesToDelete(doomed);\n+  @EJB\n+  DataverseServiceBean dataverseService;\n \n-            execCommand(new DestroyDatasetCommand(doomed, req));\n+  @EJB\n+  UserNotificationServiceBean userNotificationService;\n \n-            // If we have gotten this far, the destroy command has succeeded, \n-            // so we can finalize permanently deleting the physical files:\n-            // (DataFileService will double-check that the datafiles no \n-            // longer exist in the database, before attempting to delete \n-            // the physical files)\n-            if (!deleteStorageLocations.isEmpty()) {\n-                fileService.finalizeFileDeletes(deleteStorageLocations);\n-            }\n+  @EJB\n+  PermissionServiceBean permissionService;\n \n-            return ok(\"Dataset \" + id + \" destroyed\");\n-        });\n-    }\n-    \n-    @DELETE\n-    @Path(\"{id}/versions/{versionId}\")\n-    public Response deleteDraftVersion( @PathParam(\"id\") String id,  @PathParam(\"versionId\") String versionId ){\n-        if ( ! \":draft\".equals(versionId) ) {\n-            return badRequest(\"Only the :draft version can be deleted\");\n-        }\n+  @EJB\n+  AuthenticationServiceBean authenticationServiceBean;\n \n-        return response( req -> {\n-            Dataset dataset = findDatasetOrDie(id);\n-            DatasetVersion doomed = dataset.getLatestVersion();\n-            \n-            if (!doomed.isDraft()) {\n-                throw new WrappedResponse(error(Response.Status.UNAUTHORIZED, \"This is NOT a DRAFT version\"));\n-            }\n-            \n-            // Gather the locations of the physical files that will need to be \n-            // deleted once the destroy command execution has been finalized:\n-            \n-            Map<Long, String> deleteStorageLocations = fileService.getPhysicalFilesToDelete(doomed);\n-            \n-            execCommand( new DeleteDatasetVersionCommand(req, dataset));\n-            \n-            // If we have gotten this far, the delete command has succeeded - \n-            // by either deleting the Draft version of a published dataset, \n-            // or destroying an unpublished one. \n-            // This means we can finalize permanently deleting the physical files:\n-            // (DataFileService will double-check that the datafiles no \n-            // longer exist in the database, before attempting to delete \n-            // the physical files)\n-            if (!deleteStorageLocations.isEmpty()) {\n-                fileService.finalizeFileDeletes(deleteStorageLocations);\n-            }\n-            \n-            return ok(\"Draft version of dataset \" + id + \" deleted\");\n-        });\n-    }\n-        \n-    @DELETE\n-    @Path(\"{datasetId}/deleteLink/{linkedDataverseId}\")\n-    public Response deleteDatasetLinkingDataverse( @PathParam(\"datasetId\") String datasetId, @PathParam(\"linkedDataverseId\") String linkedDataverseId) {\n-                boolean index = true;\n-        return response(req -> {\n-            execCommand(new DeleteDatasetLinkingDataverseCommand(req, findDatasetOrDie(datasetId), findDatasetLinkingDataverseOrDie(datasetId, linkedDataverseId), index));\n-            return ok(\"Link from Dataset \" + datasetId + \" to linked Dataverse \" + linkedDataverseId + \" deleted\");\n-        });\n-    }\n-        \n-    @PUT\n-    @Path(\"{id}/citationdate\")\n-    public Response setCitationDate( @PathParam(\"id\") String id, String dsfTypeName) {\n-        return response( req -> {\n-            if ( dsfTypeName.trim().isEmpty() ){\n-                return badRequest(\"Please provide a dataset field type in the requst body.\");\n-            }\n-            DatasetFieldType dsfType = null;\n-            if (!\":publicationDate\".equals(dsfTypeName)) {\n-                dsfType = datasetFieldSvc.findByName(dsfTypeName);\n-                if (dsfType == null) {\n-                    return badRequest(\"Dataset Field Type Name \" + dsfTypeName + \" not found.\");\n-                }\n-            }\n+  @EJB\n+  DDIExportServiceBean ddiExportService;\n \n-            execCommand(new SetDatasetCitationDateCommand(req, findDatasetOrDie(id), dsfType));\n-            return ok(\"Citation Date for dataset \" + id + \" set to: \" + (dsfType != null ? dsfType.getDisplayName() : \"default\"));\n-        });\n-    }    \n-    \n-    @DELETE\n-    @Path(\"{id}/citationdate\")\n-    public Response useDefaultCitationDate( @PathParam(\"id\") String id) {\n-        return response( req -> {\n-            execCommand(new SetDatasetCitationDateCommand(req, findDatasetOrDie(id), null));\n-            return ok(\"Citation Date for dataset \" + id + \" set to default\");\n-        });\n-    }         \n-    \n-    @GET\n-    @Path(\"{id}/versions\")\n-    public Response listVersions( @PathParam(\"id\") String id ) {\n-        return response( req ->\n-             ok( execCommand( new ListVersionsCommand(req, findDatasetOrDie(id)) )\n-                                .stream()\n-                                .map( d -> json(d) )\n-                                .collect(toJsonArray())));\n-    }\n-    \n-    @GET\n-    @Path(\"{id}/versions/{versionId}\")\n-    public Response getVersion( @PathParam(\"id\") String datasetId, @PathParam(\"versionId\") String versionId, @Context UriInfo uriInfo, @Context HttpHeaders headers) {\n-        return response( req -> {\n-            DatasetVersion dsv = getDatasetVersionOrDie(req, versionId, findDatasetOrDie(datasetId), uriInfo, headers);            \n-            return (dsv == null || dsv.getId() == null) ? notFound(\"Dataset version not found\")\n-                                                        : ok(json(dsv));\n-        });\n-    }\n-    \n-    @GET\n-    @Path(\"{id}/versions/{versionId}/files\")\n-    public Response getVersionFiles( @PathParam(\"id\") String datasetId, @PathParam(\"versionId\") String versionId, @Context UriInfo uriInfo, @Context HttpHeaders headers) {\n-        return response( req -> ok( jsonFileMetadatas(\n-                         getDatasetVersionOrDie(req, versionId, findDatasetOrDie(datasetId), uriInfo, headers).getFileMetadatas())));\n+  @EJB\n+  DatasetFieldServiceBean datasetfieldService;\n+\n+  @EJB\n+  MetadataBlockServiceBean metadataBlockService;\n+\n+  @EJB\n+  DataFileServiceBean fileService;\n+\n+  @EJB\n+  IngestServiceBean ingestService;\n+\n+  @EJB\n+  EjbDataverseEngine commandEngine;\n+\n+  @EJB\n+  IndexServiceBean indexService;\n+\n+  @EJB\n+  S3PackageImporter s3PackageImporter;\n+\n+  @EJB\n+  SettingsServiceBean settingsService;\n+\n+  // TODO: Move to AbstractApiBean\n+  @EJB\n+  DatasetMetricsServiceBean datasetMetricsSvc;\n+\n+  @EJB\n+  DatasetExternalCitationsServiceBean datasetExternalCitationsService;\n+\n+  @Inject\n+  MakeDataCountLoggingServiceBean mdcLogService;\n+\n+  @Inject\n+  DataverseRequestServiceBean dvRequestService;\n+\n+  /**\n+   * Used to consolidate the way we parse and handle dataset versions.\n+   *\n+   * @param <T>\n+   */\n+  public interface DsVersionHandler<T> {\n+    T handleLatest();\n+\n+    T handleDraft();\n+\n+    T handleSpecific(long major, long minor);\n+\n+    T handleLatestPublished();\n+  }\n+\n+  @GET\n+  @Path(\"{id}\")\n+  public Response getDataset(@PathParam(\"id\") String id, @Context UriInfo uriInfo, @Context HttpHeaders headers,\n+                             @Context HttpServletResponse response) {\n+    return response(req -> {\n+      final Dataset retrieved = execCommand(new GetDatasetCommand(req, findDatasetOrDie(id)));\n+      final DatasetVersion latest = execCommand(new GetLatestAccessibleDatasetVersionCommand(req, retrieved));\n+      final JsonObjectBuilder jsonbuilder = json(retrieved);\n+      //Report MDC if this is a released version (could be draft if user has access, or user may not have access at all and is not getting metadata beyond the minimum)\n+      if ((latest != null) && latest.isReleased()) {\n+        MakeDataCountLoggingServiceBean.MakeDataCountEntry entry =\n+          new MakeDataCountEntry(uriInfo, headers, dvRequestService, retrieved);\n+        mdcLogService.logEntry(entry);\n+      }\n+      return ok(jsonbuilder.add(\"latestVersion\", (latest != null) ? json(latest) : null));\n+    });\n+  }\n+\n+  // TODO:\n+  // This API call should, ideally, call findUserOrDie() and the GetDatasetCommand\n+  // to obtain the dataset that we are trying to export - which would handle\n+  // Auth in the process... For now, Auth isn't necessary - since export ONLY\n+  // WORKS on published datasets, which are open to the world. -- L.A. 4.5\n+\n+  @GET\n+  @Path(\"/export\")\n+  @Produces({\"application/xml\", \"application/json\", \"application/html\"})\n+  public Response exportDataset(@QueryParam(\"persistentId\") String persistentId,\n+                                @QueryParam(\"exporter\") String exporter, @Context UriInfo uriInfo,\n+                                @Context HttpHeaders headers, @Context HttpServletResponse response) {\n+\n+    try {\n+      Dataset dataset = datasetService.findByGlobalId(persistentId);\n+      if (dataset == null) {\n+        return error(Response.Status.NOT_FOUND,\n+          \"A dataset with the persistentId \" + persistentId + \" could not be found.\");\n+      }\n+\n+      ExportService instance = ExportService.getInstance();\n+\n+      InputStream is = instance.getExport(dataset, exporter);\n+\n+      String mediaType = instance.getMediaType(exporter);\n+      //Export is only possible for released (non-draft) dataset versions so we can log without checking to see if this is a request for a draft\n+      MakeDataCountLoggingServiceBean.MakeDataCountEntry entry =\n+        new MakeDataCountEntry(uriInfo, headers, dvRequestService, dataset);\n+      mdcLogService.logEntry(entry);\n+\n+      return Response.ok()\n+        .entity(is)\n+        .type(mediaType).\n+          build();\n+    } catch (Exception wr) {\n+      return error(Response.Status.FORBIDDEN, \"Export Failed\");\n     }\n-    \n-    @GET\n-    @Path(\"{id}/dirindex\")\n-    @Produces(\"text/html\")\n-    public Response getFileAccessFolderView(@PathParam(\"id\") String datasetId, @QueryParam(\"version\") String versionId, @QueryParam(\"folder\") String folderName, @QueryParam(\"original\") Boolean originals, @Context UriInfo uriInfo, @Context HttpHeaders headers, @Context HttpServletResponse response) {\n-\n-        folderName = folderName == null ? \"\" : folderName;\n-        versionId = versionId == null ? \":latest-published\" : versionId; \n-        \n-        DatasetVersion version; \n-        try {\n-            DataverseRequest req = createDataverseRequest(findUserOrDie());\n-            version = getDatasetVersionOrDie(req, versionId, findDatasetOrDie(datasetId), uriInfo, headers);\n-        } catch (WrappedResponse wr) {\n-            return wr.getResponse();\n+  }\n+\n+  @DELETE\n+  @Path(\"{id}\")\n+  public Response deleteDataset(@PathParam(\"id\") String id) {\n+    // Internally, \"DeleteDatasetCommand\" simply redirects to \"DeleteDatasetVersionCommand\"\n+    // (and there's a comment that says \"TODO: remove this command\")\n+    // do we need an exposed API call for it?\n+    // And DeleteDatasetVersionCommand further redirects to DestroyDatasetCommand,\n+    // if the dataset only has 1 version... In other words, the functionality\n+    // currently provided by this API is covered between the \"deleteDraftVersion\" and\n+    // \"destroyDataset\" API calls.\n+    // (The logic below follows the current implementation of the underlying\n+    // commands!)\n+\n+    return response(req -> {\n+      Dataset doomed = findDatasetOrDie(id);\n+      DatasetVersion doomedVersion = doomed.getLatestVersion();\n+      User u = findUserOrDie();\n+      boolean destroy = false;\n+\n+      if (doomed.getVersions().size() == 1) {\n+        if (doomed.isReleased() && (!(u instanceof AuthenticatedUser) || !u.isSuperuser())) {\n+          throw new WrappedResponse(\n+            error(Response.Status.UNAUTHORIZED, \"Only superusers can delete published datasets\"));\n         }\n-        \n-        String output = FileUtil.formatFolderListingHtml(folderName, version, \"\", originals != null && originals);\n-        \n-        // return \"NOT FOUND\" if there is no such folder in the dataset version:\n-        \n-        if (\"\".equals(output)) {\n-            return notFound(\"Folder \" + folderName + \" does not exist\");\n+        destroy = true;\n+      } else {\n+        if (!doomedVersion.isDraft()) {\n+          throw new WrappedResponse(error(Response.Status.UNAUTHORIZED,\n+            \"This is a published dataset with multiple versions. This API can only delete the latest version if it is a DRAFT\"));\n         }\n-        \n-        \n-        String indexFileName = folderName.equals(\"\") ? \".index.html\"\n-                : \".index-\" + folderName.replace('/', '_') + \".html\";\n-        response.setHeader(\"Content-disposition\", \"filename=\\\"\" + indexFileName + \"\\\"\");\n-\n-        \n-        return Response.ok()\n-                .entity(output)\n-                //.type(\"application/html\").\n-                .build();\n-    }\n-    \n-    @GET\n-    @Path(\"{id}/versions/{versionId}/metadata\")\n-    public Response getVersionMetadata( @PathParam(\"id\") String datasetId, @PathParam(\"versionId\") String versionId, @Context UriInfo uriInfo, @Context HttpHeaders headers) {\n-        return response( req -> ok(\n-                    jsonByBlocks(\n-                        getDatasetVersionOrDie(req, versionId, findDatasetOrDie(datasetId), uriInfo, headers )\n-                                .getDatasetFields())));\n+      }\n+\n+      // Gather the locations of the physical files that will need to be\n+      // deleted once the destroy command execution has been finalized:\n+      Map<Long, String> deleteStorageLocations = fileService.getPhysicalFilesToDelete(doomedVersion, destroy);\n+\n+      execCommand(new DeleteDatasetCommand(req, findDatasetOrDie(id)));\n+\n+      // If we have gotten this far, the destroy command has succeeded,\n+      // so we can finalize it by permanently deleting the physical files:\n+      // (DataFileService will double-check that the datafiles no\n+      // longer exist in the database, before attempting to delete\n+      // the physical files)\n+      if (!deleteStorageLocations.isEmpty()) {\n+        fileService.finalizeFileDeletes(deleteStorageLocations);\n+      }\n+\n+      return ok(\"Dataset \" + id + \" deleted\");\n+    });\n+  }\n+\n+  @DELETE\n+  @Path(\"{id}/destroy\")\n+  public Response destroyDataset(@PathParam(\"id\") String id) {\n+\n+    return response(req -> {\n+      // first check if dataset is released, and if so, if user is a superuser\n+      Dataset doomed = findDatasetOrDie(id);\n+      User u = findUserOrDie();\n+\n+      if (doomed.isReleased() && (!(u instanceof AuthenticatedUser) || !u.isSuperuser())) {\n+        throw new WrappedResponse(error(Response.Status.UNAUTHORIZED, \"Destroy can only be called by superusers.\"));\n+      }\n+\n+      // Gather the locations of the physical files that will need to be\n+      // deleted once the destroy command execution has been finalized:\n+      Map<Long, String> deleteStorageLocations = fileService.getPhysicalFilesToDelete(doomed);\n+\n+      execCommand(new DestroyDatasetCommand(doomed, req));\n+\n+      // If we have gotten this far, the destroy command has succeeded,\n+      // so we can finalize permanently deleting the physical files:\n+      // (DataFileService will double-check that the datafiles no\n+      // longer exist in the database, before attempting to delete\n+      // the physical files)\n+      if (!deleteStorageLocations.isEmpty()) {\n+        fileService.finalizeFileDeletes(deleteStorageLocations);\n+      }\n+\n+      return ok(\"Dataset \" + id + \" destroyed\");\n+    });\n+  }\n+\n+  @DELETE\n+  @Path(\"{id}/versions/{versionId}\")\n+  public Response deleteDraftVersion(@PathParam(\"id\") String id, @PathParam(\"versionId\") String versionId) {\n+    if (!\":draft\".equals(versionId)) {\n+      return badRequest(\"Only the :draft version can be deleted\");\n     }\n-    \n-    @GET\n-    @Path(\"{id}/versions/{versionNumber}/metadata/{block}\")\n-    public Response getVersionMetadataBlock( @PathParam(\"id\") String datasetId, \n-                                             @PathParam(\"versionNumber\") String versionNumber, \n-                                             @PathParam(\"block\") String blockName, \n-                                             @Context UriInfo uriInfo, \n-                                             @Context HttpHeaders headers ) {\n-        \n-        return response( req -> {\n-            DatasetVersion dsv = getDatasetVersionOrDie(req, versionNumber, findDatasetOrDie(datasetId), uriInfo, headers );\n-            \n-            Map<MetadataBlock, List<DatasetField>> fieldsByBlock = DatasetField.groupByBlock(dsv.getDatasetFields());\n-            for ( Map.Entry<MetadataBlock, List<DatasetField>> p : fieldsByBlock.entrySet() ) {\n-                if ( p.getKey().getName().equals(blockName) ) {\n-                    return ok(json(p.getKey(), p.getValue()));\n-                }\n-            }\n-            return notFound(\"metadata block named \" + blockName + \" not found\");\n-        });\n-    }\n-    \n-    @GET\n-    @Path(\"{id}/modifyRegistration\")\n-    public Response updateDatasetTargetURL(@PathParam(\"id\") String id ) {\n-        return response( req -> {\n-            execCommand(new UpdateDatasetTargetURLCommand(findDatasetOrDie(id), req));\n-            return ok(\"Dataset \" + id + \" target url updated\");\n-        });\n+\n+    return response(req -> {\n+      Dataset dataset = findDatasetOrDie(id);\n+      DatasetVersion doomed = dataset.getLatestVersion();\n+\n+      if (!doomed.isDraft()) {\n+        throw new WrappedResponse(error(Response.Status.UNAUTHORIZED, \"This is NOT a DRAFT version\"));\n+      }\n+\n+      // Gather the locations of the physical files that will need to be\n+      // deleted once the destroy command execution has been finalized:\n+\n+      Map<Long, String> deleteStorageLocations = fileService.getPhysicalFilesToDelete(doomed);\n+\n+      execCommand(new DeleteDatasetVersionCommand(req, dataset));\n+\n+      // If we have gotten this far, the delete command has succeeded -\n+      // by either deleting the Draft version of a published dataset,\n+      // or destroying an unpublished one.\n+      // This means we can finalize permanently deleting the physical files:\n+      // (DataFileService will double-check that the datafiles no\n+      // longer exist in the database, before attempting to delete\n+      // the physical files)\n+      if (!deleteStorageLocations.isEmpty()) {\n+        fileService.finalizeFileDeletes(deleteStorageLocations);\n+      }\n+\n+      return ok(\"Draft version of dataset \" + id + \" deleted\");\n+    });\n+  }\n+\n+  @DELETE\n+  @Path(\"{datasetId}/deleteLink/{linkedDataverseId}\")\n+  public Response deleteDatasetLinkingDataverse(@PathParam(\"datasetId\") String datasetId,\n+                                                @PathParam(\"linkedDataverseId\") String linkedDataverseId) {\n+    boolean index = true;\n+    return response(req -> {\n+      execCommand(new DeleteDatasetLinkingDataverseCommand(req, findDatasetOrDie(datasetId),\n+        findDatasetLinkingDataverseOrDie(datasetId, linkedDataverseId), index));\n+      return ok(\"Link from Dataset \" + datasetId + \" to linked Dataverse \" + linkedDataverseId + \" deleted\");\n+    });\n+  }\n+\n+  @PUT\n+  @Path(\"{id}/citationdate\")\n+  public Response setCitationDate(@PathParam(\"id\") String id, String dsfTypeName) {\n+    return response(req -> {\n+      if (dsfTypeName.trim().isEmpty()) {\n+        return badRequest(\"Please provide a dataset field type in the requst body.\");\n+      }\n+      DatasetFieldType dsfType = null;\n+      if (!\":publicationDate\".equals(dsfTypeName)) {\n+        dsfType = datasetFieldSvc.findByName(dsfTypeName);\n+        if (dsfType == null) {\n+          return badRequest(\"Dataset Field Type Name \" + dsfTypeName + \" not found.\");\n+        }\n+      }\n+\n+      execCommand(new SetDatasetCitationDateCommand(req, findDatasetOrDie(id), dsfType));\n+      return ok(\n+        \"Citation Date for dataset \" + id + \" set to: \" + (dsfType != null ? dsfType.getDisplayName() : \"default\"));\n+    });\n+  }\n+\n+  @DELETE\n+  @Path(\"{id}/citationdate\")\n+  public Response useDefaultCitationDate(@PathParam(\"id\") String id) {\n+    return response(req -> {\n+      execCommand(new SetDatasetCitationDateCommand(req, findDatasetOrDie(id), null));\n+      return ok(\"Citation Date for dataset \" + id + \" set to default\");\n+    });\n+  }\n+\n+  @GET\n+  @Path(\"{id}/versions\")\n+  public Response listVersions(@PathParam(\"id\") String id) {\n+    return response(req ->\n+      ok(execCommand(new ListVersionsCommand(req, findDatasetOrDie(id)))\n+        .stream()\n+        .map(d -> json(d))\n+        .collect(toJsonArray())));\n+  }\n+\n+  @GET\n+  @Path(\"{id}/versions/{versionId}\")\n+  public Response getVersion(@PathParam(\"id\") String datasetId, @PathParam(\"versionId\") String versionId,\n+                             @Context UriInfo uriInfo, @Context HttpHeaders headers) {\n+    return response(req -> {\n+      DatasetVersion dsv = getDatasetVersionOrDie(req, versionId, findDatasetOrDie(datasetId), uriInfo, headers);\n+      return (dsv == null || dsv.getId() == null) ? notFound(\"Dataset version not found\")\n+        : ok(json(dsv));\n+    });\n+  }\n+\n+  @GET\n+  @Path(\"{id}/versions/{versionId}/files\")\n+  public Response getVersionFiles(@PathParam(\"id\") String datasetId, @PathParam(\"versionId\") String versionId,\n+                                  @Context UriInfo uriInfo, @Context HttpHeaders headers) {\n+    return response(req -> ok(jsonFileMetadatas(\n+      getDatasetVersionOrDie(req, versionId, findDatasetOrDie(datasetId), uriInfo, headers).getFileMetadatas())));\n+  }\n+\n+  @GET\n+  @Path(\"{id}/dirindex\")\n+  @Produces(\"text/html\")\n+  public Response getFileAccessFolderView(@PathParam(\"id\") String datasetId, @QueryParam(\"version\") String versionId,\n+                                          @QueryParam(\"folder\") String folderName,\n+                                          @QueryParam(\"original\") Boolean originals, @Context UriInfo uriInfo,\n+                                          @Context HttpHeaders headers, @Context HttpServletResponse response) {\n+\n+    folderName = folderName == null ? \"\" : folderName;\n+    versionId = versionId == null ? \":latest-published\" : versionId;\n+\n+    DatasetVersion version;\n+    try {\n+      DataverseRequest req = createDataverseRequest(findUserOrDie());\n+      version = getDatasetVersionOrDie(req, versionId, findDatasetOrDie(datasetId), uriInfo, headers);\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n     }\n-    \n-    @POST\n-    @Path(\"/modifyRegistrationAll\")\n-    public Response updateDatasetTargetURLAll() {\n-        return response( req -> {\n-            datasetService.findAll().forEach( ds -> {\n-                try {\n-                    execCommand(new UpdateDatasetTargetURLCommand(findDatasetOrDie(ds.getId().toString()), req));\n-                } catch (WrappedResponse ex) {\n-                    Logger.getLogger(Datasets.class.getName()).log(Level.SEVERE, null, ex);\n-                }\n-            });\n-            return ok(\"Update All Dataset target url completed\");\n-        });\n+\n+    String output = FileUtil.formatFolderListingHtml(folderName, version, \"\", originals != null && originals);\n+\n+    // return \"NOT FOUND\" if there is no such folder in the dataset version:\n+\n+    if (\"\".equals(output)) {\n+      return notFound(\"Folder \" + folderName + \" does not exist\");\n     }\n-    \n-    @POST\n-    @Path(\"{id}/modifyRegistrationMetadata\")\n-    public Response updateDatasetPIDMetadata(@PathParam(\"id\") String id) {\n \n+\n+    String indexFileName = folderName.equals(\"\") ? \".index.html\"\n+      : \".index-\" + folderName.replace('/', '_') + \".html\";\n+    response.setHeader(\"Content-disposition\", \"filename=\\\"\" + indexFileName + \"\\\"\");\n+\n+\n+    return Response.ok()\n+      .entity(output)\n+      //.type(\"application/html\").\n+      .build();\n+  }\n+\n+  @GET\n+  @Path(\"{id}/versions/{versionId}/metadata\")\n+  public Response getVersionMetadata(@PathParam(\"id\") String datasetId, @PathParam(\"versionId\") String versionId,\n+                                     @Context UriInfo uriInfo, @Context HttpHeaders headers) {\n+    return response(req -> ok(\n+      jsonByBlocks(\n+        getDatasetVersionOrDie(req, versionId, findDatasetOrDie(datasetId), uriInfo, headers)\n+          .getDatasetFields())));\n+  }\n+\n+  @GET\n+  @Path(\"{id}/versions/{versionNumber}/metadata/{block}\")\n+  public Response getVersionMetadataBlock(@PathParam(\"id\") String datasetId,\n+                                          @PathParam(\"versionNumber\") String versionNumber,\n+                                          @PathParam(\"block\") String blockName,\n+                                          @Context UriInfo uriInfo,\n+                                          @Context HttpHeaders headers) {\n+\n+    return response(req -> {\n+      DatasetVersion dsv = getDatasetVersionOrDie(req, versionNumber, findDatasetOrDie(datasetId), uriInfo, headers);\n+\n+      Map<MetadataBlock, List<DatasetField>> fieldsByBlock = DatasetField.groupByBlock(dsv.getDatasetFields());\n+      for (Map.Entry<MetadataBlock, List<DatasetField>> p : fieldsByBlock.entrySet()) {\n+        if (p.getKey().getName().equals(blockName)) {\n+          return ok(json(p.getKey(), p.getValue()));\n+        }\n+      }\n+      return notFound(\"metadata block named \" + blockName + \" not found\");\n+    });\n+  }\n+\n+  @GET\n+  @Path(\"{id}/modifyRegistration\")\n+  public Response updateDatasetTargetURL(@PathParam(\"id\") String id) {\n+    return response(req -> {\n+      execCommand(new UpdateDatasetTargetURLCommand(findDatasetOrDie(id), req));\n+      return ok(\"Dataset \" + id + \" target url updated\");\n+    });\n+  }\n+\n+  @POST\n+  @Path(\"/modifyRegistrationAll\")\n+  public Response updateDatasetTargetURLAll() {\n+    return response(req -> {\n+      datasetService.findAll().forEach(ds -> {\n         try {\n-            Dataset dataset = findDatasetOrDie(id);\n-            if (!dataset.isReleased()) {\n-                return error(Response.Status.BAD_REQUEST, BundleUtil.getStringFromBundle(\"datasets.api.updatePIDMetadata.failure.dataset.must.be.released\"));\n-            }\n+          execCommand(new UpdateDatasetTargetURLCommand(findDatasetOrDie(ds.getId().toString()), req));\n         } catch (WrappedResponse ex) {\n-            Logger.getLogger(Datasets.class.getName()).log(Level.SEVERE, null, ex);\n+          Logger.getLogger(Datasets.class.getName()).log(Level.SEVERE, null, ex);\n         }\n-\n-        return response(req -> {\n-            execCommand(new UpdateDvObjectPIDMetadataCommand(findDatasetOrDie(id), req));\n-            List<String> args = Arrays.asList(id);\n-            return ok(BundleUtil.getStringFromBundle(\"datasets.api.updatePIDMetadata.success.for.single.dataset\", args));\n-        });\n-    }\n-    \n-    @GET\n-    @Path(\"/modifyRegistrationPIDMetadataAll\")\n-    public Response updateDatasetPIDMetadataAll() {\n-        return response( req -> {\n-            datasetService.findAll().forEach( ds -> {\n-                try {\n-                    execCommand(new UpdateDvObjectPIDMetadataCommand(findDatasetOrDie(ds.getId().toString()), req));\n-                } catch (WrappedResponse ex) {\n-                    Logger.getLogger(Datasets.class.getName()).log(Level.SEVERE, null, ex);\n-                }\n-            });           \n-            return ok(BundleUtil.getStringFromBundle(\"datasets.api.updatePIDMetadata.success.for.update.all\"));\n-        });\n+      });\n+      return ok(\"Update All Dataset target url completed\");\n+    });\n+  }\n+\n+  @POST\n+  @Path(\"{id}/modifyRegistrationMetadata\")\n+  public Response updateDatasetPIDMetadata(@PathParam(\"id\") String id) {\n+\n+    try {\n+      Dataset dataset = findDatasetOrDie(id);\n+      if (!dataset.isReleased()) {\n+        return error(Response.Status.BAD_REQUEST,\n+          BundleUtil.getStringFromBundle(\"datasets.api.updatePIDMetadata.failure.dataset.must.be.released\"));\n+      }\n+    } catch (WrappedResponse ex) {\n+      Logger.getLogger(Datasets.class.getName()).log(Level.SEVERE, null, ex);\n     }\n-  \n-    @PUT\n-    @Path(\"{id}/versions/{versionId}\")\n-    public Response updateDraftVersion( String jsonBody, @PathParam(\"id\") String id,  @PathParam(\"versionId\") String versionId ){\n-        \n-        if ( ! \":draft\".equals(versionId) ) {\n-            return error( Response.Status.BAD_REQUEST, \"Only the :draft version can be updated\");\n+\n+    return response(req -> {\n+      execCommand(new UpdateDvObjectPIDMetadataCommand(findDatasetOrDie(id), req));\n+      List<String> args = Arrays.asList(id);\n+      return ok(BundleUtil.getStringFromBundle(\"datasets.api.updatePIDMetadata.success.for.single.dataset\", args));\n+    });\n+  }\n+\n+  @GET\n+  @Path(\"/modifyRegistrationPIDMetadataAll\")\n+  public Response updateDatasetPIDMetadataAll() {\n+    return response(req -> {\n+      datasetService.findAll().forEach(ds -> {\n+        try {\n+          execCommand(new UpdateDvObjectPIDMetadataCommand(findDatasetOrDie(ds.getId().toString()), req));\n+        } catch (WrappedResponse ex) {\n+          Logger.getLogger(Datasets.class.getName()).log(Level.SEVERE, null, ex);\n         }\n-        \n-        try ( StringReader rdr = new StringReader(jsonBody) ) {\n-            DataverseRequest req = createDataverseRequest(findUserOrDie());\n-            Dataset ds = findDatasetOrDie(id);\n-            JsonObject json = Json.createReader(rdr).readObject();\n-            DatasetVersion incomingVersion = jsonParser().parseDatasetVersion(json);\n-            \n-            // clear possibly stale fields from the incoming dataset version.\n-            // creation and modification dates are updated by the commands.\n-            incomingVersion.setId(null);\n-            incomingVersion.setVersionNumber(null);\n-            incomingVersion.setMinorVersionNumber(null);\n-            incomingVersion.setVersionState(DatasetVersion.VersionState.DRAFT);\n-            incomingVersion.setDataset(ds);\n-            incomingVersion.setCreateTime(null);\n-            incomingVersion.setLastUpdateTime(null);\n-            \n-            if (!incomingVersion.getFileMetadatas().isEmpty()){\n-                return error( Response.Status.BAD_REQUEST, \"You may not add files via this api.\");\n-            }\n-            \n-            boolean updateDraft = ds.getLatestVersion().isDraft();\n-            \n-            DatasetVersion managedVersion;\n-            if ( updateDraft ) {\n-                final DatasetVersion editVersion = ds.getEditVersion();\n-                editVersion.setDatasetFields(incomingVersion.getDatasetFields());\n-                editVersion.setTermsOfUseAndAccess( incomingVersion.getTermsOfUseAndAccess() );\n-                Dataset managedDataset = execCommand(new UpdateDatasetVersionCommand(ds, req));\n-                managedVersion = managedDataset.getEditVersion();\n-            } else {\n-                managedVersion = execCommand(new CreateDatasetVersionCommand(req, ds, incomingVersion));\n-            }\n+      });\n+      return ok(BundleUtil.getStringFromBundle(\"datasets.api.updatePIDMetadata.success.for.update.all\"));\n+    });\n+  }\n+\n+  @PUT\n+  @Path(\"{id}/versions/{versionId}\")\n+  public Response updateDraftVersion(String jsonBody, @PathParam(\"id\") String id,\n+                                     @PathParam(\"versionId\") String versionId) {\n+\n+    if (!\":draft\".equals(versionId)) {\n+      return error(Response.Status.BAD_REQUEST, \"Only the :draft version can be updated\");\n+    }\n+\n+    try (StringReader rdr = new StringReader(jsonBody)) {\n+      DataverseRequest req = createDataverseRequest(findUserOrDie());\n+      Dataset ds = findDatasetOrDie(id);\n+      JsonObject json = Json.createReader(rdr).readObject();\n+      DatasetVersion incomingVersion = jsonParser().parseDatasetVersion(json);\n+\n+      // clear possibly stale fields from the incoming dataset version.\n+      // creation and modification dates are updated by the commands.\n+      incomingVersion.setId(null);\n+      incomingVersion.setVersionNumber(null);\n+      incomingVersion.setMinorVersionNumber(null);\n+      incomingVersion.setVersionState(DatasetVersion.VersionState.DRAFT);\n+      incomingVersion.setDataset(ds);\n+      incomingVersion.setCreateTime(null);\n+      incomingVersion.setLastUpdateTime(null);\n+\n+      if (!incomingVersion.getFileMetadatas().isEmpty()) {\n+        return error(Response.Status.BAD_REQUEST, \"You may not add files via this api.\");\n+      }\n+\n+      boolean updateDraft = ds.getLatestVersion().isDraft();\n+\n+      DatasetVersion managedVersion;\n+      if (updateDraft) {\n+        final DatasetVersion editVersion = ds.getEditVersion();\n+        editVersion.setDatasetFields(incomingVersion.getDatasetFields());\n+        editVersion.setTermsOfUseAndAccess(incomingVersion.getTermsOfUseAndAccess());\n+        Dataset managedDataset = execCommand(new UpdateDatasetVersionCommand(ds, req));\n+        managedVersion = managedDataset.getEditVersion();\n+      } else {\n+        managedVersion = execCommand(new CreateDatasetVersionCommand(req, ds, incomingVersion));\n+      }\n //            DatasetVersion managedVersion = execCommand( updateDraft\n //                                                             ? new UpdateDatasetVersionCommand(req, incomingVersion)\n //                                                             : new CreateDatasetVersionCommand(req, ds, incomingVersion));\n-            return ok( json(managedVersion) );\n-                    \n-        } catch (JsonParseException ex) {\n-            logger.log(Level.SEVERE, \"Semantic error parsing dataset version Json: \" + ex.getMessage(), ex);\n-            return error( Response.Status.BAD_REQUEST, \"Error parsing dataset version: \" + ex.getMessage() );\n-            \n-        } catch (WrappedResponse ex) {\n-            return ex.getResponse();\n-            \n-        }\n-    }\n-    \n-    @PUT\n-    @Path(\"{id}/deleteMetadata\")\n-    public Response deleteVersionMetadata(String jsonBody, @PathParam(\"id\") String id) throws WrappedResponse {\n+      return ok(json(managedVersion));\n \n-        DataverseRequest req = createDataverseRequest(findUserOrDie());\n+    } catch (JsonParseException ex) {\n+      logger.log(Level.SEVERE, \"Semantic error parsing dataset version Json: \" + ex.getMessage(), ex);\n+      return error(Response.Status.BAD_REQUEST, \"Error parsing dataset version: \" + ex.getMessage());\n \n-        return processDatasetFieldDataDelete(jsonBody, id, req);\n-    }\n+    } catch (WrappedResponse ex) {\n+      return ex.getResponse();\n \n-    private Response processDatasetFieldDataDelete(String jsonBody, String id, DataverseRequest req) {\n-        try (StringReader rdr = new StringReader(jsonBody)) {\n+    }\n+  }\n+\n+  @PUT\n+  @Path(\"{id}/deleteMetadata\")\n+  public Response deleteVersionMetadata(String jsonBody, @PathParam(\"id\") String id) throws WrappedResponse {\n+\n+    DataverseRequest req = createDataverseRequest(findUserOrDie());\n+\n+    return processDatasetFieldDataDelete(jsonBody, id, req);\n+  }\n+\n+  private Response processDatasetFieldDataDelete(String jsonBody, String id, DataverseRequest req) {\n+    try (StringReader rdr = new StringReader(jsonBody)) {\n+\n+      Dataset ds = findDatasetOrDie(id);\n+      JsonObject json = Json.createReader(rdr).readObject();\n+      DatasetVersion dsv = ds.getEditVersion();\n+\n+      List<DatasetField> fields = new LinkedList<>();\n+      DatasetField singleField = null;\n+\n+      JsonArray fieldsJson = json.getJsonArray(\"fields\");\n+      if (fieldsJson == null) {\n+        singleField = jsonParser().parseField(json, Boolean.FALSE);\n+        fields.add(singleField);\n+      } else {\n+        fields = jsonParser().parseMultipleFields(json);\n+      }\n+\n+      dsv.setVersionState(DatasetVersion.VersionState.DRAFT);\n+\n+      List<ControlledVocabularyValue> controlledVocabularyItemsToRemove = new ArrayList<ControlledVocabularyValue>();\n+      List<DatasetFieldValue> datasetFieldValueItemsToRemove = new ArrayList<DatasetFieldValue>();\n+      List<DatasetFieldCompoundValue> datasetFieldCompoundValueItemsToRemove =\n+        new ArrayList<DatasetFieldCompoundValue>();\n+\n+      for (DatasetField updateField : fields) {\n+        boolean found = false;\n+        for (DatasetField dsf : dsv.getDatasetFields()) {\n+          if (dsf.getDatasetFieldType().equals(updateField.getDatasetFieldType())) {\n+            if (dsf.getDatasetFieldType().isAllowMultiples()) {\n+              if (updateField.getDatasetFieldType().isControlledVocabulary()) {\n+                if (dsf.getDatasetFieldType().isAllowMultiples()) {\n+                  for (ControlledVocabularyValue cvv : updateField.getControlledVocabularyValues()) {\n+                    for (ControlledVocabularyValue existing : dsf.getControlledVocabularyValues()) {\n+                      if (existing.getStrValue().equals(cvv.getStrValue())) {\n+                        found = true;\n+                        controlledVocabularyItemsToRemove.add(existing);\n+                      }\n+                    }\n+                    if (!found) {\n+                      logger.log(Level.SEVERE,\n+                        \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" +\n+                          cvv.getStrValue() + \" not found.\");\n+                      return error(Response.Status.BAD_REQUEST,\n+                        \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" +\n+                          cvv.getStrValue() + \" not found.\");\n+                    }\n+                  }\n+                  for (ControlledVocabularyValue remove : controlledVocabularyItemsToRemove) {\n+                    dsf.getControlledVocabularyValues().remove(remove);\n+                  }\n \n-            Dataset ds = findDatasetOrDie(id);\n-            JsonObject json = Json.createReader(rdr).readObject();\n-            DatasetVersion dsv = ds.getEditVersion();\n+                } else {\n+                  if (dsf.getSingleControlledVocabularyValue().getStrValue()\n+                    .equals(updateField.getSingleControlledVocabularyValue().getStrValue())) {\n+                    found = true;\n+                    dsf.setSingleControlledVocabularyValue(null);\n+                  }\n \n-            List<DatasetField> fields = new LinkedList<>();\n-            DatasetField singleField = null;\n+                }\n+              } else {\n+                if (!updateField.getDatasetFieldType().isCompound()) {\n+                  if (dsf.getDatasetFieldType().isAllowMultiples()) {\n+                    for (DatasetFieldValue dfv : updateField.getDatasetFieldValues()) {\n+                      for (DatasetFieldValue edsfv : dsf.getDatasetFieldValues()) {\n+                        if (edsfv.getDisplayValue().equals(dfv.getDisplayValue())) {\n+                          found = true;\n+                          datasetFieldValueItemsToRemove.add(dfv);\n+                        }\n+                      }\n+                      if (!found) {\n+                        logger.log(Level.SEVERE,\n+                          \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" +\n+                            dfv.getDisplayValue() + \" not found.\");\n+                        return error(Response.Status.BAD_REQUEST,\n+                          \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" +\n+                            dfv.getDisplayValue() + \" not found.\");\n+                      }\n+                    }\n+                    datasetFieldValueItemsToRemove.forEach((remove) -> {\n+                      dsf.getDatasetFieldValues().remove(remove);\n+                    });\n \n-            JsonArray fieldsJson = json.getJsonArray(\"fields\");\n-            if (fieldsJson == null) {\n-                singleField = jsonParser().parseField(json, Boolean.FALSE);\n-                fields.add(singleField);\n-            } else {\n-                fields = jsonParser().parseMultipleFields(json);\n-            }\n+                  } else {\n+                    if (dsf.getSingleValue().getDisplayValue().equals(updateField.getSingleValue().getDisplayValue())) {\n+                      found = true;\n+                      dsf.setSingleValue(null);\n+                    }\n \n-            dsv.setVersionState(DatasetVersion.VersionState.DRAFT);\n-\n-            List<ControlledVocabularyValue> controlledVocabularyItemsToRemove = new ArrayList<ControlledVocabularyValue>();\n-            List<DatasetFieldValue> datasetFieldValueItemsToRemove = new ArrayList<DatasetFieldValue>();\n-            List<DatasetFieldCompoundValue> datasetFieldCompoundValueItemsToRemove = new ArrayList<DatasetFieldCompoundValue>();\n-\n-            for (DatasetField updateField : fields) {\n-                boolean found = false;\n-                for (DatasetField dsf : dsv.getDatasetFields()) {\n-                    if (dsf.getDatasetFieldType().equals(updateField.getDatasetFieldType())) {\n-                        if (dsf.getDatasetFieldType().isAllowMultiples()) { \n-                            if (updateField.getDatasetFieldType().isControlledVocabulary()) {\n-                                if (dsf.getDatasetFieldType().isAllowMultiples()) {\n-                                    for (ControlledVocabularyValue cvv : updateField.getControlledVocabularyValues()) {\n-                                        for (ControlledVocabularyValue existing : dsf.getControlledVocabularyValues()) {\n-                                            if (existing.getStrValue().equals(cvv.getStrValue())) {\n-                                                found = true;\n-                                                controlledVocabularyItemsToRemove.add(existing);\n-                                            }\n-                                        }\n-                                        if (!found) {\n-                                            logger.log(Level.SEVERE, \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" + cvv.getStrValue() + \" not found.\");\n-                                            return error(Response.Status.BAD_REQUEST, \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" + cvv.getStrValue() + \" not found.\");\n-                                        }\n-                                    }\n-                                    for (ControlledVocabularyValue remove : controlledVocabularyItemsToRemove) {\n-                                        dsf.getControlledVocabularyValues().remove(remove);\n-                                    }\n-\n-                                } else {\n-                                    if (dsf.getSingleControlledVocabularyValue().getStrValue().equals(updateField.getSingleControlledVocabularyValue().getStrValue())) {\n-                                        found = true;\n-                                        dsf.setSingleControlledVocabularyValue(null);\n-                                    }\n-\n-                                }\n-                            } else {\n-                                if (!updateField.getDatasetFieldType().isCompound()) {\n-                                    if (dsf.getDatasetFieldType().isAllowMultiples()) {\n-                                        for (DatasetFieldValue dfv : updateField.getDatasetFieldValues()) {\n-                                            for (DatasetFieldValue edsfv : dsf.getDatasetFieldValues()) {\n-                                                if (edsfv.getDisplayValue().equals(dfv.getDisplayValue())) {\n-                                                    found = true;\n-                                                    datasetFieldValueItemsToRemove.add(dfv);\n-                                                }\n-                                            }\n-                                            if (!found) {\n-                                                logger.log(Level.SEVERE, \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" + dfv.getDisplayValue() + \" not found.\");\n-                                                return error(Response.Status.BAD_REQUEST, \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" + dfv.getDisplayValue() + \" not found.\");\n-                                            }\n-                                        }\n-                                        datasetFieldValueItemsToRemove.forEach((remove) -> {\n-                                            dsf.getDatasetFieldValues().remove(remove);\n-                                        });\n-\n-                                    } else {\n-                                        if (dsf.getSingleValue().getDisplayValue().equals(updateField.getSingleValue().getDisplayValue())) {\n-                                            found = true;\n-                                            dsf.setSingleValue(null);\n-                                        }\n-\n-                                    }\n-                                } else {\n-                                    for (DatasetFieldCompoundValue dfcv : updateField.getDatasetFieldCompoundValues()) {\n-                                        String deleteVal = getCompoundDisplayValue(dfcv);\n-                                        for (DatasetFieldCompoundValue existing : dsf.getDatasetFieldCompoundValues()) {\n-                                            String existingString = getCompoundDisplayValue(existing);\n-                                            if (existingString.equals(deleteVal)) {\n-                                                found = true;\n-                                                datasetFieldCompoundValueItemsToRemove.add(existing);\n-                                            }\n-                                        }\n-                                        datasetFieldCompoundValueItemsToRemove.forEach((remove) -> {\n-                                            dsf.getDatasetFieldCompoundValues().remove(remove);\n-                                        });\n-                                        if (!found) { \n-                                            logger.log(Level.SEVERE, \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" + deleteVal + \" not found.\");\n-                                            return error(Response.Status.BAD_REQUEST, \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" + deleteVal + \" not found.\");\n-                                        }\n-                                    }\n-                                }\n-                            }\n-                        } else {\n-                            found = true;\n-                            dsf.setSingleValue(null);\n-                            dsf.setSingleControlledVocabularyValue(null);\n-                        }\n-                        break;\n+                  }\n+                } else {\n+                  for (DatasetFieldCompoundValue dfcv : updateField.getDatasetFieldCompoundValues()) {\n+                    String deleteVal = getCompoundDisplayValue(dfcv);\n+                    for (DatasetFieldCompoundValue existing : dsf.getDatasetFieldCompoundValues()) {\n+                      String existingString = getCompoundDisplayValue(existing);\n+                      if (existingString.equals(deleteVal)) {\n+                        found = true;\n+                        datasetFieldCompoundValueItemsToRemove.add(existing);\n+                      }\n                     }\n+                    datasetFieldCompoundValueItemsToRemove.forEach((remove) -> {\n+                      dsf.getDatasetFieldCompoundValues().remove(remove);\n+                    });\n+                    if (!found) {\n+                      logger.log(Level.SEVERE,\n+                        \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" +\n+                          deleteVal + \" not found.\");\n+                      return error(Response.Status.BAD_REQUEST,\n+                        \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" +\n+                          deleteVal + \" not found.\");\n+                    }\n+                  }\n                 }\n-                if (!found){\n-                    String displayValue = !updateField.getDisplayValue().isEmpty() ? updateField.getDisplayValue() : updateField.getCompoundDisplayValue();\n-                    logger.log(Level.SEVERE, \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" + displayValue + \" not found.\" );\n-                    return error(Response.Status.BAD_REQUEST, \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" + displayValue + \" not found.\" );\n-                }\n-            }           \n+              }\n+            } else {\n+              found = true;\n+              dsf.setSingleValue(null);\n+              dsf.setSingleControlledVocabularyValue(null);\n+            }\n+            break;\n+          }\n+        }\n+        if (!found) {\n+          String displayValue = !updateField.getDisplayValue().isEmpty() ? updateField.getDisplayValue() :\n+            updateField.getCompoundDisplayValue();\n+          logger.log(Level.SEVERE,\n+            \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" + displayValue +\n+              \" not found.\");\n+          return error(Response.Status.BAD_REQUEST,\n+            \"Delete metadata failed: \" + updateField.getDatasetFieldType().getDisplayName() + \": \" + displayValue +\n+              \" not found.\");\n+        }\n+      }\n \n \n-            \n-            boolean updateDraft = ds.getLatestVersion().isDraft();\n-            DatasetVersion managedVersion = updateDraft \n-                    ? execCommand(new UpdateDatasetVersionCommand(ds, req)).getEditVersion()\n-                    : execCommand(new CreateDatasetVersionCommand(req, ds, dsv));\n-            return ok(json(managedVersion));\n+      boolean updateDraft = ds.getLatestVersion().isDraft();\n+      DatasetVersion managedVersion = updateDraft\n+        ? execCommand(new UpdateDatasetVersionCommand(ds, req)).getEditVersion()\n+        : execCommand(new CreateDatasetVersionCommand(req, ds, dsv));\n+      return ok(json(managedVersion));\n \n-        } catch (JsonParseException ex) {\n-            logger.log(Level.SEVERE, \"Semantic error parsing dataset update Json: \" + ex.getMessage(), ex);\n-            return error(Response.Status.BAD_REQUEST, \"Error processing metadata delete: \" + ex.getMessage());\n+    } catch (JsonParseException ex) {\n+      logger.log(Level.SEVERE, \"Semantic error parsing dataset update Json: \" + ex.getMessage(), ex);\n+      return error(Response.Status.BAD_REQUEST, \"Error processing metadata delete: \" + ex.getMessage());\n \n-        } catch (WrappedResponse ex) {\n-            logger.log(Level.SEVERE, \"Delete metadata error: \" + ex.getMessage(), ex);\n-            return ex.getResponse();\n+    } catch (WrappedResponse ex) {\n+      logger.log(Level.SEVERE, \"Delete metadata error: \" + ex.getMessage(), ex);\n+      return ex.getResponse();\n \n-        }\n-    \n-    }\n-    \n-    private String getCompoundDisplayValue (DatasetFieldCompoundValue dscv){\n-        String returnString = \"\";\n-                    for (DatasetField dsf : dscv.getChildDatasetFields()) {\n-                for (String value : dsf.getValues()) {\n-                    if (!(value == null)) {\n-                        returnString += (returnString.isEmpty() ? \"\" : \"; \") + value.trim();\n-                    }\n-                }\n-            }\n-        return returnString;\n     }\n-    \n-    @PUT\n-    @Path(\"{id}/editMetadata\")\n-    public Response editVersionMetadata(String jsonBody, @PathParam(\"id\") String id, @QueryParam(\"replace\") Boolean replace) {\n \n-        Boolean replaceData = replace != null;\n-        DataverseRequest req = null;\n-        try {\n-         req = createDataverseRequest(findUserOrDie());\n-        } catch (WrappedResponse ex) {\n-            logger.log(Level.SEVERE, \"Edit metdata error: \" + ex.getMessage(), ex);\n-            return ex.getResponse();\n-        }\n+  }\n \n-        return processDatasetUpdate(jsonBody, id, req, replaceData);\n+  private String getCompoundDisplayValue(DatasetFieldCompoundValue dscv) {\n+    String returnString = \"\";\n+    for (DatasetField dsf : dscv.getChildDatasetFields()) {\n+      for (String value : dsf.getValues()) {\n+        if (!(value == null)) {\n+          returnString += (returnString.isEmpty() ? \"\" : \"; \") + value.trim();\n+        }\n+      }\n+    }\n+    return returnString;\n+  }\n+\n+  @PUT\n+  @Path(\"{id}/editMetadata\")\n+  public Response editVersionMetadata(String jsonBody, @PathParam(\"id\") String id,\n+                                      @QueryParam(\"replace\") Boolean replace) {\n+\n+    Boolean replaceData = replace != null;\n+    DataverseRequest req = null;\n+    try {\n+      req = createDataverseRequest(findUserOrDie());\n+    } catch (WrappedResponse ex) {\n+      logger.log(Level.SEVERE, \"Edit metdata error: \" + ex.getMessage(), ex);\n+      return ex.getResponse();\n     }\n-    \n-    \n-    private Response processDatasetUpdate(String jsonBody, String id, DataverseRequest req, Boolean replaceData){\n-        try (StringReader rdr = new StringReader(jsonBody)) {\n-           \n-            Dataset ds = findDatasetOrDie(id);\n-            JsonObject json = Json.createReader(rdr).readObject();\n-            DatasetVersion dsv = ds.getEditVersion();\n-            \n-            List<DatasetField> fields = new LinkedList<>();\n-            DatasetField singleField = null; \n-            \n-            JsonArray fieldsJson = json.getJsonArray(\"fields\");\n-            if( fieldsJson == null ){\n-                singleField  = jsonParser().parseField(json, Boolean.FALSE);\n-                fields.add(singleField);\n-            } else{\n-                fields = jsonParser().parseMultipleFields(json);\n-            }\n-            \n \n-            String valdationErrors = validateDatasetFieldValues(fields);\n+    return processDatasetUpdate(jsonBody, id, req, replaceData);\n+  }\n \n-            if (!valdationErrors.isEmpty()) {\n-                logger.log(Level.SEVERE, \"Semantic error parsing dataset update Json: \" + valdationErrors, valdationErrors);\n-                return error(Response.Status.BAD_REQUEST, \"Error parsing dataset update: \" + valdationErrors);\n-            }\n \n-            dsv.setVersionState(DatasetVersion.VersionState.DRAFT);\n+  private Response processDatasetUpdate(String jsonBody, String id, DataverseRequest req, Boolean replaceData) {\n+    try (StringReader rdr = new StringReader(jsonBody)) {\n \n-            //loop through the update fields     \n-            // and compare to the version fields  \n-            //if exist add/replace values\n-            //if not add entire dsf\n-            for (DatasetField updateField : fields) {\n-                boolean found = false;\n-                for (DatasetField dsf : dsv.getDatasetFields()) {\n-                    if (dsf.getDatasetFieldType().equals(updateField.getDatasetFieldType())) {\n-                        found = true;\n-                        if (dsf.isEmpty() || dsf.getDatasetFieldType().isAllowMultiples() || replaceData) {\n-                            List priorCVV = new ArrayList<>();\n-                            String cvvDisplay = \"\";\n-\n-                            if (updateField.getDatasetFieldType().isControlledVocabulary()) {\n-                                cvvDisplay = dsf.getDisplayValue();\n-                                for (ControlledVocabularyValue cvvOld : dsf.getControlledVocabularyValues()) {\n-                                    priorCVV.add(cvvOld);\n-                                }\n-                            }\n-\n-                            if (replaceData) {\n-                                if (dsf.getDatasetFieldType().isAllowMultiples()) {\n-                                    dsf.setDatasetFieldCompoundValues(new ArrayList<>());\n-                                    dsf.setDatasetFieldValues(new ArrayList<>());\n-                                    dsf.setControlledVocabularyValues(new ArrayList<>());\n-                                    priorCVV.clear();\n-                                    dsf.getControlledVocabularyValues().clear();\n-                                } else {\n-                                    dsf.setSingleValue(\"\");\n-                                    dsf.setSingleControlledVocabularyValue(null);\n-                                }\n-                            }\n-                            if (updateField.getDatasetFieldType().isControlledVocabulary()) {\n-                                if (dsf.getDatasetFieldType().isAllowMultiples()) {\n-                                    for (ControlledVocabularyValue cvv : updateField.getControlledVocabularyValues()) {\n-                                        if (!cvvDisplay.contains(cvv.getStrValue())) {\n-                                            priorCVV.add(cvv);\n-                                        }\n-                                    }\n-                                    dsf.setControlledVocabularyValues(priorCVV);\n-                                } else {\n-                                    dsf.setSingleControlledVocabularyValue(updateField.getSingleControlledVocabularyValue());\n-                                }\n-                            } else {\n-                                if (!updateField.getDatasetFieldType().isCompound()) {\n-                                    if (dsf.getDatasetFieldType().isAllowMultiples()) {\n-                                        for (DatasetFieldValue dfv : updateField.getDatasetFieldValues()) {\n-                                            if (!dsf.getDisplayValue().contains(dfv.getDisplayValue())) {\n-                                                dfv.setDatasetField(dsf);\n-                                                dsf.getDatasetFieldValues().add(dfv);\n-                                            }\n-                                        }\n-                                    } else {\n-                                        dsf.setSingleValue(updateField.getValue());\n-                                    }\n-                                } else {\n-                                    for (DatasetFieldCompoundValue dfcv : updateField.getDatasetFieldCompoundValues()) {\n-                                        if (!dsf.getCompoundDisplayValue().contains(updateField.getCompoundDisplayValue())) {\n-                                            dfcv.setParentDatasetField(dsf);\n-                                            dsf.setDatasetVersion(dsv);\n-                                            dsf.getDatasetFieldCompoundValues().add(dfcv);\n-                                        }\n-                                    }\n-                                }\n-                            }\n-                        } else {\n-                            if (!dsf.isEmpty() && !dsf.getDatasetFieldType().isAllowMultiples() || !replaceData) {\n-                                return error(Response.Status.BAD_REQUEST, \"You may not add data to a field that already has data and does not allow multiples. Use replace=true to replace existing data (\" + dsf.getDatasetFieldType().getDisplayName() + \")\");\n-                            }\n-                        }\n-                        break;\n-                    }\n-                }\n-                if (!found) {\n-                    updateField.setDatasetVersion(dsv);\n-                    dsv.getDatasetFields().add(updateField);\n-                }\n-            }\n-            boolean updateDraft = ds.getLatestVersion().isDraft();\n-            DatasetVersion managedVersion;\n+      Dataset ds = findDatasetOrDie(id);\n+      JsonObject json = Json.createReader(rdr).readObject();\n+      DatasetVersion dsv = ds.getEditVersion();\n \n-            if (updateDraft) {\n-                managedVersion = execCommand(new UpdateDatasetVersionCommand(ds, req)).getEditVersion();\n-            } else {\n-                managedVersion = execCommand(new CreateDatasetVersionCommand(req, ds, dsv));\n-            }\n+      List<DatasetField> fields = new LinkedList<>();\n+      DatasetField singleField = null;\n \n-            return ok(json(managedVersion));\n+      JsonArray fieldsJson = json.getJsonArray(\"fields\");\n+      if (fieldsJson == null) {\n+        singleField = jsonParser().parseField(json, Boolean.FALSE);\n+        fields.add(singleField);\n+      } else {\n+        fields = jsonParser().parseMultipleFields(json);\n+      }\n \n-        } catch (JsonParseException ex) {\n-            logger.log(Level.SEVERE, \"Semantic error parsing dataset update Json: \" + ex.getMessage(), ex);\n-            return error(Response.Status.BAD_REQUEST, \"Error parsing dataset update: \" + ex.getMessage());\n \n-        } catch (WrappedResponse ex) {\n-            logger.log(Level.SEVERE, \"Update metdata error: \" + ex.getMessage(), ex);\n-            return ex.getResponse();\n+      String valdationErrors = validateDatasetFieldValues(fields);\n \n-        }\n-    }\n-    \n-    private String validateDatasetFieldValues(List<DatasetField> fields) {\n-        StringBuilder error = new StringBuilder();\n-\n-        for (DatasetField dsf : fields) {\n-            if (dsf.getDatasetFieldType().isAllowMultiples() && dsf.getControlledVocabularyValues().isEmpty()\n-                    && dsf.getDatasetFieldCompoundValues().isEmpty() && dsf.getDatasetFieldValues().isEmpty()) {\n-                error.append(\"Empty multiple value for field: \").append(dsf.getDatasetFieldType().getDisplayName()).append(\" \");\n-            } else if (!dsf.getDatasetFieldType().isAllowMultiples() && dsf.getSingleValue().getValue().isEmpty()) {\n-                error.append(\"Empty value for field: \").append(dsf.getDatasetFieldType().getDisplayName()).append(\" \");\n-            }\n-        }\n+      if (!valdationErrors.isEmpty()) {\n+        logger.log(Level.SEVERE, \"Semantic error parsing dataset update Json: \" + valdationErrors, valdationErrors);\n+        return error(Response.Status.BAD_REQUEST, \"Error parsing dataset update: \" + valdationErrors);\n+      }\n \n-        if (!error.toString().isEmpty()) {\n-            return (error.toString());\n-        }\n-        return \"\";\n-    }\n-    \n-    /**\n-     * @deprecated This was shipped as a GET but should have been a POST, see https://github.com/IQSS/dataverse/issues/2431\n-     */\n-    @GET\n-    @Path(\"{id}/actions/:publish\")\n-    @Deprecated\n-    public Response publishDataseUsingGetDeprecated( @PathParam(\"id\") String id, @QueryParam(\"type\") String type ) {\n-        logger.info(\"publishDataseUsingGetDeprecated called on id \" + id + \". Encourage use of POST rather than GET, which is deprecated.\");\n-        return publishDataset(id, type, false);\n-    }\n+      dsv.setVersionState(DatasetVersion.VersionState.DRAFT);\n \n-    @POST\n-    @Path(\"{id}/actions/:publish\")\n-    public Response publishDataset(@PathParam(\"id\") String id, @QueryParam(\"type\") String type, @QueryParam(\"assureIsIndexed\") boolean mustBeIndexed) {\n-        try {\n-            if (type == null) {\n-                return error(Response.Status.BAD_REQUEST, \"Missing 'type' parameter (either 'major','minor', or 'updatecurrent').\");\n-            }\n-            boolean updateCurrent=false;\n-            AuthenticatedUser user = findAuthenticatedUserOrDie();\n-            type = type.toLowerCase();\n-            boolean isMinor=false;\n-            switch (type) {\n-                case \"minor\":\n-                    isMinor = true;\n-                    break;\n-                case \"major\":\n-                    isMinor = false;\n-                    break;\n-            case \"updatecurrent\":\n-                if(user.isSuperuser()) {\n-                  updateCurrent=true;\n-                } else {\n-                    return error(Response.Status.FORBIDDEN, \"Only superusers can update the current version\"); \n-                }\n-                break;\n-                default:\n-                return error(Response.Status.BAD_REQUEST, \"Illegal 'type' parameter value '\" + type + \"'. It needs to be either 'major', 'minor', or 'updatecurrent'.\");\n-            }\n+      //loop through the update fields\n+      // and compare to the version fields\n+      //if exist add/replace values\n+      //if not add entire dsf\n+      for (DatasetField updateField : fields) {\n+        boolean found = false;\n+        for (DatasetField dsf : dsv.getDatasetFields()) {\n+          if (dsf.getDatasetFieldType().equals(updateField.getDatasetFieldType())) {\n+            found = true;\n+            if (dsf.isEmpty() || dsf.getDatasetFieldType().isAllowMultiples() || replaceData) {\n+              List priorCVV = new ArrayList<>();\n+              String cvvDisplay = \"\";\n \n-            Dataset ds = findDatasetOrDie(id);\n-            if (mustBeIndexed) {\n-                logger.fine(\"IT: \" + ds.getIndexTime());\n-                logger.fine(\"MT: \" + ds.getModificationTime());\n-                logger.fine(\"PIT: \" + ds.getPermissionIndexTime());\n-                logger.fine(\"PMT: \" + ds.getPermissionModificationTime());\n-                if (ds.getIndexTime() != null && ds.getModificationTime() != null) {\n-                    logger.fine(\"ITMT: \" + (ds.getIndexTime().compareTo(ds.getModificationTime()) <= 0));\n+              if (updateField.getDatasetFieldType().isControlledVocabulary()) {\n+                cvvDisplay = dsf.getDisplayValue();\n+                for (ControlledVocabularyValue cvvOld : dsf.getControlledVocabularyValues()) {\n+                  priorCVV.add(cvvOld);\n                 }\n-                /*\n-                 * Some calls, such as the /datasets/actions/:import* commands do not set the\n-                 * modification or permission modification times. The checks here are trying to\n-                 * see if indexing or permissionindexing could be pending, so they check to see\n-                 * if the relevant modification time is set and if so, whether the index is also\n-                 * set and if so, if it after the modification time. If the modification time is\n-                 * set and the index time is null or is before the mod time, the 409/conflict\n-                 * error is returned.\n-                 * \n-                 */\n-                if ((ds.getModificationTime()!=null && (ds.getIndexTime() == null || (ds.getIndexTime().compareTo(ds.getModificationTime()) <= 0))) ||\n-                        (ds.getPermissionModificationTime()!=null && (ds.getPermissionIndexTime() == null || (ds.getPermissionIndexTime().compareTo(ds.getPermissionModificationTime()) <= 0)))) {\n-                    return error(Response.Status.CONFLICT, \"Dataset is awaiting indexing\");\n+              }\n+\n+              if (replaceData) {\n+                if (dsf.getDatasetFieldType().isAllowMultiples()) {\n+                  dsf.setDatasetFieldCompoundValues(new ArrayList<>());\n+                  dsf.setDatasetFieldValues(new ArrayList<>());\n+                  dsf.setControlledVocabularyValues(new ArrayList<>());\n+                  priorCVV.clear();\n+                  dsf.getControlledVocabularyValues().clear();\n+                } else {\n+                  dsf.setSingleValue(\"\");\n+                  dsf.setSingleControlledVocabularyValue(null);\n                 }\n-            }\n-            if (updateCurrent) {\n-                /*\n-                 * Note: The code here mirrors that in the\n-                 * edu.harvard.iq.dataverse.DatasetPage:updateCurrentVersion method. Any changes\n-                 * to the core logic (i.e. beyond updating the messaging about results) should\n-                 * be applied to the code there as well.\n-                 */\n-                String errorMsg = null;\n-                String successMsg = null;\n-                try {\n-                    CuratePublishedDatasetVersionCommand cmd = new CuratePublishedDatasetVersionCommand(ds, createDataverseRequest(user));\n-                    ds = commandEngine.submit(cmd);\n-                    successMsg = BundleUtil.getStringFromBundle(\"datasetversion.update.success\");\n-\n-                    // If configured, update archive copy as well\n-                    String className = settingsService.get(SettingsServiceBean.Key.ArchiverClassName.toString());\n-                    DatasetVersion updateVersion = ds.getLatestVersion();\n-                    AbstractSubmitToArchiveCommand archiveCommand = ArchiverUtil.createSubmitToArchiveCommand(className, createDataverseRequest(user), updateVersion);\n-                    if (archiveCommand != null) {\n-                        // Delete the record of any existing copy since it is now out of date/incorrect\n-                        updateVersion.setArchivalCopyLocation(null);\n-                        /*\n-                         * Then try to generate and submit an archival copy. Note that running this\n-                         * command within the CuratePublishedDatasetVersionCommand was causing an error:\n-                         * \"The attribute [id] of class\n-                         * [edu.harvard.iq.dataverse.DatasetFieldCompoundValue] is mapped to a primary\n-                         * key column in the database. Updates are not allowed.\" To avoid that, and to\n-                         * simplify reporting back to the GUI whether this optional step succeeded, I've\n-                         * pulled this out as a separate submit().\n-                         */\n-                        try {\n-                            updateVersion = commandEngine.submit(archiveCommand);\n-                            if (updateVersion.getArchivalCopyLocation() != null) {\n-                                successMsg = BundleUtil.getStringFromBundle(\"datasetversion.update.archive.success\");\n-                            } else {\n-                                successMsg = BundleUtil.getStringFromBundle(\"datasetversion.update.archive.failure\");\n-                            }\n-                        } catch (CommandException ex) {\n-                            successMsg = BundleUtil.getStringFromBundle(\"datasetversion.update.archive.failure\") + \" - \" + ex.toString();\n-                            logger.severe(ex.getMessage());\n-                        }\n+              }\n+              if (updateField.getDatasetFieldType().isControlledVocabulary()) {\n+                if (dsf.getDatasetFieldType().isAllowMultiples()) {\n+                  for (ControlledVocabularyValue cvv : updateField.getControlledVocabularyValues()) {\n+                    if (!cvvDisplay.contains(cvv.getStrValue())) {\n+                      priorCVV.add(cvv);\n                     }\n-                } catch (CommandException ex) {\n-                    errorMsg = BundleUtil.getStringFromBundle(\"datasetversion.update.failure\") + \" - \" + ex.toString();\n-                    logger.severe(ex.getMessage());\n+                  }\n+                  dsf.setControlledVocabularyValues(priorCVV);\n+                } else {\n+                  dsf.setSingleControlledVocabularyValue(updateField.getSingleControlledVocabularyValue());\n                 }\n-                if (errorMsg != null) {\n-                    return error(Response.Status.INTERNAL_SERVER_ERROR, errorMsg);\n+              } else {\n+                if (!updateField.getDatasetFieldType().isCompound()) {\n+                  if (dsf.getDatasetFieldType().isAllowMultiples()) {\n+                    for (DatasetFieldValue dfv : updateField.getDatasetFieldValues()) {\n+                      if (!dsf.getDisplayValue().contains(dfv.getDisplayValue())) {\n+                        dfv.setDatasetField(dsf);\n+                        dsf.getDatasetFieldValues().add(dfv);\n+                      }\n+                    }\n+                  } else {\n+                    dsf.setSingleValue(updateField.getValue());\n+                  }\n                 } else {\n-                    return Response.ok(Json.createObjectBuilder()\n-                            .add(\"status\", STATUS_OK)\n-                            .add(\"status_details\", successMsg)\n-                            .add(\"data\", json(ds)).build())\n-                            .type(MediaType.APPLICATION_JSON)\n-                            .build();\n+                  for (DatasetFieldCompoundValue dfcv : updateField.getDatasetFieldCompoundValues()) {\n+                    if (!dsf.getCompoundDisplayValue().contains(updateField.getCompoundDisplayValue())) {\n+                      dfcv.setParentDatasetField(dsf);\n+                      dsf.setDatasetVersion(dsv);\n+                      dsf.getDatasetFieldCompoundValues().add(dfcv);\n+                    }\n+                  }\n                 }\n+              }\n             } else {\n-            PublishDatasetResult res = execCommand(new PublishDatasetCommand(ds,\n-                        createDataverseRequest(user),\n-                    isMinor));\n-            return res.isWorkflow() ? accepted(json(res.getDataset())) : ok(json(res.getDataset()));\n+              if (!dsf.isEmpty() && !dsf.getDatasetFieldType().isAllowMultiples() || !replaceData) {\n+                return error(Response.Status.BAD_REQUEST,\n+                  \"You may not add data to a field that already has data and does not allow multiples. Use replace=true to replace existing data (\" +\n+                    dsf.getDatasetFieldType().getDisplayName() + \")\");\n+              }\n             }\n-        } catch (WrappedResponse ex) {\n-            return ex.getResponse();\n+            break;\n+          }\n+        }\n+        if (!found) {\n+          updateField.setDatasetVersion(dsv);\n+          dsv.getDatasetFields().add(updateField);\n         }\n+      }\n+      boolean updateDraft = ds.getLatestVersion().isDraft();\n+      DatasetVersion managedVersion;\n+\n+      if (updateDraft) {\n+        managedVersion = execCommand(new UpdateDatasetVersionCommand(ds, req)).getEditVersion();\n+      } else {\n+        managedVersion = execCommand(new CreateDatasetVersionCommand(req, ds, dsv));\n+      }\n+\n+      return ok(json(managedVersion));\n+\n+    } catch (JsonParseException ex) {\n+      logger.log(Level.SEVERE, \"Semantic error parsing dataset update Json: \" + ex.getMessage(), ex);\n+      return error(Response.Status.BAD_REQUEST, \"Error parsing dataset update: \" + ex.getMessage());\n+\n+    } catch (WrappedResponse ex) {\n+      logger.log(Level.SEVERE, \"Update metdata error: \" + ex.getMessage(), ex);\n+      return ex.getResponse();\n+\n+    }\n+  }\n+\n+  private String validateDatasetFieldValues(List<DatasetField> fields) {\n+    StringBuilder error = new StringBuilder();\n+\n+    for (DatasetField dsf : fields) {\n+      if (dsf.getDatasetFieldType().isAllowMultiples() && dsf.getControlledVocabularyValues().isEmpty()\n+        && dsf.getDatasetFieldCompoundValues().isEmpty() && dsf.getDatasetFieldValues().isEmpty()) {\n+        error.append(\"Empty multiple value for field: \").append(dsf.getDatasetFieldType().getDisplayName()).append(\" \");\n+      } else if (!dsf.getDatasetFieldType().isAllowMultiples() && dsf.getSingleValue().getValue().isEmpty()) {\n+        error.append(\"Empty value for field: \").append(dsf.getDatasetFieldType().getDisplayName()).append(\" \");\n+      }\n+    }\n+\n+    if (!error.toString().isEmpty()) {\n+      return (error.toString());\n     }\n-    \n-    @POST\n-    @Path(\"{id}/move/{targetDataverseAlias}\")\n-    public Response moveDataset(@PathParam(\"id\") String id, @PathParam(\"targetDataverseAlias\") String targetDataverseAlias, @QueryParam(\"forceMove\") Boolean force) {\n+    return \"\";\n+  }\n+\n+  /**\n+   * @deprecated This was shipped as a GET but should have been a POST, see https://github.com/IQSS/dataverse/issues/2431\n+   */\n+  @GET\n+  @Path(\"{id}/actions/:publish\")\n+  @Deprecated\n+  public Response publishDataseUsingGetDeprecated(@PathParam(\"id\") String id, @QueryParam(\"type\") String type) {\n+    logger.info(\"publishDataseUsingGetDeprecated called on id \" + id +\n+      \". Encourage use of POST rather than GET, which is deprecated.\");\n+    return publishDataset(id, type, false);\n+  }\n+\n+  @POST\n+  @Path(\"{id}/actions/:publish\")\n+  public Response publishDataset(@PathParam(\"id\") String id, @QueryParam(\"type\") String type,\n+                                 @QueryParam(\"assureIsIndexed\") boolean mustBeIndexed) {\n+    try {\n+      if (type == null) {\n+        return error(Response.Status.BAD_REQUEST,\n+          \"Missing 'type' parameter (either 'major','minor', or 'updatecurrent').\");\n+      }\n+      boolean updateCurrent = false;\n+      AuthenticatedUser user = findAuthenticatedUserOrDie();\n+      type = type.toLowerCase();\n+      boolean isMinor = false;\n+      switch (type) {\n+        case \"minor\":\n+          isMinor = true;\n+          break;\n+        case \"major\":\n+          isMinor = false;\n+          break;\n+        case \"updatecurrent\":\n+          if (user.isSuperuser()) {\n+            updateCurrent = true;\n+          } else {\n+            return error(Response.Status.FORBIDDEN, \"Only superusers can update the current version\");\n+          }\n+          break;\n+        default:\n+          return error(Response.Status.BAD_REQUEST, \"Illegal 'type' parameter value '\" + type +\n+            \"'. It needs to be either 'major', 'minor', or 'updatecurrent'.\");\n+      }\n+\n+      Dataset ds = findDatasetOrDie(id);\n+      if (mustBeIndexed) {\n+        logger.fine(\"IT: \" + ds.getIndexTime());\n+        logger.fine(\"MT: \" + ds.getModificationTime());\n+        logger.fine(\"PIT: \" + ds.getPermissionIndexTime());\n+        logger.fine(\"PMT: \" + ds.getPermissionModificationTime());\n+        if (ds.getIndexTime() != null && ds.getModificationTime() != null) {\n+          logger.fine(\"ITMT: \" + (ds.getIndexTime().compareTo(ds.getModificationTime()) <= 0));\n+        }\n+        /*\n+         * Some calls, such as the /datasets/actions/:import* commands do not set the\n+         * modification or permission modification times. The checks here are trying to\n+         * see if indexing or permissionindexing could be pending, so they check to see\n+         * if the relevant modification time is set and if so, whether the index is also\n+         * set and if so, if it after the modification time. If the modification time is\n+         * set and the index time is null or is before the mod time, the 409/conflict\n+         * error is returned.\n+         *\n+         */\n+        if ((ds.getModificationTime() != null &&\n+          (ds.getIndexTime() == null || (ds.getIndexTime().compareTo(ds.getModificationTime()) <= 0))) ||\n+          (ds.getPermissionModificationTime() != null && (ds.getPermissionIndexTime() == null ||\n+            (ds.getPermissionIndexTime().compareTo(ds.getPermissionModificationTime()) <= 0)))) {\n+          return error(Response.Status.CONFLICT, \"Dataset is awaiting indexing\");\n+        }\n+      }\n+      if (updateCurrent) {\n+        /*\n+         * Note: The code here mirrors that in the\n+         * edu.harvard.iq.dataverse.DatasetPage:updateCurrentVersion method. Any changes\n+         * to the core logic (i.e. beyond updating the messaging about results) should\n+         * be applied to the code there as well.\n+         */\n+        String errorMsg = null;\n+        String successMsg = null;\n         try {\n-            User u = findUserOrDie();            \n-            Dataset ds = findDatasetOrDie(id);\n-            Dataverse target = dataverseService.findByAlias(targetDataverseAlias);\n-            if (target == null) {\n-                return error(Response.Status.BAD_REQUEST, BundleUtil.getStringFromBundle(\"datasets.api.moveDataset.error.targetDataverseNotFound\"));\n-            }\n-            //Command requires Super user - it will be tested by the command\n-            execCommand(new MoveDatasetCommand(\n-                    createDataverseRequest(u), ds, target, force\n-            ));\n-            return ok(BundleUtil.getStringFromBundle(\"datasets.api.moveDataset.success\"));\n-        } catch (WrappedResponse ex) {\n-            if (ex.getCause() instanceof UnforcedCommandException) {\n-                return ex.refineResponse(BundleUtil.getStringFromBundle(\"datasets.api.moveDataset.error.suggestForce\"));\n-            } else {\n-                return ex.getResponse();\n+          CuratePublishedDatasetVersionCommand cmd =\n+            new CuratePublishedDatasetVersionCommand(ds, createDataverseRequest(user));\n+          ds = commandEngine.submit(cmd);\n+          successMsg = BundleUtil.getStringFromBundle(\"datasetversion.update.success\");\n+\n+          // If configured, update archive copy as well\n+          String className = settingsService.get(SettingsServiceBean.Key.ArchiverClassName.toString());\n+          DatasetVersion updateVersion = ds.getLatestVersion();\n+          AbstractSubmitToArchiveCommand archiveCommand =\n+            ArchiverUtil.createSubmitToArchiveCommand(className, createDataverseRequest(user), updateVersion);\n+          if (archiveCommand != null) {\n+            // Delete the record of any existing copy since it is now out of date/incorrect\n+            updateVersion.setArchivalCopyLocation(null);\n+            /*\n+             * Then try to generate and submit an archival copy. Note that running this\n+             * command within the CuratePublishedDatasetVersionCommand was causing an error:\n+             * \"The attribute [id] of class\n+             * [edu.harvard.iq.dataverse.DatasetFieldCompoundValue] is mapped to a primary\n+             * key column in the database. Updates are not allowed.\" To avoid that, and to\n+             * simplify reporting back to the GUI whether this optional step succeeded, I've\n+             * pulled this out as a separate submit().\n+             */\n+            try {\n+              updateVersion = commandEngine.submit(archiveCommand);\n+              if (updateVersion.getArchivalCopyLocation() != null) {\n+                successMsg = BundleUtil.getStringFromBundle(\"datasetversion.update.archive.success\");\n+              } else {\n+                successMsg = BundleUtil.getStringFromBundle(\"datasetversion.update.archive.failure\");\n+              }\n+            } catch (CommandException ex) {\n+              successMsg =\n+                BundleUtil.getStringFromBundle(\"datasetversion.update.archive.failure\") + \" - \" + ex.toString();\n+              logger.severe(ex.getMessage());\n             }\n+          }\n+        } catch (CommandException ex) {\n+          errorMsg = BundleUtil.getStringFromBundle(\"datasetversion.update.failure\") + \" - \" + ex.toString();\n+          logger.severe(ex.getMessage());\n+        }\n+        if (errorMsg != null) {\n+          return error(Response.Status.INTERNAL_SERVER_ERROR, errorMsg);\n+        } else {\n+          return Response.ok(Json.createObjectBuilder()\n+            .add(\"status\", STATUS_OK)\n+            .add(\"status_details\", successMsg)\n+            .add(\"data\", json(ds)).build())\n+            .type(MediaType.APPLICATION_JSON)\n+            .build();\n         }\n+      } else {\n+        PublishDatasetResult res = execCommand(new PublishDatasetCommand(ds,\n+          createDataverseRequest(user),\n+          isMinor));\n+        return res.isWorkflow() ? accepted(json(res.getDataset())) : ok(json(res.getDataset()));\n+      }\n+    } catch (WrappedResponse ex) {\n+      return ex.getResponse();\n     }\n-    \n-    @PUT\n-    @Path(\"{linkedDatasetId}/link/{linkingDataverseAlias}\") \n-    public Response linkDataset(@PathParam(\"linkedDatasetId\") String linkedDatasetId, @PathParam(\"linkingDataverseAlias\") String linkingDataverseAlias) {        \n-        try{\n-            User u = findUserOrDie();            \n-            Dataset linked = findDatasetOrDie(linkedDatasetId);\n-            Dataverse linking = findDataverseOrDie(linkingDataverseAlias);\n-            if (linked == null){\n-                return error(Response.Status.BAD_REQUEST, \"Linked Dataset not found.\");\n-            } \n-            if (linking == null){\n-                return error(Response.Status.BAD_REQUEST, \"Linking Dataverse not found.\");\n-            }   \n-            execCommand(new LinkDatasetCommand(\n-                    createDataverseRequest(u), linking, linked\n-                    ));\n-            return ok(\"Dataset \" + linked.getId() + \" linked successfully to \" + linking.getAlias());\n-        } catch (WrappedResponse ex) {\n-            return ex.getResponse();\n+  }\n+\n+  @POST\n+  @Path(\"{id}/move/{targetDataverseAlias}\")\n+  public Response moveDataset(@PathParam(\"id\") String id,\n+                              @PathParam(\"targetDataverseAlias\") String targetDataverseAlias,\n+                              @QueryParam(\"forceMove\") Boolean force) {\n+    try {\n+      User u = findUserOrDie();\n+      Dataset ds = findDatasetOrDie(id);\n+      Dataverse target = dataverseService.findByAlias(targetDataverseAlias);\n+      if (target == null) {\n+        return error(Response.Status.BAD_REQUEST,\n+          BundleUtil.getStringFromBundle(\"datasets.api.moveDataset.error.targetDataverseNotFound\"));\n+      }\n+      //Command requires Super user - it will be tested by the command\n+      execCommand(new MoveDatasetCommand(\n+        createDataverseRequest(u), ds, target, force\n+      ));\n+      return ok(BundleUtil.getStringFromBundle(\"datasets.api.moveDataset.success\"));\n+    } catch (WrappedResponse ex) {\n+      if (ex.getCause() instanceof UnforcedCommandException) {\n+        return ex.refineResponse(BundleUtil.getStringFromBundle(\"datasets.api.moveDataset.error.suggestForce\"));\n+      } else {\n+        return ex.getResponse();\n+      }\n+    }\n+  }\n+\n+  @PUT\n+  @Path(\"{linkedDatasetId}/link/{linkingDataverseAlias}\")\n+  public Response linkDataset(@PathParam(\"linkedDatasetId\") String linkedDatasetId,\n+                              @PathParam(\"linkingDataverseAlias\") String linkingDataverseAlias) {\n+    try {\n+      User u = findUserOrDie();\n+      Dataset linked = findDatasetOrDie(linkedDatasetId);\n+      Dataverse linking = findDataverseOrDie(linkingDataverseAlias);\n+      if (linked == null) {\n+        return error(Response.Status.BAD_REQUEST, \"Linked Dataset not found.\");\n+      }\n+      if (linking == null) {\n+        return error(Response.Status.BAD_REQUEST, \"Linking Dataverse not found.\");\n+      }\n+      execCommand(new LinkDatasetCommand(\n+        createDataverseRequest(u), linking, linked\n+      ));\n+      return ok(\"Dataset \" + linked.getId() + \" linked successfully to \" + linking.getAlias());\n+    } catch (WrappedResponse ex) {\n+      return ex.getResponse();\n+    }\n+  }\n+\n+  @GET\n+  @Path(\"{id}/links\")\n+  public Response getLinks(@PathParam(\"id\") String idSupplied) {\n+    try {\n+      User u = findUserOrDie();\n+      if (!u.isSuperuser()) {\n+        return error(Response.Status.FORBIDDEN, \"Not a superuser\");\n+      }\n+      Dataset dataset = findDatasetOrDie(idSupplied);\n+\n+      long datasetId = dataset.getId();\n+      List<Dataverse> dvsThatLinkToThisDatasetId = dataverseSvc.findDataversesThatLinkToThisDatasetId(datasetId);\n+      JsonArrayBuilder dataversesThatLinkToThisDatasetIdBuilder = Json.createArrayBuilder();\n+      for (Dataverse dataverse : dvsThatLinkToThisDatasetId) {\n+        dataversesThatLinkToThisDatasetIdBuilder.add(dataverse.getAlias() + \" (id \" + dataverse.getId() + \")\");\n+      }\n+      JsonObjectBuilder response = Json.createObjectBuilder();\n+      response.add(\"dataverses that link to dataset id \" + datasetId, dataversesThatLinkToThisDatasetIdBuilder);\n+      return ok(response);\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n+    }\n+  }\n+\n+  /**\n+   * Add a given assignment to a given user or group\n+   *\n+   * @param ra     role assignment DTO\n+   * @param id     dataset id\n+   * @param apiKey\n+   */\n+  @POST\n+  @Path(\"{identifier}/assignments\")\n+  public Response createAssignment(RoleAssignmentDTO ra, @PathParam(\"identifier\") String id,\n+                                   @QueryParam(\"key\") String apiKey) {\n+    try {\n+      Dataset dataset = findDatasetOrDie(id);\n+\n+      RoleAssignee assignee = findAssignee(ra.getAssignee());\n+      if (assignee == null) {\n+        return error(Response.Status.BAD_REQUEST,\n+          BundleUtil.getStringFromBundle(\"datasets.api.grant.role.assignee.not.found.error\"));\n+      }\n+\n+      DataverseRole theRole;\n+      Dataverse dv = dataset.getOwner();\n+      theRole = null;\n+      while ((theRole == null) && (dv != null)) {\n+        for (DataverseRole aRole : rolesSvc.availableRoles(dv.getId())) {\n+          if (aRole.getAlias().equals(ra.getRole())) {\n+            theRole = aRole;\n+            break;\n+          }\n         }\n+        dv = dv.getOwner();\n+      }\n+      if (theRole == null) {\n+        List<String> args = Arrays.asList(ra.getRole(), dataset.getOwner().getDisplayName());\n+        return error(Status.BAD_REQUEST,\n+          BundleUtil.getStringFromBundle(\"datasets.api.grant.role.not.found.error\", args));\n+      }\n+\n+      String privateUrlToken = null;\n+      return ok(\n+        json(execCommand(new AssignRoleCommand(assignee, theRole, dataset, createDataverseRequest(findUserOrDie()),\n+          privateUrlToken))));\n+    } catch (WrappedResponse ex) {\n+      List<String> args = Arrays.asList(ex.getMessage());\n+      logger.log(Level.WARNING,\n+        BundleUtil.getStringFromBundle(\"datasets.api.grant.role.cant.create.assignment.error\", args));\n+      return ex.getResponse();\n     }\n-    \n-    @GET\n-    @Path(\"{id}/links\")\n-    public Response getLinks(@PathParam(\"id\") String idSupplied ) {\n-        try {\n-            User u = findUserOrDie();\n-            if (!u.isSuperuser()) {\n-                return error(Response.Status.FORBIDDEN, \"Not a superuser\");\n-            }\n-            Dataset dataset = findDatasetOrDie(idSupplied);\n \n-            long datasetId = dataset.getId();\n-            List<Dataverse> dvsThatLinkToThisDatasetId = dataverseSvc.findDataversesThatLinkToThisDatasetId(datasetId);\n-            JsonArrayBuilder dataversesThatLinkToThisDatasetIdBuilder = Json.createArrayBuilder();\n-            for (Dataverse dataverse : dvsThatLinkToThisDatasetId) {\n-                dataversesThatLinkToThisDatasetIdBuilder.add(dataverse.getAlias() + \" (id \" + dataverse.getId() + \")\");\n-            }\n-            JsonObjectBuilder response = Json.createObjectBuilder();\n-            response.add(\"dataverses that link to dataset id \" + datasetId, dataversesThatLinkToThisDatasetIdBuilder);\n-            return ok(response);\n-        } catch (WrappedResponse wr) {\n-            return wr.getResponse();\n+  }\n+\n+  @DELETE\n+  @Path(\"{identifier}/assignments/{id}\")\n+  public Response deleteAssignment(@PathParam(\"id\") long assignmentId, @PathParam(\"identifier\") String dsId) {\n+    RoleAssignment ra = em.find(RoleAssignment.class, assignmentId);\n+    if (ra != null) {\n+      try {\n+        findDatasetOrDie(dsId);\n+        execCommand(new RevokeRoleCommand(ra, createDataverseRequest(findUserOrDie())));\n+        List<String> args = Arrays.asList(ra.getRole().getName(), ra.getAssigneeIdentifier(),\n+          ra.getDefinitionPoint().accept(DvObject.NamePrinter));\n+        return ok(BundleUtil.getStringFromBundle(\"datasets.api.revoke.role.success\", args));\n+      } catch (WrappedResponse ex) {\n+        return ex.getResponse();\n+      }\n+    } else {\n+      List<String> args = Arrays.asList(Long.toString(assignmentId));\n+      return error(Status.NOT_FOUND, BundleUtil.getStringFromBundle(\"datasets.api.revoke.role.not.found.error\", args));\n+    }\n+  }\n+\n+  @GET\n+  @Path(\"{identifier}/assignments\")\n+  public Response getAssignments(@PathParam(\"identifier\") String id) {\n+    return response(req ->\n+      ok(execCommand(\n+        new ListRoleAssignments(req, findDatasetOrDie(id)))\n+        .stream().map(ra -> json(ra)).collect(toJsonArray())));\n+  }\n+\n+  @GET\n+  @Path(\"{id}/privateUrl\")\n+  public Response getPrivateUrlData(@PathParam(\"id\") String idSupplied) {\n+    return response(req -> {\n+      PrivateUrl privateUrl = execCommand(new GetPrivateUrlCommand(req, findDatasetOrDie(idSupplied)));\n+      return (privateUrl != null) ? ok(json(privateUrl))\n+        : error(Response.Status.NOT_FOUND, \"Private URL not found.\");\n+    });\n+  }\n+\n+  @POST\n+  @Path(\"{id}/privateUrl\")\n+  public Response createPrivateUrl(@PathParam(\"id\") String idSupplied) {\n+    return response(req ->\n+      ok(json(execCommand(\n+        new CreatePrivateUrlCommand(req, findDatasetOrDie(idSupplied))))));\n+  }\n+\n+  @DELETE\n+  @Path(\"{id}/privateUrl\")\n+  public Response deletePrivateUrl(@PathParam(\"id\") String idSupplied) {\n+    return response(req -> {\n+      Dataset dataset = findDatasetOrDie(idSupplied);\n+      PrivateUrl privateUrl = execCommand(new GetPrivateUrlCommand(req, dataset));\n+      if (privateUrl != null) {\n+        execCommand(new DeletePrivateUrlCommand(req, dataset));\n+        return ok(\"Private URL deleted.\");\n+      } else {\n+        return notFound(\"No Private URL to delete.\");\n+      }\n+    });\n+  }\n+\n+  @GET\n+  @Path(\"{id}/thumbnail/candidates\")\n+  public Response getDatasetThumbnailCandidates(@PathParam(\"id\") String idSupplied) {\n+    try {\n+      Dataset dataset = findDatasetOrDie(idSupplied);\n+      boolean canUpdateThumbnail = false;\n+      try {\n+        canUpdateThumbnail = permissionSvc.requestOn(createDataverseRequest(findUserOrDie()), dataset)\n+          .canIssue(UpdateDatasetThumbnailCommand.class);\n+      } catch (WrappedResponse ex) {\n+        logger.info(\"Exception thrown while trying to figure out permissions while getting thumbnail for dataset id \" +\n+          dataset.getId() + \": \" + ex.getLocalizedMessage());\n+      }\n+      if (!canUpdateThumbnail) {\n+        return error(Response.Status.FORBIDDEN, \"You are not permitted to list dataset thumbnail candidates.\");\n+      }\n+      JsonArrayBuilder data = Json.createArrayBuilder();\n+      boolean considerDatasetLogoAsCandidate = true;\n+      for (DatasetThumbnail datasetThumbnail : DatasetUtil\n+        .getThumbnailCandidates(dataset, considerDatasetLogoAsCandidate, ImageThumbConverter.DEFAULT_CARDIMAGE_SIZE)) {\n+        JsonObjectBuilder candidate = Json.createObjectBuilder();\n+        String base64image = datasetThumbnail.getBase64image();\n+        if (base64image != null) {\n+          logger.fine(\"found a candidate!\");\n+          candidate.add(\"base64image\", base64image);\n         }\n+        DataFile dataFile = datasetThumbnail.getDataFile();\n+        if (dataFile != null) {\n+          candidate.add(\"dataFileId\", dataFile.getId());\n+        }\n+        data.add(candidate);\n+      }\n+      return ok(data);\n+    } catch (WrappedResponse ex) {\n+      return error(Response.Status.NOT_FOUND, \"Could not find dataset based on id supplied: \" + idSupplied + \".\");\n     }\n-\n-    /**\n-     * Add a given assignment to a given user or group\n-     * @param ra role assignment DTO\n-     * @param id dataset id\n-     * @param apiKey\n-     */\n-    @POST\n-    @Path(\"{identifier}/assignments\")\n-    public Response createAssignment(RoleAssignmentDTO ra, @PathParam(\"identifier\") String id, @QueryParam(\"key\") String apiKey) {\n-        try {\n-            Dataset dataset = findDatasetOrDie(id);\n-            \n-            RoleAssignee assignee = findAssignee(ra.getAssignee());\n-            if (assignee == null) {\n-                return error(Response.Status.BAD_REQUEST, BundleUtil.getStringFromBundle(\"datasets.api.grant.role.assignee.not.found.error\"));\n-            }           \n-            \n-            DataverseRole theRole;\n-            Dataverse dv = dataset.getOwner();\n-            theRole = null;\n-            while ((theRole == null) && (dv != null)) {\n-                for (DataverseRole aRole : rolesSvc.availableRoles(dv.getId())) {\n-                    if (aRole.getAlias().equals(ra.getRole())) {\n-                        theRole = aRole;\n-                        break;\n-                    }\n-                }\n-                dv = dv.getOwner();\n+  }\n+\n+  @GET\n+  @Produces({\"image/png\"})\n+  @Path(\"{id}/thumbnail\")\n+  public Response getDatasetThumbnail(@PathParam(\"id\") String idSupplied) {\n+    try {\n+      Dataset dataset = findDatasetOrDie(idSupplied);\n+      InputStream is = DatasetUtil.getThumbnailAsInputStream(dataset, ImageThumbConverter.DEFAULT_CARDIMAGE_SIZE);\n+      if (is == null) {\n+        return notFound(\"Thumbnail not available\");\n+      }\n+      return Response.ok(is).build();\n+    } catch (WrappedResponse wr) {\n+      return notFound(\"Thumbnail not available\");\n+    }\n+  }\n+\n+  // TODO: Rather than only supporting looking up files by their database IDs (dataFileIdSupplied), consider supporting persistent identifiers.\n+  @POST\n+  @Path(\"{id}/thumbnail/{dataFileId}\")\n+  public Response setDataFileAsThumbnail(@PathParam(\"id\") String idSupplied,\n+                                         @PathParam(\"dataFileId\") long dataFileIdSupplied) {\n+    try {\n+      DatasetThumbnail datasetThumbnail = execCommand(\n+        new UpdateDatasetThumbnailCommand(createDataverseRequest(findUserOrDie()), findDatasetOrDie(idSupplied),\n+          UpdateDatasetThumbnailCommand.UserIntent.setDatasetFileAsThumbnail, dataFileIdSupplied, null));\n+      return ok(\"Thumbnail set to \" + datasetThumbnail.getBase64image());\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n+    }\n+  }\n+\n+  @POST\n+  @Path(\"{id}/thumbnail\")\n+  @Consumes(MediaType.MULTIPART_FORM_DATA)\n+  public Response uploadDatasetLogo(@PathParam(\"id\") String idSupplied, @FormDataParam(\"file\") InputStream inputStream\n+  ) {\n+    try {\n+      DatasetThumbnail datasetThumbnail = execCommand(\n+        new UpdateDatasetThumbnailCommand(createDataverseRequest(findUserOrDie()), findDatasetOrDie(idSupplied),\n+          UpdateDatasetThumbnailCommand.UserIntent.setNonDatasetFileAsThumbnail, null, inputStream));\n+      return ok(\"Thumbnail is now \" + datasetThumbnail.getBase64image());\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n+    }\n+  }\n+\n+  @DELETE\n+  @Path(\"{id}/thumbnail\")\n+  public Response removeDatasetLogo(@PathParam(\"id\") String idSupplied) {\n+    try {\n+      DatasetThumbnail datasetThumbnail = execCommand(\n+        new UpdateDatasetThumbnailCommand(createDataverseRequest(findUserOrDie()), findDatasetOrDie(idSupplied),\n+          UpdateDatasetThumbnailCommand.UserIntent.removeThumbnail, null, null));\n+      return ok(\"Dataset thumbnail removed.\");\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n+    }\n+  }\n+\n+  @GET\n+  @Path(\"{identifier}/dataCaptureModule/rsync\")\n+  public Response getRsync(@PathParam(\"identifier\") String id) {\n+    //TODO - does it make sense to switch this to dataset identifier for consistency with the rest of the DCM APIs?\n+    if (!DataCaptureModuleUtil.rsyncSupportEnabled(settingsSvc.getValueForKey(SettingsServiceBean.Key.UploadMethods))) {\n+      return error(Response.Status.METHOD_NOT_ALLOWED,\n+        SettingsServiceBean.Key.UploadMethods + \" does not contain \" + SystemConfig.FileUploadMethods.RSYNC + \".\");\n+    }\n+    Dataset dataset = null;\n+    try {\n+      dataset = findDatasetOrDie(id);\n+      AuthenticatedUser user = findAuthenticatedUserOrDie();\n+      ScriptRequestResponse scriptRequestResponse =\n+        execCommand(new RequestRsyncScriptCommand(createDataverseRequest(user), dataset));\n+\n+      DatasetLock lock =\n+        datasetService.addDatasetLock(dataset.getId(), DatasetLock.Reason.DcmUpload, user.getId(), \"script downloaded\");\n+      if (lock == null) {\n+        logger.log(Level.WARNING, \"Failed to lock the dataset (dataset id={0})\", dataset.getId());\n+        return error(Response.Status.FORBIDDEN, \"Failed to lock the dataset (dataset id=\" + dataset.getId() + \")\");\n+      }\n+      return ok(scriptRequestResponse.getScript(), MediaType.valueOf(MediaType.TEXT_PLAIN), null);\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n+    } catch (EJBException ex) {\n+      return error(Response.Status.INTERNAL_SERVER_ERROR,\n+        \"Something went wrong attempting to download rsync script: \" + EjbUtil.ejbExceptionToString(ex));\n+    }\n+  }\n+\n+  /**\n+   * This api endpoint triggers the creation of a \"package\" file in a dataset\n+   * after that package has been moved onto the same filesystem via the Data Capture Module.\n+   * The package is really just a way that Dataverse interprets a folder created by DCM, seeing it as just one file.\n+   * The \"package\" can be downloaded over RSAL.\n+   * <p>\n+   * This endpoint currently supports both posix file storage and AWS s3 storage in Dataverse, and depending on which one is active acts accordingly.\n+   * <p>\n+   * The initial design of the DCM/Dataverse interaction was not to use packages, but to allow import of all individual files natively into Dataverse.\n+   * But due to the possibly immense number of files (millions) the package approach was taken.\n+   * This is relevant because the posix (\"file\") code contains many remnants of that development work.\n+   * The s3 code was written later and is set to only support import as packages. It takes a lot from FileRecordWriter.\n+   * -MAD 4.9.1\n+   */\n+  @POST\n+  @Path(\"{identifier}/dataCaptureModule/checksumValidation\")\n+  public Response receiveChecksumValidationResults(@PathParam(\"identifier\") String id, JsonObject jsonFromDcm) {\n+    logger.log(Level.FINE, \"jsonFromDcm: {0}\", jsonFromDcm);\n+    AuthenticatedUser authenticatedUser = null;\n+    try {\n+      authenticatedUser = findAuthenticatedUserOrDie();\n+    } catch (WrappedResponse ex) {\n+      return error(Response.Status.BAD_REQUEST, \"Authentication is required.\");\n+    }\n+    if (!authenticatedUser.isSuperuser()) {\n+      return error(Response.Status.FORBIDDEN, \"Superusers only.\");\n+    }\n+    String statusMessageFromDcm = jsonFromDcm.getString(\"status\");\n+    try {\n+      Dataset dataset = findDatasetOrDie(id);\n+      if (\"validation passed\".equals(statusMessageFromDcm)) {\n+        logger.log(Level.INFO, \"Checksum Validation passed for DCM.\");\n+\n+        String storageDriver = dataset.getDataverseContext().getEffectiveStorageDriverId();\n+        String uploadFolder = jsonFromDcm.getString(\"uploadFolder\");\n+        int totalSize = jsonFromDcm.getInt(\"totalSize\");\n+        String storageDriverType = System.getProperty(\"dataverse.file.\" + storageDriver + \".type\");\n+\n+        if (storageDriverType.equals(\"file\")) {\n+          logger.log(Level.INFO, \"File storage driver used for (dataset id={0})\", dataset.getId());\n+\n+          ImportMode importMode = ImportMode.MERGE;\n+          try {\n+            JsonObject jsonFromImportJobKickoff = execCommand(\n+              new ImportFromFileSystemCommand(createDataverseRequest(findUserOrDie()), dataset, uploadFolder,\n+                new Long(totalSize), importMode));\n+            long jobId = jsonFromImportJobKickoff.getInt(\"executionId\");\n+            String message = jsonFromImportJobKickoff.getString(\"message\");\n+            JsonObjectBuilder job = Json.createObjectBuilder();\n+            job.add(\"jobId\", jobId);\n+            job.add(\"message\", message);\n+            return ok(job);\n+          } catch (WrappedResponse wr) {\n+            String message = wr.getMessage();\n+            return error(Response.Status.INTERNAL_SERVER_ERROR,\n+              \"Uploaded files have passed checksum validation but something went wrong while attempting to put the files into Dataverse. Message was '\" +\n+                message + \"'.\");\n+          }\n+        } else if (storageDriverType.equals(\"s3\")) {\n+\n+          logger.log(Level.INFO, \"S3 storage driver used for DCM (dataset id={0})\", dataset.getId());\n+          try {\n+\n+            //Where the lifting is actually done, moving the s3 files over and having dataverse know of the existance of the package\n+            s3PackageImporter.copyFromS3(dataset, uploadFolder);\n+            DataFile packageFile = s3PackageImporter.createPackageDataFile(dataset, uploadFolder, new Long(totalSize));\n+\n+            if (packageFile == null) {\n+              logger.log(Level.SEVERE, \"S3 File package import failed.\");\n+              return error(Response.Status.INTERNAL_SERVER_ERROR, \"S3 File package import failed.\");\n             }\n-            if (theRole == null) {\n-                List<String> args = Arrays.asList(ra.getRole(), dataset.getOwner().getDisplayName());\n-                return error(Status.BAD_REQUEST, BundleUtil.getStringFromBundle(\"datasets.api.grant.role.not.found.error\", args));\n+            DatasetLock dcmLock = dataset.getLockFor(DatasetLock.Reason.DcmUpload);\n+            if (dcmLock == null) {\n+              logger.log(Level.WARNING, \"Dataset not locked for DCM upload\");\n+            } else {\n+              datasetService.removeDatasetLocks(dataset, DatasetLock.Reason.DcmUpload);\n+              dataset.removeLock(dcmLock);\n             }\n \n-            String privateUrlToken = null;\n-            return ok(\n-                    json(execCommand(new AssignRoleCommand(assignee, theRole, dataset, createDataverseRequest(findUserOrDie()), privateUrlToken))));\n-        } catch (WrappedResponse ex) {\n-            List<String> args = Arrays.asList(ex.getMessage());\n-            logger.log(Level.WARNING, BundleUtil.getStringFromBundle(\"datasets.api.grant.role.cant.create.assignment.error\", args));\n-            return ex.getResponse();\n-        }\n-\n-    }\n-    \n-    @DELETE\n-    @Path(\"{identifier}/assignments/{id}\")\n-    public Response deleteAssignment(@PathParam(\"id\") long assignmentId, @PathParam(\"identifier\") String dsId) {\n-        RoleAssignment ra = em.find(RoleAssignment.class, assignmentId);\n-        if (ra != null) {\n-            try {\n-                findDatasetOrDie(dsId);\n-                execCommand(new RevokeRoleCommand(ra, createDataverseRequest(findUserOrDie())));\n-                List<String> args = Arrays.asList(ra.getRole().getName(), ra.getAssigneeIdentifier(), ra.getDefinitionPoint().accept(DvObject.NamePrinter));\n-                return ok(BundleUtil.getStringFromBundle(\"datasets.api.revoke.role.success\", args));\n-            } catch (WrappedResponse ex) {\n-                return ex.getResponse();\n+            // update version using the command engine to enforce user permissions and constraints\n+            if (dataset.getVersions().size() == 1 &&\n+              dataset.getLatestVersion().getVersionState() == DatasetVersion.VersionState.DRAFT) {\n+              try {\n+                Command<Dataset> cmd;\n+                cmd = new UpdateDatasetVersionCommand(dataset,\n+                  new DataverseRequest(authenticatedUser, (HttpServletRequest) null));\n+                commandEngine.submit(cmd);\n+              } catch (CommandException ex) {\n+                return error(Response.Status.INTERNAL_SERVER_ERROR,\n+                  \"CommandException updating DatasetVersion from batch job: \" + ex.getMessage());\n+              }\n+            } else {\n+              String constraintError = \"ConstraintException updating DatasetVersion form batch job: dataset must be a \"\n+                + \"single version in draft mode.\";\n+              logger.log(Level.SEVERE, constraintError);\n             }\n+\n+            JsonObjectBuilder job = Json.createObjectBuilder();\n+            return ok(job);\n+\n+          } catch (IOException e) {\n+            String message = e.getMessage();\n+            return error(Response.Status.INTERNAL_SERVER_ERROR,\n+              \"Uploaded files have passed checksum validation but something went wrong while attempting to move the files into Dataverse. Message was '\" +\n+                message + \"'.\");\n+          }\n         } else {\n-            List<String> args = Arrays.asList(Long.toString(assignmentId));\n-            return error(Status.NOT_FOUND, BundleUtil.getStringFromBundle(\"datasets.api.revoke.role.not.found.error\", args));\n+          return error(Response.Status.INTERNAL_SERVER_ERROR,\n+            \"Invalid storage driver in Dataverse, not compatible with dcm\");\n         }\n+      } else if (\"validation failed\".equals(statusMessageFromDcm)) {\n+        Map<String, AuthenticatedUser> distinctAuthors =\n+          permissionService.getDistinctUsersWithPermissionOn(Permission.EditDataset, dataset);\n+        distinctAuthors.values().forEach((value) -> {\n+          userNotificationService.sendNotification((AuthenticatedUser) value, new Timestamp(new Date().getTime()),\n+            UserNotification.Type.CHECKSUMFAIL, dataset.getId());\n+        });\n+        List<AuthenticatedUser> superUsers = authenticationServiceBean.findSuperUsers();\n+        if (superUsers != null && !superUsers.isEmpty()) {\n+          superUsers.forEach((au) -> {\n+            userNotificationService\n+              .sendNotification(au, new Timestamp(new Date().getTime()), UserNotification.Type.CHECKSUMFAIL,\n+                dataset.getId());\n+          });\n+        }\n+        return ok(\"User notified about checksum validation failure.\");\n+      } else {\n+        return error(Response.Status.BAD_REQUEST, \"Unexpected status cannot be processed: \" + statusMessageFromDcm);\n+      }\n+    } catch (WrappedResponse ex) {\n+      return ex.getResponse();\n     }\n+  }\n \n-    @GET\n-    @Path(\"{identifier}/assignments\")\n-    public Response getAssignments(@PathParam(\"identifier\") String id) {\n-        return response( req -> \n-            ok( execCommand(\n-                       new ListRoleAssignments(req, findDatasetOrDie(id)))\n-                     .stream().map(ra->json(ra)).collect(toJsonArray())) );\n-    }\n \n-    @GET\n-    @Path(\"{id}/privateUrl\")\n-    public Response getPrivateUrlData(@PathParam(\"id\") String idSupplied) {\n-        return response( req -> {\n-            PrivateUrl privateUrl = execCommand(new GetPrivateUrlCommand(req, findDatasetOrDie(idSupplied)));\n-            return (privateUrl != null) ? ok(json(privateUrl)) \n-                                        : error(Response.Status.NOT_FOUND, \"Private URL not found.\");\n-        });\n-    }\n+  @POST\n+  @Path(\"{id}/submitForReview\")\n+  public Response submitForReview(@PathParam(\"id\") String idSupplied) {\n+    try {\n+      Dataset updatedDataset = execCommand(\n+        new SubmitDatasetForReviewCommand(createDataverseRequest(findUserOrDie()), findDatasetOrDie(idSupplied)));\n+      JsonObjectBuilder result = Json.createObjectBuilder();\n \n-    @POST\n-    @Path(\"{id}/privateUrl\")\n-    public Response createPrivateUrl(@PathParam(\"id\") String idSupplied) {\n-        return response( req -> \n-                ok(json(execCommand(\n-                        new CreatePrivateUrlCommand(req, findDatasetOrDie(idSupplied))))));\n-    }\n+      boolean inReview = updatedDataset.isLockedFor(DatasetLock.Reason.InReview);\n \n-    @DELETE\n-    @Path(\"{id}/privateUrl\")\n-    public Response deletePrivateUrl(@PathParam(\"id\") String idSupplied) {\n-        return response( req -> {\n-            Dataset dataset = findDatasetOrDie(idSupplied);\n-            PrivateUrl privateUrl = execCommand(new GetPrivateUrlCommand(req, dataset));\n-            if (privateUrl != null) {\n-                execCommand(new DeletePrivateUrlCommand(req, dataset));\n-                return ok(\"Private URL deleted.\");\n-            } else {\n-                return notFound(\"No Private URL to delete.\");\n-            }\n-        });\n+      result.add(\"inReview\", inReview);\n+      result.add(\"message\", \"Dataset id \" + updatedDataset.getId() + \" has been submitted for review.\");\n+      return ok(result);\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n     }\n+  }\n \n-    @GET\n-    @Path(\"{id}/thumbnail/candidates\")\n-    public Response getDatasetThumbnailCandidates(@PathParam(\"id\") String idSupplied) {\n+  @POST\n+  @Path(\"{id}/returnToAuthor\")\n+  public Response returnToAuthor(@PathParam(\"id\") String idSupplied, String jsonBody) {\n+\n+    if (jsonBody == null || jsonBody.isEmpty()) {\n+      return error(Response.Status.BAD_REQUEST,\n+        \"You must supply JSON to this API endpoint and it must contain a reason for returning the dataset (field: reasonForReturn).\");\n+    }\n+    StringReader rdr = new StringReader(jsonBody);\n+    JsonObject json = Json.createReader(rdr).readObject();\n+    try {\n+      Dataset dataset = findDatasetOrDie(idSupplied);\n+      String reasonForReturn = null;\n+      reasonForReturn = json.getString(\"reasonForReturn\");\n+      // TODO: Once we add a box for the curator to type into, pass the reason for return to the ReturnDatasetToAuthorCommand and delete this check and call to setReturnReason on the API side.\n+      if (reasonForReturn == null || reasonForReturn.isEmpty()) {\n+        return error(Response.Status.BAD_REQUEST, \"You must enter a reason for returning a dataset to the author(s).\");\n+      }\n+      AuthenticatedUser authenticatedUser = findAuthenticatedUserOrDie();\n+      Dataset updatedDataset = execCommand(\n+        new ReturnDatasetToAuthorCommand(createDataverseRequest(authenticatedUser), dataset, reasonForReturn));\n+\n+      JsonObjectBuilder result = Json.createObjectBuilder();\n+      result.add(\"inReview\", false);\n+      result.add(\"message\", \"Dataset id \" + updatedDataset.getId() + \" has been sent back to the author(s).\");\n+      return ok(result);\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n+    }\n+  }\n+\n+  @GET\n+  @Path(\"{id}/uploadsid\")\n+  @Deprecated\n+  public Response getUploadUrl(@PathParam(\"id\") String idSupplied) {\n+    try {\n+      Dataset dataset = findDatasetOrDie(idSupplied);\n+\n+      boolean canUpdateDataset = false;\n+      try {\n+        canUpdateDataset = permissionSvc.requestOn(createDataverseRequest(findUserOrDie()), dataset)\n+          .canIssue(UpdateDatasetVersionCommand.class);\n+      } catch (WrappedResponse ex) {\n+        logger.info(\"Exception thrown while trying to figure out permissions while getting upload URL for dataset id \" +\n+          dataset.getId() + \": \" + ex.getLocalizedMessage());\n+        throw ex;\n+      }\n+      if (!canUpdateDataset) {\n+        return error(Response.Status.FORBIDDEN, \"You are not permitted to upload files to this dataset.\");\n+      }\n+      S3AccessIO<?> s3io = FileUtil.getS3AccessForDirectUpload(dataset);\n+      if (s3io == null) {\n+        return error(Response.Status.NOT_FOUND,\n+          \"Direct upload not supported for files in this dataset: \" + dataset.getId());\n+      }\n+      String url = null;\n+      String storageIdentifier = null;\n+      try {\n+        url = s3io.generateTemporaryS3UploadUrl();\n+        storageIdentifier = FileUtil.getStorageIdentifierFromLocation(s3io.getStorageLocation());\n+      } catch (IOException io) {\n+        logger.warning(io.getMessage());\n+        throw new WrappedResponse(io,\n+          error(Response.Status.INTERNAL_SERVER_ERROR, \"Could not create process direct upload request\"));\n+      }\n+\n+      JsonObjectBuilder response = Json.createObjectBuilder()\n+        .add(\"url\", url)\n+        .add(\"storageIdentifier\", storageIdentifier);\n+      return ok(response);\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n+    }\n+  }\n+\n+  @GET\n+  @Path(\"{id}/uploadurls\")\n+  public Response getMPUploadUrls(@PathParam(\"id\") String idSupplied, @QueryParam(\"size\") long fileSize) {\n+    try {\n+      Dataset dataset = findDatasetOrDie(idSupplied);\n+\n+      boolean canUpdateDataset = false;\n+      try {\n+        canUpdateDataset = permissionSvc.requestOn(createDataverseRequest(findUserOrDie()), dataset)\n+          .canIssue(UpdateDatasetVersionCommand.class);\n+      } catch (WrappedResponse ex) {\n+        logger.info(\n+          \"Exception thrown while trying to figure out permissions while getting upload URLs for dataset id \"\n+            + dataset.getId() + \": \" + ex.getLocalizedMessage());\n+        throw ex;\n+      }\n+      if (!canUpdateDataset) {\n+        return error(Response.Status.FORBIDDEN, \"You are not permitted to upload files to this dataset.\");\n+      }\n+      S3AccessIO<DataFile> s3io = FileUtil.getS3AccessForDirectUpload(dataset);\n+      if (s3io == null) {\n+        return error(Response.Status.NOT_FOUND,\n+          \"Direct upload not supported for files in this dataset: \" + dataset.getId());\n+      }\n+      JsonObjectBuilder response = null;\n+      String storageIdentifier = null;\n+      try {\n+        storageIdentifier = FileUtil.getStorageIdentifierFromLocation(s3io.getStorageLocation());\n+        response = s3io.generateTemporaryS3UploadUrls(dataset.getGlobalId().asString(), storageIdentifier, fileSize);\n+\n+      } catch (IOException io) {\n+        logger.warning(io.getMessage());\n+        throw new WrappedResponse(io,\n+          error(Response.Status.INTERNAL_SERVER_ERROR, \"Could not create process direct upload request\"));\n+      }\n+\n+      response.add(\"storageIdentifier\", storageIdentifier);\n+      return ok(response);\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n+    }\n+  }\n+\n+  @DELETE\n+  @Path(\"mpupload\")\n+  public Response abortMPUpload(@QueryParam(\"globalid\") String idSupplied,\n+                                @QueryParam(\"storageidentifier\") String storageidentifier,\n+                                @QueryParam(\"uploadid\") String uploadId) {\n+    try {\n+      Dataset dataset = datasetSvc.findByGlobalId(idSupplied);\n+      //Allow the API to be used within a session (e.g. for direct upload in the UI)\n+      User user = session.getUser();\n+      if (!user.isAuthenticated()) {\n         try {\n-            Dataset dataset = findDatasetOrDie(idSupplied);\n-            boolean canUpdateThumbnail = false;\n-            try {\n-                canUpdateThumbnail = permissionSvc.requestOn(createDataverseRequest(findUserOrDie()), dataset).canIssue(UpdateDatasetThumbnailCommand.class);\n-            } catch (WrappedResponse ex) {\n-                logger.info(\"Exception thrown while trying to figure out permissions while getting thumbnail for dataset id \" + dataset.getId() + \": \" + ex.getLocalizedMessage());\n-            }\n-            if (!canUpdateThumbnail) {\n-                return error(Response.Status.FORBIDDEN, \"You are not permitted to list dataset thumbnail candidates.\");\n-            }\n-            JsonArrayBuilder data = Json.createArrayBuilder();\n-            boolean considerDatasetLogoAsCandidate = true;\n-            for (DatasetThumbnail datasetThumbnail : DatasetUtil.getThumbnailCandidates(dataset, considerDatasetLogoAsCandidate, ImageThumbConverter.DEFAULT_CARDIMAGE_SIZE)) {\n-                JsonObjectBuilder candidate = Json.createObjectBuilder();\n-                String base64image = datasetThumbnail.getBase64image();\n-                if (base64image != null) {\n-                    logger.fine(\"found a candidate!\");\n-                    candidate.add(\"base64image\", base64image);\n-                }\n-                DataFile dataFile = datasetThumbnail.getDataFile();\n-                if (dataFile != null) {\n-                    candidate.add(\"dataFileId\", dataFile.getId());\n-                }\n-                data.add(candidate);\n-            }\n-            return ok(data);\n+          user = findAuthenticatedUserOrDie();\n         } catch (WrappedResponse ex) {\n-            return error(Response.Status.NOT_FOUND, \"Could not find dataset based on id supplied: \" + idSupplied + \".\");\n+          logger.info(\n+            \"Exception thrown while trying to figure out permissions while getting aborting upload for dataset id \"\n+              + dataset.getId() + \": \" + ex.getLocalizedMessage());\n+          throw ex;\n         }\n+      }\n+      boolean allowed = false;\n+      if (dataset != null) {\n+        allowed = permissionSvc.requestOn(createDataverseRequest(user), dataset)\n+          .canIssue(UpdateDatasetVersionCommand.class);\n+      } else {\n+        /*\n+         * The only legitimate case where a global id won't correspond to a dataset is\n+         * for uploads during creation. Given that this call will still fail unless all\n+         * three parameters correspond to an active multipart upload, it should be safe\n+         * to allow the attempt for an authenticated user. If there are concerns about\n+         * permissions, one could check with the current design that the user is allowed\n+         * to create datasets in some dataverse that is configured to use the storage\n+         * provider specified in the storageidentifier, but testing for the ability to\n+         * create a dataset in a specific dataverse would requiring changing the design\n+         * somehow (e.g. adding the ownerId to this call).\n+         */\n+        allowed = true;\n+      }\n+      if (!allowed) {\n+        return error(Response.Status.FORBIDDEN,\n+          \"You are not permitted to abort file uploads with the supplied parameters.\");\n+      }\n+      try {\n+        S3AccessIO.abortMultipartUpload(idSupplied, storageidentifier, uploadId);\n+      } catch (IOException io) {\n+        logger.warning(\"Multipart upload abort failed for uploadId: \" + uploadId + \" storageidentifier=\"\n+          + storageidentifier + \" dataset Id: \" + dataset.getId());\n+        logger.warning(io.getMessage());\n+        throw new WrappedResponse(io,\n+          error(Response.Status.INTERNAL_SERVER_ERROR, \"Could not abort multipart upload\"));\n+      }\n+      return Response.noContent().build();\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n     }\n-\n-    @GET\n-    @Produces({\"image/png\"})\n-    @Path(\"{id}/thumbnail\")\n-    public Response getDatasetThumbnail(@PathParam(\"id\") String idSupplied) {\n+  }\n+\n+  @PUT\n+  @Path(\"mpupload\")\n+  public Response completeMPUpload(String partETagBody, @QueryParam(\"globalid\") String idSupplied,\n+                                   @QueryParam(\"storageidentifier\") String storageidentifier,\n+                                   @QueryParam(\"uploadid\") String uploadId) {\n+    try {\n+      Dataset dataset = datasetSvc.findByGlobalId(idSupplied);\n+      //Allow the API to be used within a session (e.g. for direct upload in the UI)\n+      User user = session.getUser();\n+      if (!user.isAuthenticated()) {\n         try {\n-            Dataset dataset = findDatasetOrDie(idSupplied);\n-            InputStream is = DatasetUtil.getThumbnailAsInputStream(dataset, ImageThumbConverter.DEFAULT_CARDIMAGE_SIZE);\n-            if(is == null) {\n-                return notFound(\"Thumbnail not available\");\n-            }\n-            return Response.ok(is).build();\n-        } catch (WrappedResponse wr) {\n-            return notFound(\"Thumbnail not available\");\n+          user = findAuthenticatedUserOrDie();\n+        } catch (WrappedResponse ex) {\n+          logger.info(\n+            \"Exception thrown while trying to figure out permissions to complete mpupload for dataset id \"\n+              + dataset.getId() + \": \" + ex.getLocalizedMessage());\n+          throw ex;\n         }\n-    }\n-\n-    // TODO: Rather than only supporting looking up files by their database IDs (dataFileIdSupplied), consider supporting persistent identifiers.\n-    @POST\n-    @Path(\"{id}/thumbnail/{dataFileId}\")\n-    public Response setDataFileAsThumbnail(@PathParam(\"id\") String idSupplied, @PathParam(\"dataFileId\") long dataFileIdSupplied) {\n+      }\n+      boolean allowed = false;\n+      if (dataset != null) {\n+        allowed = permissionSvc.requestOn(createDataverseRequest(user), dataset)\n+          .canIssue(UpdateDatasetVersionCommand.class);\n+      } else {\n+        /*\n+         * The only legitimate case where a global id won't correspond to a dataset is\n+         * for uploads during creation. Given that this call will still fail unless all\n+         * three parameters correspond to an active multipart upload, it should be safe\n+         * to allow the attempt for an authenticated user. If there are concerns about\n+         * permissions, one could check with the current design that the user is allowed\n+         * to create datasets in some dataverse that is configured to use the storage\n+         * provider specified in the storageidentifier, but testing for the ability to\n+         * create a dataset in a specific dataverse would requiring changing the design\n+         * somehow (e.g. adding the ownerId to this call).\n+         */\n+        allowed = true;\n+      }\n+      if (!allowed) {\n+        return error(Response.Status.FORBIDDEN,\n+          \"You are not permitted to complete file uploads with the supplied parameters.\");\n+      }\n+      List<PartETag> eTagList = new ArrayList<PartETag>();\n+      logger.info(\"Etags: \" + partETagBody);\n+      try {\n+        JsonReader jsonReader = Json.createReader(new StringReader(partETagBody));\n+        JsonObject object = jsonReader.readObject();\n+        jsonReader.close();\n+        for (String partNo : object.keySet()) {\n+          eTagList.add(new PartETag(Integer.parseInt(partNo), object.getString(partNo)));\n+        }\n+        for (PartETag et : eTagList) {\n+          logger.info(\"Part: \" + et.getPartNumber() + \" : \" + et.getETag());\n+        }\n+      } catch (JsonException je) {\n+        logger.info(\"Unable to parse eTags from: \" + partETagBody);\n+        throw new WrappedResponse(je,\n+          error(Response.Status.INTERNAL_SERVER_ERROR, \"Could not complete multipart upload\"));\n+      }\n+      try {\n+        S3AccessIO.completeMultipartUpload(idSupplied, storageidentifier, uploadId, eTagList);\n+      } catch (IOException io) {\n+        logger.warning(\n+          \"Multipart upload completion failed for uploadId: \" + uploadId + \" storageidentifier=\" + storageidentifier +\n+            \" globalId: \" + idSupplied);\n+        logger.warning(io.getMessage());\n         try {\n-            DatasetThumbnail datasetThumbnail = execCommand(new UpdateDatasetThumbnailCommand(createDataverseRequest(findUserOrDie()), findDatasetOrDie(idSupplied), UpdateDatasetThumbnailCommand.UserIntent.setDatasetFileAsThumbnail, dataFileIdSupplied, null));\n-            return ok(\"Thumbnail set to \" + datasetThumbnail.getBase64image());\n-        } catch (WrappedResponse wr) {\n-            return wr.getResponse();\n+          S3AccessIO.abortMultipartUpload(idSupplied, storageidentifier, uploadId);\n+        } catch (IOException e) {\n+          logger.severe(\"Also unable to abort the upload (and release the space on S3 for uploadId: \" + uploadId +\n+            \" storageidentifier=\" + storageidentifier + \" globalId: \" + idSupplied);\n+          logger.severe(io.getMessage());\n         }\n+\n+        throw new WrappedResponse(io,\n+          error(Response.Status.INTERNAL_SERVER_ERROR, \"Could not complete multipart upload\"));\n+      }\n+      return ok(\"Multipart Upload completed\");\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n+    }\n+  }\n+\n+  /**\n+   * Add a File to an existing Dataset\n+   *\n+   * @param idSupplied\n+   * @param jsonData\n+   * @param fileInputStream\n+   * @param contentDispositionHeader\n+   * @param formDataBodyPart\n+   * @return\n+   */\n+  @POST\n+  @Path(\"{id}/add\")\n+  @Consumes(MediaType.MULTIPART_FORM_DATA)\n+  public Response addFileToDataset(@PathParam(\"id\") String idSupplied,\n+                                   @FormDataParam(\"jsonData\") String jsonData,\n+                                   @FormDataParam(\"file\") InputStream fileInputStream,\n+                                   @FormDataParam(\"file\") FormDataContentDisposition contentDispositionHeader,\n+                                   @FormDataParam(\"file\") final FormDataBodyPart formDataBodyPart\n+  ) {\n+\n+    if (!systemConfig.isHTTPUpload()) {\n+      return error(Response.Status.SERVICE_UNAVAILABLE, BundleUtil.getStringFromBundle(\"file.api.httpDisabled\"));\n     }\n \n-    @POST\n-    @Path(\"{id}/thumbnail\")\n-    @Consumes(MediaType.MULTIPART_FORM_DATA)\n-    public Response uploadDatasetLogo(@PathParam(\"id\") String idSupplied, @FormDataParam(\"file\") InputStream inputStream\n-    ) {\n-        try {\n-            DatasetThumbnail datasetThumbnail = execCommand(new UpdateDatasetThumbnailCommand(createDataverseRequest(findUserOrDie()), findDatasetOrDie(idSupplied), UpdateDatasetThumbnailCommand.UserIntent.setNonDatasetFileAsThumbnail, null, inputStream));\n-            return ok(\"Thumbnail is now \" + datasetThumbnail.getBase64image());\n-        } catch (WrappedResponse wr) {\n-            return wr.getResponse();\n-        }\n+    // -------------------------------------\n+    // (1) Get the user from the API key\n+    // -------------------------------------\n+    User authUser;\n+    try {\n+      authUser = findUserOrDie();\n+    } catch (WrappedResponse ex) {\n+      return error(Response.Status.FORBIDDEN,\n+        BundleUtil.getStringFromBundle(\"file.addreplace.error.auth\")\n+      );\n     }\n \n-    @DELETE\n-    @Path(\"{id}/thumbnail\")\n-    public Response removeDatasetLogo(@PathParam(\"id\") String idSupplied) {\n-        try {\n-            DatasetThumbnail datasetThumbnail = execCommand(new UpdateDatasetThumbnailCommand(createDataverseRequest(findUserOrDie()), findDatasetOrDie(idSupplied), UpdateDatasetThumbnailCommand.UserIntent.removeThumbnail, null, null));\n-            return ok(\"Dataset thumbnail removed.\");\n-        } catch (WrappedResponse wr) {\n-            return wr.getResponse();\n-        }\n+\n+    // -------------------------------------\n+    // (2) Get the Dataset Id\n+    //\n+    // -------------------------------------\n+    Dataset dataset;\n+\n+    try {\n+      dataset = findDatasetOrDie(idSupplied);\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n     }\n \n-    @GET\n-    @Path(\"{identifier}/dataCaptureModule/rsync\")\n-    public Response getRsync(@PathParam(\"identifier\") String id) {\n-        //TODO - does it make sense to switch this to dataset identifier for consistency with the rest of the DCM APIs?\n-        if (!DataCaptureModuleUtil.rsyncSupportEnabled(settingsSvc.getValueForKey(SettingsServiceBean.Key.UploadMethods))) {\n-            return error(Response.Status.METHOD_NOT_ALLOWED, SettingsServiceBean.Key.UploadMethods + \" does not contain \" + SystemConfig.FileUploadMethods.RSYNC + \".\");\n-        }\n-        Dataset dataset = null;\n-        try {\n-            dataset = findDatasetOrDie(id);\n-            AuthenticatedUser user = findAuthenticatedUserOrDie();\n-            ScriptRequestResponse scriptRequestResponse = execCommand(new RequestRsyncScriptCommand(createDataverseRequest(user), dataset));\n-            \n-            DatasetLock lock = datasetService.addDatasetLock(dataset.getId(), DatasetLock.Reason.DcmUpload, user.getId(), \"script downloaded\");\n-            if (lock == null) {\n-                logger.log(Level.WARNING, \"Failed to lock the dataset (dataset id={0})\", dataset.getId());\n-                return error(Response.Status.FORBIDDEN, \"Failed to lock the dataset (dataset id=\"+dataset.getId()+\")\");\n-            }\n-            return ok(scriptRequestResponse.getScript(), MediaType.valueOf(MediaType.TEXT_PLAIN), null);\n-        } catch (WrappedResponse wr) {\n-            return wr.getResponse();\n-        } catch (EJBException ex) {\n-            return error(Response.Status.INTERNAL_SERVER_ERROR, \"Something went wrong attempting to download rsync script: \" + EjbUtil.ejbExceptionToString(ex));\n-        }\n+    //------------------------------------\n+    // (2a) Make sure dataset does not have package file\n+    //\n+    // --------------------------------------\n+\n+    for (DatasetVersion dv : dataset.getVersions()) {\n+      if (dv.isHasPackageFile()) {\n+        return error(Response.Status.FORBIDDEN,\n+          BundleUtil.getStringFromBundle(\"file.api.alreadyHasPackageFile\")\n+        );\n+      }\n     }\n-    \n-    /**\n-     * This api endpoint triggers the creation of a \"package\" file in a dataset \n-     *    after that package has been moved onto the same filesystem via the Data Capture Module.\n-     * The package is really just a way that Dataverse interprets a folder created by DCM, seeing it as just one file.\n-     * The \"package\" can be downloaded over RSAL.\n-     * \n-     * This endpoint currently supports both posix file storage and AWS s3 storage in Dataverse, and depending on which one is active acts accordingly.\n-     * \n-     * The initial design of the DCM/Dataverse interaction was not to use packages, but to allow import of all individual files natively into Dataverse.\n-     * But due to the possibly immense number of files (millions) the package approach was taken.\n-     * This is relevant because the posix (\"file\") code contains many remnants of that development work.\n-     * The s3 code was written later and is set to only support import as packages. It takes a lot from FileRecordWriter.\n-     * -MAD 4.9.1\n-     */\n-    @POST\n-    @Path(\"{identifier}/dataCaptureModule/checksumValidation\")\n-    public Response receiveChecksumValidationResults(@PathParam(\"identifier\") String id, JsonObject jsonFromDcm) {\n-        logger.log(Level.FINE, \"jsonFromDcm: {0}\", jsonFromDcm);\n-        AuthenticatedUser authenticatedUser = null;\n-        try {\n-            authenticatedUser = findAuthenticatedUserOrDie();\n-        } catch (WrappedResponse ex) {\n-            return error(Response.Status.BAD_REQUEST, \"Authentication is required.\");\n-        }\n-        if (!authenticatedUser.isSuperuser()) {\n-            return error(Response.Status.FORBIDDEN, \"Superusers only.\");\n-        }\n-        String statusMessageFromDcm = jsonFromDcm.getString(\"status\");\n-        try {\n-            Dataset dataset = findDatasetOrDie(id);\n-            if (\"validation passed\".equals(statusMessageFromDcm)) {\n-               logger.log(Level.INFO, \"Checksum Validation passed for DCM.\"); \n-\n-                String storageDriver = dataset.getDataverseContext().getEffectiveStorageDriverId();\n-                String uploadFolder = jsonFromDcm.getString(\"uploadFolder\");\n-                int totalSize = jsonFromDcm.getInt(\"totalSize\");\n-                String storageDriverType = System.getProperty(\"dataverse.file.\" + storageDriver + \".type\");\n-                \n-                if (storageDriverType.equals(\"file\")) {\n-                    logger.log(Level.INFO, \"File storage driver used for (dataset id={0})\", dataset.getId());\n-\n-                    ImportMode importMode = ImportMode.MERGE;\n-                    try {\n-                        JsonObject jsonFromImportJobKickoff = execCommand(new ImportFromFileSystemCommand(createDataverseRequest(findUserOrDie()), dataset, uploadFolder, new Long(totalSize), importMode));\n-                        long jobId = jsonFromImportJobKickoff.getInt(\"executionId\");\n-                        String message = jsonFromImportJobKickoff.getString(\"message\");\n-                        JsonObjectBuilder job = Json.createObjectBuilder();\n-                        job.add(\"jobId\", jobId);\n-                        job.add(\"message\", message);\n-                        return ok(job);\n-                    } catch (WrappedResponse wr) {\n-                        String message = wr.getMessage();\n-                        return error(Response.Status.INTERNAL_SERVER_ERROR, \"Uploaded files have passed checksum validation but something went wrong while attempting to put the files into Dataverse. Message was '\" + message + \"'.\");\n-                    }\n-                } else if(storageDriverType.equals(\"s3\")) {\n-                    \n-                    logger.log(Level.INFO, \"S3 storage driver used for DCM (dataset id={0})\", dataset.getId());\n-                    try {\n-                        \n-                        //Where the lifting is actually done, moving the s3 files over and having dataverse know of the existance of the package\n-                        s3PackageImporter.copyFromS3(dataset, uploadFolder);\n-                        DataFile packageFile = s3PackageImporter.createPackageDataFile(dataset, uploadFolder, new Long(totalSize));\n-                        \n-                        if (packageFile == null) {\n-                            logger.log(Level.SEVERE, \"S3 File package import failed.\");\n-                            return error(Response.Status.INTERNAL_SERVER_ERROR, \"S3 File package import failed.\");\n-                        }\n-                        DatasetLock dcmLock = dataset.getLockFor(DatasetLock.Reason.DcmUpload);\n-                        if (dcmLock == null) {\n-                            logger.log(Level.WARNING, \"Dataset not locked for DCM upload\");\n-                        } else {\n-                            datasetService.removeDatasetLocks(dataset, DatasetLock.Reason.DcmUpload);\n-                            dataset.removeLock(dcmLock);\n-                        }\n-                        \n-                        // update version using the command engine to enforce user permissions and constraints\n-                        if (dataset.getVersions().size() == 1 && dataset.getLatestVersion().getVersionState() == DatasetVersion.VersionState.DRAFT) {\n-                            try {\n-                                Command<Dataset> cmd;\n-                                cmd = new UpdateDatasetVersionCommand(dataset, new DataverseRequest(authenticatedUser, (HttpServletRequest) null));\n-                                commandEngine.submit(cmd);\n-                            } catch (CommandException ex) {\n-                                return error(Response.Status.INTERNAL_SERVER_ERROR, \"CommandException updating DatasetVersion from batch job: \" + ex.getMessage());\n-                            }\n-                        } else {\n-                            String constraintError = \"ConstraintException updating DatasetVersion form batch job: dataset must be a \"\n-                                    + \"single version in draft mode.\";\n-                            logger.log(Level.SEVERE, constraintError);\n-                        }\n \n-                        JsonObjectBuilder job = Json.createObjectBuilder();\n-                        return ok(job);\n-                        \n-                    }  catch (IOException e) {\n-                        String message = e.getMessage();\n-                        return error(Response.Status.INTERNAL_SERVER_ERROR, \"Uploaded files have passed checksum validation but something went wrong while attempting to move the files into Dataverse. Message was '\" + message + \"'.\");\n-                    } \n-                } else {\n-                    return error(Response.Status.INTERNAL_SERVER_ERROR, \"Invalid storage driver in Dataverse, not compatible with dcm\");\n-                }\n-            } else if (\"validation failed\".equals(statusMessageFromDcm)) {\n-                Map<String, AuthenticatedUser> distinctAuthors = permissionService.getDistinctUsersWithPermissionOn(Permission.EditDataset, dataset);\n-                distinctAuthors.values().forEach((value) -> {\n-                    userNotificationService.sendNotification((AuthenticatedUser) value, new Timestamp(new Date().getTime()), UserNotification.Type.CHECKSUMFAIL, dataset.getId());\n-                });\n-                List<AuthenticatedUser> superUsers = authenticationServiceBean.findSuperUsers();\n-                if (superUsers != null && !superUsers.isEmpty()) {\n-                    superUsers.forEach((au) -> {\n-                        userNotificationService.sendNotification(au, new Timestamp(new Date().getTime()), UserNotification.Type.CHECKSUMFAIL, dataset.getId());\n-                    });\n-                }\n-                return ok(\"User notified about checksum validation failure.\");\n-            } else {\n-                return error(Response.Status.BAD_REQUEST, \"Unexpected status cannot be processed: \" + statusMessageFromDcm);\n-            }\n-        } catch (WrappedResponse ex) {\n-            return ex.getResponse();\n-        }\n+    // (2a) Load up optional params via JSON\n+    //---------------------------------------\n+    OptionalFileParams optionalFileParams = null;\n+    msgt(\"(api) jsonData: \" + jsonData);\n+\n+    try {\n+      optionalFileParams = new OptionalFileParams(jsonData);\n+    } catch (DataFileTagException ex) {\n+      return error(Response.Status.BAD_REQUEST, ex.getMessage());\n+    } catch (ClassCastException | com.google.gson.JsonParseException ex) {\n+      return error(Response.Status.BAD_REQUEST, BundleUtil.getStringFromBundle(\"file.addreplace.error.parsing\"));\n     }\n-    \n \n-    @POST\n-    @Path(\"{id}/submitForReview\")\n-    public Response submitForReview(@PathParam(\"id\") String idSupplied) {\n-        try {\n-            Dataset updatedDataset = execCommand(new SubmitDatasetForReviewCommand(createDataverseRequest(findUserOrDie()), findDatasetOrDie(idSupplied)));\n-            JsonObjectBuilder result = Json.createObjectBuilder();\n-            \n-            boolean inReview = updatedDataset.isLockedFor(DatasetLock.Reason.InReview);\n-            \n-            result.add(\"inReview\", inReview);\n-            result.add(\"message\", \"Dataset id \" + updatedDataset.getId() + \" has been submitted for review.\");\n-            return ok(result);\n-        } catch (WrappedResponse wr) {\n-            return wr.getResponse();\n+    // -------------------------------------\n+    // (3) Get the file name and content type\n+    // -------------------------------------\n+    String newFilename = null;\n+    String newFileContentType = null;\n+    String newStorageIdentifier = null;\n+    if (null == contentDispositionHeader) {\n+      if (optionalFileParams.hasStorageIdentifier()) {\n+        newStorageIdentifier = optionalFileParams.getStorageIdentifier();\n+        // ToDo - check that storageIdentifier is valid\n+        if (optionalFileParams.hasFileName()) {\n+          newFilename = optionalFileParams.getFileName();\n+          if (optionalFileParams.hasMimetype()) {\n+            newFileContentType = optionalFileParams.getMimeType();\n+          }\n         }\n+      } else {\n+        return error(BAD_REQUEST,\n+          \"You must upload a file or provide a storageidentifier, filename, and mimetype.\");\n+      }\n+    } else {\n+      newFilename = contentDispositionHeader.getFileName();\n+      newFileContentType = formDataBodyPart.getMediaType().toString();\n     }\n \n-    @POST\n-    @Path(\"{id}/returnToAuthor\")\n-    public Response returnToAuthor(@PathParam(\"id\") String idSupplied, String jsonBody) {\n-        \n-        if (jsonBody == null || jsonBody.isEmpty()) {\n-            return error(Response.Status.BAD_REQUEST, \"You must supply JSON to this API endpoint and it must contain a reason for returning the dataset (field: reasonForReturn).\");\n-        }\n-        StringReader rdr = new StringReader(jsonBody);\n-        JsonObject json = Json.createReader(rdr).readObject();\n-        try {\n-            Dataset dataset = findDatasetOrDie(idSupplied);\n-            String reasonForReturn = null;           \n-            reasonForReturn = json.getString(\"reasonForReturn\");\n-            // TODO: Once we add a box for the curator to type into, pass the reason for return to the ReturnDatasetToAuthorCommand and delete this check and call to setReturnReason on the API side.\n-            if (reasonForReturn == null || reasonForReturn.isEmpty()) {\n-                return error(Response.Status.BAD_REQUEST, \"You must enter a reason for returning a dataset to the author(s).\");\n-            }\n-            AuthenticatedUser authenticatedUser = findAuthenticatedUserOrDie();\n-            Dataset updatedDataset = execCommand(new ReturnDatasetToAuthorCommand(createDataverseRequest(authenticatedUser), dataset, reasonForReturn ));\n-\n-            JsonObjectBuilder result = Json.createObjectBuilder();\n-            result.add(\"inReview\", false);\n-            result.add(\"message\", \"Dataset id \" + updatedDataset.getId() + \" has been sent back to the author(s).\");\n-            return ok(result);\n-        } catch (WrappedResponse wr) {\n-            return wr.getResponse();\n+\n+    //-------------------\n+    // (3) Create the AddReplaceFileHelper object\n+    //-------------------\n+    msg(\"ADD!\");\n+\n+    DataverseRequest dvRequest2 = createDataverseRequest(authUser);\n+    AddReplaceFileHelper addFileHelper = new AddReplaceFileHelper(dvRequest2,\n+      ingestService,\n+      datasetService,\n+      fileService,\n+      permissionSvc,\n+      commandEngine,\n+      systemConfig);\n+\n+\n+    //-------------------\n+    // (4) Run \"runAddFileByDatasetId\"\n+    //-------------------\n+    addFileHelper.runAddFileByDataset(dataset,\n+      newFilename,\n+      newFileContentType,\n+      newStorageIdentifier,\n+      fileInputStream,\n+      optionalFileParams);\n+\n+\n+    if (addFileHelper.hasError()) {\n+      return error(addFileHelper.getHttpErrorCode(), addFileHelper.getErrorMessagesAsString(\"\\n\"));\n+    } else {\n+      String successMsg = BundleUtil.getStringFromBundle(\"file.addreplace.success.add\");\n+      try {\n+        //msgt(\"as String: \" + addFileHelper.getSuccessResult());\n+        /**\n+         * @todo We need a consistent, sane way to communicate a human\n+         * readable message to an API client suitable for human\n+         * consumption. Imagine if the UI were built in Angular or React\n+         * and we want to return a message from the API as-is to the\n+         * user. Human readable.\n+         */\n+        logger.fine(\"successMsg: \" + successMsg);\n+        String duplicateWarning = addFileHelper.getDuplicateFileWarning();\n+        if (duplicateWarning != null && !duplicateWarning.isEmpty()) {\n+          return ok(addFileHelper.getDuplicateFileWarning(), addFileHelper.getSuccessResultAsJsonObjectBuilder());\n+        } else {\n+          return ok(addFileHelper.getSuccessResultAsJsonObjectBuilder());\n         }\n+\n+        //\"Look at that!  You added a file! (hey hey, it may have worked)\");\n+      } catch (NoFilesException ex) {\n+        Logger.getLogger(Files.class.getName()).log(Level.SEVERE, null, ex);\n+        return error(Response.Status.BAD_REQUEST, \"NoFileException!  Serious Error! See administrator!\");\n+\n+      }\n     }\n \n-@GET\n-@Path(\"{id}/uploadsid\")\n-@Deprecated\n-public Response getUploadUrl(@PathParam(\"id\") String idSupplied) {\n-\ttry {\n-\t\tDataset dataset = findDatasetOrDie(idSupplied);\n-\n-\t\tboolean canUpdateDataset = false;\n-\t\ttry {\n-\t\t\tcanUpdateDataset = permissionSvc.requestOn(createDataverseRequest(findUserOrDie()), dataset).canIssue(UpdateDatasetVersionCommand.class);\n-\t\t} catch (WrappedResponse ex) {\n-\t\t\tlogger.info(\"Exception thrown while trying to figure out permissions while getting upload URL for dataset id \" + dataset.getId() + \": \" + ex.getLocalizedMessage());\n-\t\t\tthrow ex;\n-\t\t}\n-\t\tif (!canUpdateDataset) {\n-            return error(Response.Status.FORBIDDEN, \"You are not permitted to upload files to this dataset.\");\n-        }\n-        S3AccessIO<?> s3io = FileUtil.getS3AccessForDirectUpload(dataset);\n-        if(s3io == null) {\n-        \treturn error(Response.Status.NOT_FOUND,\"Direct upload not supported for files in this dataset: \" + dataset.getId());\n-\t\t}\n-\t\tString url = null;\n-        String storageIdentifier = null;\n-\t\ttry {\n-\t\t\turl = s3io.generateTemporaryS3UploadUrl();\n-        \tstorageIdentifier = FileUtil.getStorageIdentifierFromLocation(s3io.getStorageLocation());\n-        } catch (IOException io) {\n-        \tlogger.warning(io.getMessage());\n-        \tthrow new WrappedResponse(io, error( Response.Status.INTERNAL_SERVER_ERROR, \"Could not create process direct upload request\"));\n-\t\t}\n-        \n-\t\tJsonObjectBuilder response = Json.createObjectBuilder()\n-\t            .add(\"url\", url)\n-\t            .add(\"storageIdentifier\", storageIdentifier );\n-\t\treturn ok(response);\n-\t} catch (WrappedResponse wr) {\n-\t\treturn wr.getResponse();\n-\t}\n-}\n+  } // end: addFileToDataset\n \n-@GET\n-@Path(\"{id}/uploadurls\")\n-public Response getMPUploadUrls(@PathParam(\"id\") String idSupplied, @QueryParam(\"size\") long fileSize) {\n-\ttry {\n-\t\tDataset dataset = findDatasetOrDie(idSupplied);\n-\n-\t\tboolean canUpdateDataset = false;\n-\t\ttry {\n-\t\t\tcanUpdateDataset = permissionSvc.requestOn(createDataverseRequest(findUserOrDie()), dataset)\n-\t\t\t\t\t.canIssue(UpdateDatasetVersionCommand.class);\n-\t\t} catch (WrappedResponse ex) {\n-\t\t\tlogger.info(\n-\t\t\t\t\t\"Exception thrown while trying to figure out permissions while getting upload URLs for dataset id \"\n-\t\t\t\t\t\t\t+ dataset.getId() + \": \" + ex.getLocalizedMessage());\n-\t\t\tthrow ex;\n-\t\t}\n-\t\tif (!canUpdateDataset) {\n-\t\t\treturn error(Response.Status.FORBIDDEN, \"You are not permitted to upload files to this dataset.\");\n-\t\t}\n-\t\tS3AccessIO<DataFile> s3io = FileUtil.getS3AccessForDirectUpload(dataset);\n-\t\tif (s3io == null) {\n-\t\t\treturn error(Response.Status.NOT_FOUND,\n-\t\t\t\t\t\"Direct upload not supported for files in this dataset: \" + dataset.getId());\n-\t\t}\n-\t\tJsonObjectBuilder response = null;\n-\t\tString storageIdentifier = null;\n-\t\ttry {\n-\t\t\tstorageIdentifier = FileUtil.getStorageIdentifierFromLocation(s3io.getStorageLocation());\n-\t\t\tresponse = s3io.generateTemporaryS3UploadUrls(dataset.getGlobalId().asString(), storageIdentifier, fileSize);\n-\n-\t\t} catch (IOException io) {\n-\t\t\tlogger.warning(io.getMessage());\n-\t\t\tthrow new WrappedResponse(io,\n-\t\t\t\t\terror(Response.Status.INTERNAL_SERVER_ERROR, \"Could not create process direct upload request\"));\n-\t\t}\n-\n-\t\tresponse.add(\"storageIdentifier\", storageIdentifier);\n-\t\treturn ok(response);\n-\t} catch (WrappedResponse wr) {\n-\t\treturn wr.getResponse();\n-\t}\n-}\n \n-@DELETE\n-@Path(\"mpupload\")\n-public Response abortMPUpload(@QueryParam(\"globalid\") String idSupplied, @QueryParam(\"storageidentifier\") String storageidentifier, @QueryParam(\"uploadid\") String uploadId) {\n-\ttry {\n-\t\tDataset dataset = datasetSvc.findByGlobalId(idSupplied);\n-\t\t//Allow the API to be used within a session (e.g. for direct upload in the UI)\n-\t\tUser user =session.getUser();\n-\t\tif (!user.isAuthenticated()) {\n-\t\t\ttry {\n-\t\t\t\tuser = findAuthenticatedUserOrDie();\n-\t\t\t} catch (WrappedResponse ex) {\n-\t\t\t\tlogger.info(\n-\t\t\t\t\t\t\"Exception thrown while trying to figure out permissions while getting aborting upload for dataset id \"\n-\t\t\t\t\t\t\t\t+ dataset.getId() + \": \" + ex.getLocalizedMessage());\n-\t\t\t\tthrow ex;\n-\t\t\t}\n-\t\t}\n-\t\tboolean allowed = false;\n-\t\tif (dataset != null) {\n-\t\t\t\tallowed = permissionSvc.requestOn(createDataverseRequest(user), dataset)\n-\t\t\t\t\t\t.canIssue(UpdateDatasetVersionCommand.class);\n-\t\t} else {\n-\t\t\t/*\n-\t\t\t * The only legitimate case where a global id won't correspond to a dataset is\n-\t\t\t * for uploads during creation. Given that this call will still fail unless all\n-\t\t\t * three parameters correspond to an active multipart upload, it should be safe\n-\t\t\t * to allow the attempt for an authenticated user. If there are concerns about\n-\t\t\t * permissions, one could check with the current design that the user is allowed\n-\t\t\t * to create datasets in some dataverse that is configured to use the storage\n-\t\t\t * provider specified in the storageidentifier, but testing for the ability to\n-\t\t\t * create a dataset in a specific dataverse would requiring changing the design\n-\t\t\t * somehow (e.g. adding the ownerId to this call).\n-\t\t\t */\n-\t\t\tallowed = true;\n-\t\t}\n-\t\tif (!allowed) {\n-\t\t\treturn error(Response.Status.FORBIDDEN,\n-\t\t\t\t\t\"You are not permitted to abort file uploads with the supplied parameters.\");\n-\t\t}\n-\t\ttry {\n-\t\t\tS3AccessIO.abortMultipartUpload(idSupplied, storageidentifier, uploadId);\n-\t\t} catch (IOException io) {\n-\t\t\tlogger.warning(\"Multipart upload abort failed for uploadId: \" + uploadId + \" storageidentifier=\"\n-\t\t\t\t\t+ storageidentifier + \" dataset Id: \" + dataset.getId());\n-\t\t\tlogger.warning(io.getMessage());\n-\t\t\tthrow new WrappedResponse(io,\n-\t\t\t\t\terror(Response.Status.INTERNAL_SERVER_ERROR, \"Could not abort multipart upload\"));\n-\t\t}\n-\t\treturn Response.noContent().build();\n-\t} catch (WrappedResponse wr) {\n-\t\treturn wr.getResponse();\n-\t}\n-}\n+  private void msg(String m) {\n+    //System.out.println(m);\n+    logger.fine(m);\n+  }\n \n-@PUT\n-@Path(\"mpupload\")\n-public Response completeMPUpload(String partETagBody, @QueryParam(\"globalid\") String idSupplied, @QueryParam(\"storageidentifier\") String storageidentifier, @QueryParam(\"uploadid\") String uploadId)  {\n-\ttry {\n-\t\tDataset dataset = datasetSvc.findByGlobalId(idSupplied);\n-\t\t//Allow the API to be used within a session (e.g. for direct upload in the UI)\n-\t\tUser user =session.getUser();\n-\t\tif (!user.isAuthenticated()) {\n-\t\t\ttry {\n-\t\t\t\tuser=findAuthenticatedUserOrDie();\n-\t\t\t} catch (WrappedResponse ex) {\n-\t\t\t\tlogger.info(\n-\t\t\t\t\t\t\"Exception thrown while trying to figure out permissions to complete mpupload for dataset id \"\n-\t\t\t\t\t\t\t\t+ dataset.getId() + \": \" + ex.getLocalizedMessage());\n-\t\t\t\tthrow ex;\n-\t\t\t}\n-\t\t}\n-\t\tboolean allowed = false;\n-\t\tif (dataset != null) {\n-\t\t\t\tallowed = permissionSvc.requestOn(createDataverseRequest(user), dataset)\n-\t\t\t\t\t\t.canIssue(UpdateDatasetVersionCommand.class);\n-\t\t} else {\n-\t\t\t/*\n-\t\t\t * The only legitimate case where a global id won't correspond to a dataset is\n-\t\t\t * for uploads during creation. Given that this call will still fail unless all\n-\t\t\t * three parameters correspond to an active multipart upload, it should be safe\n-\t\t\t * to allow the attempt for an authenticated user. If there are concerns about\n-\t\t\t * permissions, one could check with the current design that the user is allowed\n-\t\t\t * to create datasets in some dataverse that is configured to use the storage\n-\t\t\t * provider specified in the storageidentifier, but testing for the ability to\n-\t\t\t * create a dataset in a specific dataverse would requiring changing the design\n-\t\t\t * somehow (e.g. adding the ownerId to this call).\n-\t\t\t */\n-\t\t\tallowed = true;\n-\t\t}\n-\t\tif (!allowed) {\n-\t\t\treturn error(Response.Status.FORBIDDEN,\n-\t\t\t\t\t\"You are not permitted to complete file uploads with the supplied parameters.\");\n-\t\t}\n-\t\tList<PartETag> eTagList = new ArrayList<PartETag>();\n-        logger.info(\"Etags: \" + partETagBody);\n-\t\ttry {\n-\t\t\tJsonReader jsonReader = Json.createReader(new StringReader(partETagBody));\n-\t\t\tJsonObject object = jsonReader.readObject();\n-\t\t\tjsonReader.close();\n-\t\t\tfor(String partNo : object.keySet()) {\n-\t\t\t\teTagList.add(new PartETag(Integer.parseInt(partNo), object.getString(partNo)));\n-\t\t\t}\n-\t\t\tfor(PartETag et: eTagList) {\n-\t\t\t\tlogger.info(\"Part: \" + et.getPartNumber() + \" : \" + et.getETag());\n-\t\t\t}\n-\t\t} catch (JsonException je) {\n-\t\t\tlogger.info(\"Unable to parse eTags from: \" + partETagBody);\n-\t\t\tthrow new WrappedResponse(je, error( Response.Status.INTERNAL_SERVER_ERROR, \"Could not complete multipart upload\"));\n-\t\t}\n-\t\ttry {\n-\t\t\tS3AccessIO.completeMultipartUpload(idSupplied, storageidentifier, uploadId, eTagList);\n-\t\t} catch (IOException io) {\n-\t\t\tlogger.warning(\"Multipart upload completion failed for uploadId: \" + uploadId +\" storageidentifier=\" + storageidentifier + \" globalId: \" + idSupplied);\n-\t\t\tlogger.warning(io.getMessage());\n-\t\t\ttry {\n-\t\t\t\tS3AccessIO.abortMultipartUpload(idSupplied, storageidentifier, uploadId);\n-\t\t\t} catch (IOException e) {\n-\t\t\t\tlogger.severe(\"Also unable to abort the upload (and release the space on S3 for uploadId: \" + uploadId +\" storageidentifier=\" + storageidentifier + \" globalId: \" + idSupplied);\n-\t\t\t\tlogger.severe(io.getMessage());\n-\t\t\t}\n-\n-\t\t\tthrow new WrappedResponse(io, error( Response.Status.INTERNAL_SERVER_ERROR, \"Could not complete multipart upload\")); \n-\t\t}\n-\t\treturn ok(\"Multipart Upload completed\");\n-\t} catch (WrappedResponse wr) {\n-\t\treturn wr.getResponse();\n-\t}\n-}\n+  private void dashes() {\n+    msg(\"----------------\");\n+  }\n \n-    /**\n-     * Add a File to an existing Dataset\n-     * \n-     * @param idSupplied\n-     * @param jsonData\n-     * @param fileInputStream\n-     * @param contentDispositionHeader\n-     * @param formDataBodyPart\n-     * @return \n-     */\n-    @POST\n-    @Path(\"{id}/add\")\n-    @Consumes(MediaType.MULTIPART_FORM_DATA)\n-    public Response addFileToDataset(@PathParam(\"id\") String idSupplied,\n-                    @FormDataParam(\"jsonData\") String jsonData,\n-                    @FormDataParam(\"file\") InputStream fileInputStream,\n-                    @FormDataParam(\"file\") FormDataContentDisposition contentDispositionHeader,\n-                    @FormDataParam(\"file\") final FormDataBodyPart formDataBodyPart\n-                    ){\n-\n-        if (!systemConfig.isHTTPUpload()) {\n-            return error(Response.Status.SERVICE_UNAVAILABLE, BundleUtil.getStringFromBundle(\"file.api.httpDisabled\"));\n-        }\n+  private void msgt(String m) {\n+    dashes();\n+    msg(m);\n+    dashes();\n+  }\n \n-        // -------------------------------------\n-        // (1) Get the user from the API key\n-        // -------------------------------------\n-        User authUser;\n-        try {\n-            authUser = findUserOrDie();\n-        } catch (WrappedResponse ex) {\n-            return error(Response.Status.FORBIDDEN,\n-                    BundleUtil.getStringFromBundle(\"file.addreplace.error.auth\")\n-                    );\n-        }\n-        \n-        \n-        // -------------------------------------\n-        // (2) Get the Dataset Id\n-        //  \n-        // -------------------------------------\n-        Dataset dataset;\n-        \n+\n+  public static <T> T handleVersion(String versionId, DsVersionHandler<T> hdl)\n+    throws WrappedResponse {\n+    switch (versionId) {\n+      case \":latest\":\n+        return hdl.handleLatest();\n+      case \":draft\":\n+        return hdl.handleDraft();\n+      case \":latest-published\":\n+        return hdl.handleLatestPublished();\n+      default:\n         try {\n-            dataset = findDatasetOrDie(idSupplied);\n-        } catch (WrappedResponse wr) {\n-            return wr.getResponse();           \n+          String[] versions = versionId.split(\"\\\\.\");\n+          switch (versions.length) {\n+            case 1:\n+              return hdl.handleSpecific(Long.parseLong(versions[0]), (long) 0.0);\n+            case 2:\n+              return hdl.handleSpecific(Long.parseLong(versions[0]), Long.parseLong(versions[1]));\n+            default:\n+              throw new WrappedResponse(\n+                error(Response.Status.BAD_REQUEST, \"Illegal version identifier '\" + versionId + \"'\"));\n+          }\n+        } catch (NumberFormatException nfe) {\n+          throw new WrappedResponse(\n+            error(Response.Status.BAD_REQUEST, \"Illegal version identifier '\" + versionId + \"'\"));\n         }\n-        \n-        //------------------------------------\n-        // (2a) Make sure dataset does not have package file\n-        //\n-        // --------------------------------------\n-        \n-        for (DatasetVersion dv : dataset.getVersions()) {\n-            if (dv.isHasPackageFile()) {\n-                return error(Response.Status.FORBIDDEN,\n-                        BundleUtil.getStringFromBundle(\"file.api.alreadyHasPackageFile\")\n-                );\n-            }\n+    }\n+  }\n+\n+  private DatasetVersion getDatasetVersionOrDie(final DataverseRequest req, String versionNumber, final Dataset ds,\n+                                                UriInfo uriInfo, HttpHeaders headers) throws WrappedResponse {\n+    DatasetVersion dsv = execCommand(handleVersion(versionNumber, new DsVersionHandler<Command<DatasetVersion>>() {\n+\n+      @Override\n+      public Command<DatasetVersion> handleLatest() {\n+        return new GetLatestAccessibleDatasetVersionCommand(req, ds);\n+      }\n+\n+      @Override\n+      public Command<DatasetVersion> handleDraft() {\n+        return new GetDraftDatasetVersionCommand(req, ds);\n+      }\n+\n+      @Override\n+      public Command<DatasetVersion> handleSpecific(long major, long minor) {\n+        return new GetSpecificPublishedDatasetVersionCommand(req, ds, major, minor);\n+      }\n+\n+      @Override\n+      public Command<DatasetVersion> handleLatestPublished() {\n+        return new GetLatestPublishedDatasetVersionCommand(req, ds);\n+      }\n+    }));\n+    if (dsv == null || dsv.getId() == null) {\n+      throw new WrappedResponse(\n+        notFound(\"Dataset version \" + versionNumber + \" of dataset \" + ds.getId() + \" not found\"));\n+    }\n+    if (dsv.isReleased()) {\n+      MakeDataCountLoggingServiceBean.MakeDataCountEntry entry =\n+        new MakeDataCountEntry(uriInfo, headers, dvRequestService, ds);\n+      mdcLogService.logEntry(entry);\n+    }\n+    return dsv;\n+  }\n+\n+  @GET\n+  @Path(\"{identifier}/locks\")\n+  public Response getLocks(@PathParam(\"identifier\") String id, @QueryParam(\"type\") DatasetLock.Reason lockType) {\n+\n+    Dataset dataset = null;\n+    try {\n+      dataset = findDatasetOrDie(id);\n+      Set<DatasetLock> locks;\n+      if (lockType == null) {\n+        locks = dataset.getLocks();\n+      } else {\n+        // request for a specific type lock:\n+        DatasetLock lock = dataset.getLockFor(lockType);\n+\n+        locks = new HashSet<>();\n+        if (lock != null) {\n+          locks.add(lock);\n         }\n+      }\n \n-        // (2a) Load up optional params via JSON\n-        //---------------------------------------\n-        OptionalFileParams optionalFileParams = null;\n-        msgt(\"(api) jsonData: \" +  jsonData);\n+      return ok(locks.stream().map(lock -> json(lock)).collect(toJsonArray()));\n \n-        try {\n-            optionalFileParams = new OptionalFileParams(jsonData);\n-        } catch (DataFileTagException ex) {\n-            return error( Response.Status.BAD_REQUEST, ex.getMessage());            \n-        }\n-        catch (ClassCastException | com.google.gson.JsonParseException ex) {\n-            return error(Response.Status.BAD_REQUEST, BundleUtil.getStringFromBundle(\"file.addreplace.error.parsing\"));\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n+    }\n+  }\n+\n+  @DELETE\n+  @Path(\"{identifier}/locks\")\n+  public Response deleteLocks(@PathParam(\"identifier\") String id, @QueryParam(\"type\") DatasetLock.Reason lockType) {\n+\n+    return response(req -> {\n+      try {\n+        AuthenticatedUser user = findAuthenticatedUserOrDie();\n+        if (!user.isSuperuser()) {\n+          return error(Response.Status.FORBIDDEN, \"This API end point can be used by superusers only.\");\n         }\n-        \n-        // -------------------------------------\n-        // (3) Get the file name and content type\n-        // -------------------------------------\n-        String newFilename = null;\n-        String newFileContentType = null;\n-        String newStorageIdentifier = null;\n-\t\tif (null == contentDispositionHeader) {\n-\t\t\tif (optionalFileParams.hasStorageIdentifier()) {\n-\t\t\t\tnewStorageIdentifier = optionalFileParams.getStorageIdentifier();\n-\t\t\t\t// ToDo - check that storageIdentifier is valid\n-\t\t\t\tif (optionalFileParams.hasFileName()) {\n-\t\t\t\t\tnewFilename = optionalFileParams.getFileName();\n-\t\t\t\t\tif (optionalFileParams.hasMimetype()) {\n-\t\t\t\t\t\tnewFileContentType = optionalFileParams.getMimeType();\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\treturn error(BAD_REQUEST,\n-\t\t\t\t\t\t\"You must upload a file or provide a storageidentifier, filename, and mimetype.\");\n-\t\t\t}\n-\t\t} else {\n-\t\t\tnewFilename = contentDispositionHeader.getFileName();\n-\t\t\tnewFileContentType = formDataBodyPart.getMediaType().toString();\n-\t\t}\n-\n-        \n-        //-------------------\n-        // (3) Create the AddReplaceFileHelper object\n-        //-------------------\n-        msg(\"ADD!\");\n-\n-        DataverseRequest dvRequest2 = createDataverseRequest(authUser);\n-        AddReplaceFileHelper addFileHelper = new AddReplaceFileHelper(dvRequest2,\n-                                                ingestService,\n-                                                datasetService,\n-                                                fileService,\n-                                                permissionSvc,\n-                                                commandEngine,\n-                                                systemConfig);\n-\n-\n-        //-------------------\n-        // (4) Run \"runAddFileByDatasetId\"\n-        //-------------------\n-        addFileHelper.runAddFileByDataset(dataset,\n-                                newFilename,\n-                                newFileContentType,\n-                                newStorageIdentifier,\n-                                fileInputStream,\n-                                optionalFileParams);\n-\n-\n-        if (addFileHelper.hasError()){\n-            return error(addFileHelper.getHttpErrorCode(), addFileHelper.getErrorMessagesAsString(\"\\n\"));\n-        }else{\n-            String successMsg = BundleUtil.getStringFromBundle(\"file.addreplace.success.add\");\n+        Dataset dataset = findDatasetOrDie(id);\n+\n+        if (lockType == null) {\n+          Set<DatasetLock.Reason> locks = new HashSet<>();\n+          for (DatasetLock lock : dataset.getLocks()) {\n+            locks.add(lock.getReason());\n+          }\n+          if (!locks.isEmpty()) {\n+            for (DatasetLock.Reason locktype : locks) {\n+              execCommand(new RemoveLockCommand(req, dataset, locktype));\n+              // refresh the dataset:\n+              dataset = findDatasetOrDie(id);\n+            }\n+            // kick of dataset reindexing, in case the locks removed\n+            // affected the search card:\n             try {\n-                //msgt(\"as String: \" + addFileHelper.getSuccessResult());\n-                /**\n-                 * @todo We need a consistent, sane way to communicate a human\n-                 * readable message to an API client suitable for human\n-                 * consumption. Imagine if the UI were built in Angular or React\n-                 * and we want to return a message from the API as-is to the\n-                 * user. Human readable.\n-                 */\n-                logger.fine(\"successMsg: \" + successMsg);\n-                String duplicateWarning = addFileHelper.getDuplicateFileWarning();\n-                if (duplicateWarning != null && !duplicateWarning.isEmpty()) {\n-                    return ok(addFileHelper.getDuplicateFileWarning(), addFileHelper.getSuccessResultAsJsonObjectBuilder());\n-                } else {\n-                    return ok(addFileHelper.getSuccessResultAsJsonObjectBuilder());\n-                }\n-                \n-                //\"Look at that!  You added a file! (hey hey, it may have worked)\");\n-            } catch (NoFilesException ex) {\n-                Logger.getLogger(Files.class.getName()).log(Level.SEVERE, null, ex);\n-                return error(Response.Status.BAD_REQUEST, \"NoFileException!  Serious Error! See administrator!\");\n+              indexService.indexDataset(dataset, true);\n+            } catch (IOException | SolrServerException e) {\n+              String failureLogText =\n+                \"Post lock removal indexing failed. You can kickoff a re-index of this dataset with: \\r\\n curl http://localhost:8080/api/admin/index/datasets/\" +\n+                  dataset.getId().toString();\n+              failureLogText += \"\\r\\n\" + e.getLocalizedMessage();\n+              LoggingUtil.writeOnSuccessFailureLog(null, failureLogText, dataset);\n \n             }\n+            return ok(\"locks removed\");\n+          }\n+          return ok(\"dataset not locked\");\n         }\n-            \n-    } // end: addFileToDataset\n-\n-\n-    \n-    private void msg(String m){\n-        //System.out.println(m);\n-        logger.fine(m);\n-    }\n-    private void dashes(){\n-        msg(\"----------------\");\n-    }\n-    private void msgt(String m){\n-        dashes(); msg(m); dashes();\n-    }\n-    \n-    \n-    public static <T> T handleVersion( String versionId, DsVersionHandler<T> hdl )\n-        throws WrappedResponse {\n-        switch (versionId) {\n-            case \":latest\": return hdl.handleLatest();\n-            case \":draft\": return hdl.handleDraft();\n-            case \":latest-published\": return hdl.handleLatestPublished();\n-            default:\n-                try {\n-                    String[] versions = versionId.split(\"\\\\.\");\n-                    switch (versions.length) {\n-                        case 1:\n-                            return hdl.handleSpecific(Long.parseLong(versions[0]), (long)0.0);\n-                        case 2:\n-                            return hdl.handleSpecific( Long.parseLong(versions[0]), Long.parseLong(versions[1]) );\n-                        default:\n-                            throw new WrappedResponse(error( Response.Status.BAD_REQUEST, \"Illegal version identifier '\" + versionId + \"'\"));\n-                    }\n-                } catch ( NumberFormatException nfe ) {\n-                    throw new WrappedResponse( error( Response.Status.BAD_REQUEST, \"Illegal version identifier '\" + versionId + \"'\") );\n-                }\n+        // request for a specific type lock:\n+        DatasetLock lock = dataset.getLockFor(lockType);\n+        if (lock != null) {\n+          execCommand(new RemoveLockCommand(req, dataset, lock.getReason()));\n+          // refresh the dataset:\n+          dataset = findDatasetOrDie(id);\n+          // ... and kick of dataset reindexing, in case the lock removed\n+          // affected the search card:\n+          try {\n+            indexService.indexDataset(dataset, true);\n+          } catch (IOException | SolrServerException e) {\n+            String failureLogText =\n+              \"Post lock removal indexing failed. You can kickoff a re-index of this dataset with: \\r\\n curl http://localhost:8080/api/admin/index/datasets/\" +\n+                dataset.getId().toString();\n+            failureLogText += \"\\r\\n\" + e.getLocalizedMessage();\n+            LoggingUtil.writeOnSuccessFailureLog(null, failureLogText, dataset);\n+\n+          }\n+          return ok(\"lock type \" + lock.getReason() + \" removed\");\n         }\n-    }\n-    \n-    private DatasetVersion getDatasetVersionOrDie( final DataverseRequest req, String versionNumber, final Dataset ds, UriInfo uriInfo, HttpHeaders headers) throws WrappedResponse {\n-        DatasetVersion dsv = execCommand( handleVersion(versionNumber, new DsVersionHandler<Command<DatasetVersion>>(){\n+        return ok(\"no lock type \" + lockType + \" on the dataset\");\n+      } catch (WrappedResponse wr) {\n+        return wr.getResponse();\n+      }\n \n-                @Override\n-                public Command<DatasetVersion> handleLatest() {\n-                    return new GetLatestAccessibleDatasetVersionCommand(req, ds);\n-                }\n+    });\n \n-                @Override\n-                public Command<DatasetVersion> handleDraft() {\n-                    return new GetDraftDatasetVersionCommand(req, ds);\n-                }\n-  \n-                @Override\n-                public Command<DatasetVersion> handleSpecific(long major, long minor) {\n-                    return new GetSpecificPublishedDatasetVersionCommand(req, ds, major, minor);\n-                }\n+  }\n \n-                @Override\n-                public Command<DatasetVersion> handleLatestPublished() {\n-                    return new GetLatestPublishedDatasetVersionCommand(req, ds);\n-                }\n-            }));\n-        if ( dsv == null || dsv.getId() == null ) {\n-            throw new WrappedResponse( notFound(\"Dataset version \" + versionNumber + \" of dataset \" + ds.getId() + \" not found\") );\n+  @POST\n+  @Path(\"{identifier}/lock/{type}\")\n+  public Response lockDataset(@PathParam(\"identifier\") String id, @PathParam(\"type\") DatasetLock.Reason lockType) {\n+    return response(req -> {\n+      try {\n+        AuthenticatedUser user = findAuthenticatedUserOrDie();\n+        if (!user.isSuperuser()) {\n+          return error(Response.Status.FORBIDDEN, \"This API end point can be used by superusers only.\");\n         }\n-        if (dsv.isReleased()) {\n-            MakeDataCountLoggingServiceBean.MakeDataCountEntry entry = new MakeDataCountEntry(uriInfo, headers, dvRequestService, ds);\n-            mdcLogService.logEntry(entry);\n+        Dataset dataset = findDatasetOrDie(id);\n+        DatasetLock lock = dataset.getLockFor(lockType);\n+        if (lock != null) {\n+          return error(Response.Status.FORBIDDEN, \"dataset already locked with lock type \" + lockType);\n         }\n-        return dsv;\n-    }\n-    \n-    @GET\n-    @Path(\"{identifier}/locks\")\n-    public Response getLocks(@PathParam(\"identifier\") String id, @QueryParam(\"type\") DatasetLock.Reason lockType) {\n-\n-        Dataset dataset = null;\n+        lock = new DatasetLock(lockType, user);\n+        execCommand(new AddLockCommand(req, dataset, lock));\n+        // refresh the dataset:\n+        dataset = findDatasetOrDie(id);\n+        // ... and kick of dataset reindexing:\n         try {\n-            dataset = findDatasetOrDie(id);\n-            Set<DatasetLock> locks;      \n-            if (lockType == null) {\n-                locks = dataset.getLocks();\n-            } else {\n-                // request for a specific type lock:\n-                DatasetLock lock = dataset.getLockFor(lockType);\n+          indexService.indexDataset(dataset, true);\n+        } catch (IOException | SolrServerException e) {\n+          String failureLogText =\n+            \"Post add lock indexing failed. You can kickoff a re-index of this dataset with: \\r\\n curl http://localhost:8080/api/admin/index/datasets/\" +\n+              dataset.getId().toString();\n+          failureLogText += \"\\r\\n\" + e.getLocalizedMessage();\n+          LoggingUtil.writeOnSuccessFailureLog(null, failureLogText, dataset);\n \n-                locks = new HashSet<>(); \n-                if (lock != null) {\n-                    locks.add(lock);\n-                }\n-            }\n-            \n-            return ok(locks.stream().map(lock -> json(lock)).collect(toJsonArray()));\n-\n-        } catch (WrappedResponse wr) {\n-            return wr.getResponse();\n-        } \n-    }   \n-    \n-    @DELETE\n-    @Path(\"{identifier}/locks\")\n-    public Response deleteLocks(@PathParam(\"identifier\") String id, @QueryParam(\"type\") DatasetLock.Reason lockType) {\n-\n-        return response(req -> {\n-            try {\n-                AuthenticatedUser user = findAuthenticatedUserOrDie();\n-                if (!user.isSuperuser()) {\n-                    return error(Response.Status.FORBIDDEN, \"This API end point can be used by superusers only.\");\n-                }\n-                Dataset dataset = findDatasetOrDie(id);\n-                \n-                if (lockType == null) {\n-                    Set<DatasetLock.Reason> locks = new HashSet<>();\n-                    for (DatasetLock lock : dataset.getLocks()) {\n-                        locks.add(lock.getReason());\n-                    }\n-                    if (!locks.isEmpty()) {\n-                        for (DatasetLock.Reason locktype : locks) {\n-                            execCommand(new RemoveLockCommand(req, dataset, locktype));\n-                            // refresh the dataset:\n-                            dataset = findDatasetOrDie(id);\n-                        }\n-                        // kick of dataset reindexing, in case the locks removed \n-                        // affected the search card:\n-                        try {\n-                            indexService.indexDataset(dataset, true);\n-                        } catch (IOException | SolrServerException e) {\n-                            String failureLogText = \"Post lock removal indexing failed. You can kickoff a re-index of this dataset with: \\r\\n curl http://localhost:8080/api/admin/index/datasets/\" + dataset.getId().toString();\n-                            failureLogText += \"\\r\\n\" + e.getLocalizedMessage();\n-                            LoggingUtil.writeOnSuccessFailureLog(null, failureLogText, dataset);\n+        }\n \n-                        }\n-                        return ok(\"locks removed\");\n-                    }\n-                    return ok(\"dataset not locked\");\n-                }\n-                // request for a specific type lock:\n-                DatasetLock lock = dataset.getLockFor(lockType);\n-                if (lock != null) {\n-                    execCommand(new RemoveLockCommand(req, dataset, lock.getReason()));\n-                    // refresh the dataset:\n-                    dataset = findDatasetOrDie(id);\n-                    // ... and kick of dataset reindexing, in case the lock removed \n-                    // affected the search card:\n-                    try {\n-                        indexService.indexDataset(dataset, true);\n-                    } catch (IOException | SolrServerException e) {\n-                        String failureLogText = \"Post lock removal indexing failed. You can kickoff a re-index of this dataset with: \\r\\n curl http://localhost:8080/api/admin/index/datasets/\" + dataset.getId().toString();\n-                        failureLogText += \"\\r\\n\" + e.getLocalizedMessage();\n-                        LoggingUtil.writeOnSuccessFailureLog(null, failureLogText, dataset);\n+        return ok(\"dataset locked with lock type \" + lockType);\n+      } catch (WrappedResponse wr) {\n+        return wr.getResponse();\n+      }\n+\n+    });\n+  }\n+\n+  @GET\n+  @Path(\"{id}/makeDataCount/citations\")\n+  public Response getMakeDataCountCitations(@PathParam(\"id\") String idSupplied) {\n+\n+    try {\n+      Dataset dataset = findDatasetOrDie(idSupplied);\n+      JsonArrayBuilder datasetsCitations = Json.createArrayBuilder();\n+      List<DatasetExternalCitations> externalCitations =\n+        datasetExternalCitationsService.getDatasetExternalCitationsByDataset(dataset);\n+      for (DatasetExternalCitations citation : externalCitations) {\n+        JsonObjectBuilder candidateObj = Json.createObjectBuilder();\n+        /**\n+         * In the future we can imagine storing and presenting more\n+         * information about the citation such as the title of the paper\n+         * and the names of the authors. For now, we'll at least give\n+         * the URL of the citation so people can click and find out more\n+         * about the citation.\n+         */\n+        candidateObj.add(\"citationUrl\", citation.getCitedByUrl());\n+        datasetsCitations.add(candidateObj);\n+      }\n+      return ok(datasetsCitations);\n+\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n+    }\n \n-                    }\n-                    return ok(\"lock type \" + lock.getReason() + \" removed\");\n-                }\n-                return ok(\"no lock type \" + lockType + \" on the dataset\");\n-            } catch (WrappedResponse wr) {\n-                return wr.getResponse();\n-            }\n+  }\n+\n+  @GET\n+  @Path(\"{id}/makeDataCount/{metric}\")\n+  public Response getMakeDataCountMetricCurrentMonth(@PathParam(\"id\") String idSupplied,\n+                                                     @PathParam(\"metric\") String metricSupplied,\n+                                                     @QueryParam(\"country\") String country) {\n+    String nullCurrentMonth = null;\n+    return getMakeDataCountMetric(idSupplied, metricSupplied, nullCurrentMonth, country);\n+  }\n+\n+  @GET\n+  @Path(\"{identifier}/storagesize\")\n+  public Response getStorageSize(@PathParam(\"identifier\") String dvIdtf,\n+                                 @QueryParam(\"includeCached\") boolean includeCached,\n+                                 @Context UriInfo uriInfo, @Context HttpHeaders headers) throws WrappedResponse {\n+\n+    return response(req -> ok(MessageFormat.format(BundleUtil.getStringFromBundle(\"datasets.api.datasize.storage\"),\n+      execCommand(new GetDatasetStorageSizeCommand(req, findDatasetOrDie(dvIdtf), includeCached,\n+        GetDatasetStorageSizeCommand.Mode.STORAGE, null)))));\n+  }\n+\n+  @GET\n+  @Path(\"{identifier}/versions/{versionId}/downloadsize\")\n+  public Response getDownloadSize(@PathParam(\"identifier\") String dvIdtf, @PathParam(\"versionId\") String version,\n+                                  @Context UriInfo uriInfo, @Context HttpHeaders headers) throws WrappedResponse {\n+\n+    return response(req -> ok(MessageFormat.format(BundleUtil.getStringFromBundle(\"datasets.api.datasize.download\"),\n+      execCommand(new GetDatasetStorageSizeCommand(req, findDatasetOrDie(dvIdtf), false,\n+        GetDatasetStorageSizeCommand.Mode.DOWNLOAD,\n+        getDatasetVersionOrDie(req, version, findDatasetOrDie(dvIdtf), uriInfo, headers))))));\n+  }\n+\n+  @GET\n+  @Path(\"{id}/makeDataCount/{metric}/{yyyymm}\")\n+  public Response getMakeDataCountMetric(@PathParam(\"id\") String idSupplied, @PathParam(\"metric\") String metricSupplied,\n+                                         @PathParam(\"yyyymm\") String yyyymm, @QueryParam(\"country\") String country) {\n+    try {\n+      Dataset dataset = findDatasetOrDie(idSupplied);\n+      NullSafeJsonBuilder jsonObjectBuilder = jsonObjectBuilder();\n+      MakeDataCountUtil.MetricType metricType = null;\n+      try {\n+        metricType = MakeDataCountUtil.MetricType.fromString(metricSupplied);\n+      } catch (IllegalArgumentException ex) {\n+        return error(Response.Status.BAD_REQUEST, ex.getMessage());\n+      }\n+      String monthYear = null;\n+      if (yyyymm != null) {\n+        // We add \"-01\" because we store \"2018-05-01\" rather than \"2018-05\" in the \"monthyear\" column.\n+        // Dates come to us as \"2018-05-01\" in the SUSHI JSON (\"begin-date\") and we decided to store them as-is.\n+        monthYear = MetricsUtil.sanitizeYearMonthUserInput(yyyymm) + \"-01\";\n+      }\n+      if (country != null) {\n+        country = country.toLowerCase();\n+        if (!MakeDataCountUtil.isValidCountryCode(country)) {\n+          return error(Response.Status.BAD_REQUEST, \"Country must be one of the ISO 1366 Country Codes\");\n+        }\n+      }\n+      DatasetMetrics datasetMetrics =\n+        datasetMetricsSvc.getDatasetMetricsByDatasetForDisplay(dataset, monthYear, country);\n+      if (datasetMetrics == null) {\n+        return ok(\n+          \"No metrics available for dataset \" + dataset.getId() + \" for \" + yyyymm + \" for country code \" + country +\n+            \".\");\n+      } else if (datasetMetrics.getDownloadsTotal() + datasetMetrics.getViewsTotal() == 0) {\n+        return ok(\n+          \"No metrics available for dataset \" + dataset.getId() + \" for \" + yyyymm + \" for country code \" + country +\n+            \".\");\n+      }\n+      Long viewsTotalRegular = null;\n+      Long viewsUniqueRegular = null;\n+      Long downloadsTotalRegular = null;\n+      Long downloadsUniqueRegular = null;\n+      Long viewsTotalMachine = null;\n+      Long viewsUniqueMachine = null;\n+      Long downloadsTotalMachine = null;\n+      Long downloadsUniqueMachine = null;\n+      Long viewsTotal = null;\n+      Long viewsUnique = null;\n+      Long downloadsTotal = null;\n+      Long downloadsUnique = null;\n+      switch (metricSupplied) {\n+        case \"viewsTotal\":\n+          viewsTotal = datasetMetrics.getViewsTotal();\n+          break;\n+        case \"viewsTotalRegular\":\n+          viewsTotalRegular = datasetMetrics.getViewsTotalRegular();\n+          break;\n+        case \"viewsTotalMachine\":\n+          viewsTotalMachine = datasetMetrics.getViewsTotalMachine();\n+          break;\n+        case \"viewsUnique\":\n+          viewsUnique = datasetMetrics.getViewsUnique();\n+          break;\n+        case \"viewsUniqueRegular\":\n+          viewsUniqueRegular = datasetMetrics.getViewsUniqueRegular();\n+          break;\n+        case \"viewsUniqueMachine\":\n+          viewsUniqueMachine = datasetMetrics.getViewsUniqueMachine();\n+          break;\n+        case \"downloadsTotal\":\n+          downloadsTotal = datasetMetrics.getDownloadsTotal();\n+          break;\n+        case \"downloadsTotalRegular\":\n+          downloadsTotalRegular = datasetMetrics.getDownloadsTotalRegular();\n+          break;\n+        case \"downloadsTotalMachine\":\n+          downloadsTotalMachine = datasetMetrics.getDownloadsTotalMachine();\n+          break;\n+        case \"downloadsUnique\":\n+          downloadsUnique = datasetMetrics.getDownloadsUnique();\n+          break;\n+        case \"downloadsUniqueRegular\":\n+          downloadsUniqueRegular = datasetMetrics.getDownloadsUniqueRegular();\n+          break;\n+        case \"downloadsUniqueMachine\":\n+          downloadsUniqueMachine = datasetMetrics.getDownloadsUniqueMachine();\n+          break;\n+        default:\n+          break;\n+      }\n+      /**\n+       * TODO: Think more about the JSON output and the API design.\n+       * getDatasetMetricsByDatasetMonthCountry returns a single row right\n+       * now, by country. We could return multiple metrics (viewsTotal,\n+       * viewsUnique, downloadsTotal, and downloadsUnique) by country.\n+       */\n+      jsonObjectBuilder.add(\"viewsTotalRegular\", viewsTotalRegular);\n+      jsonObjectBuilder.add(\"viewsUniqueRegular\", viewsUniqueRegular);\n+      jsonObjectBuilder.add(\"downloadsTotalRegular\", downloadsTotalRegular);\n+      jsonObjectBuilder.add(\"downloadsUniqueRegular\", downloadsUniqueRegular);\n+      jsonObjectBuilder.add(\"viewsTotalMachine\", viewsTotalMachine);\n+      jsonObjectBuilder.add(\"viewsUniqueMachine\", viewsUniqueMachine);\n+      jsonObjectBuilder.add(\"downloadsTotalMachine\", downloadsTotalMachine);\n+      jsonObjectBuilder.add(\"downloadsUniqueMachine\", downloadsUniqueMachine);\n+      jsonObjectBuilder.add(\"viewsTotal\", viewsTotal);\n+      jsonObjectBuilder.add(\"viewsUnique\", viewsUnique);\n+      jsonObjectBuilder.add(\"downloadsTotal\", downloadsTotal);\n+      jsonObjectBuilder.add(\"downloadsUnique\", downloadsUnique);\n+      return ok(jsonObjectBuilder);\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n+    } catch (Exception e) {\n+      //bad date - caught in sanitize call\n+      return error(BAD_REQUEST, e.getMessage());\n+    }\n+  }\n \n-        });\n+  @GET\n+  @Path(\"{identifier}/storageDriver\")\n+  public Response getFileStore(@PathParam(\"identifier\") String dvIdtf,\n+                               @Context UriInfo uriInfo, @Context HttpHeaders headers) throws WrappedResponse {\n \n+    Dataset dataset;\n+\n+    try {\n+      dataset = findDatasetOrDie(dvIdtf);\n+    } catch (WrappedResponse ex) {\n+      return error(Response.Status.NOT_FOUND, \"No such dataset\");\n     }\n-    \n-    @POST\n-    @Path(\"{identifier}/lock/{type}\")\n-    public Response lockDataset(@PathParam(\"identifier\") String id, @PathParam(\"type\") DatasetLock.Reason lockType) {\n-        return response(req -> {\n-            try {\n-                AuthenticatedUser user = findAuthenticatedUserOrDie();\n-                if (!user.isSuperuser()) {\n-                    return error(Response.Status.FORBIDDEN, \"This API end point can be used by superusers only.\");\n-                }   \n-                Dataset dataset = findDatasetOrDie(id);\n-                DatasetLock lock = dataset.getLockFor(lockType);\n-                if (lock != null) {\n-                    return error(Response.Status.FORBIDDEN, \"dataset already locked with lock type \" + lockType);\n-                }\n-                lock = new DatasetLock(lockType, user);\n-                execCommand(new AddLockCommand(req, dataset, lock));\n-                // refresh the dataset:\n-                dataset = findDatasetOrDie(id);\n-                // ... and kick of dataset reindexing:\n-                try {\n-                    indexService.indexDataset(dataset, true);\n-                } catch (IOException | SolrServerException e) {\n-                    String failureLogText = \"Post add lock indexing failed. You can kickoff a re-index of this dataset with: \\r\\n curl http://localhost:8080/api/admin/index/datasets/\" + dataset.getId().toString();\n-                    failureLogText += \"\\r\\n\" + e.getLocalizedMessage();\n-                    LoggingUtil.writeOnSuccessFailureLog(null, failureLogText, dataset);\n \n-                }\n+    return response(req -> ok(dataset.getEffectiveStorageDriverId()));\n+  }\n+\n+  @PUT\n+  @Path(\"{identifier}/storageDriver\")\n+  public Response setFileStore(@PathParam(\"identifier\") String dvIdtf,\n+                               String storageDriverLabel,\n+                               @Context UriInfo uriInfo, @Context HttpHeaders headers) throws WrappedResponse {\n+\n+    // Superuser-only:\n+    AuthenticatedUser user;\n+    try {\n+      user = findAuthenticatedUserOrDie();\n+    } catch (WrappedResponse ex) {\n+      return error(Response.Status.BAD_REQUEST, \"Authentication is required.\");\n+    }\n+    if (!user.isSuperuser()) {\n+      return error(Response.Status.FORBIDDEN, \"Superusers only.\");\n+    }\n \n-                return ok(\"dataset locked with lock type \" + lockType);\n-            } catch (WrappedResponse wr) {\n-                return wr.getResponse();\n-            }\n+    Dataset dataset;\n \n-        });\n+    try {\n+      dataset = findDatasetOrDie(dvIdtf);\n+    } catch (WrappedResponse ex) {\n+      return error(Response.Status.NOT_FOUND, \"No such dataset\");\n     }\n-    \n-    @GET\n-    @Path(\"{id}/makeDataCount/citations\")\n-    public Response getMakeDataCountCitations(@PathParam(\"id\") String idSupplied) {\n-        \n-        try {\n-            Dataset dataset = findDatasetOrDie(idSupplied);\n-            JsonArrayBuilder datasetsCitations = Json.createArrayBuilder();\n-            List<DatasetExternalCitations> externalCitations = datasetExternalCitationsService.getDatasetExternalCitationsByDataset(dataset);\n-            for (DatasetExternalCitations citation : externalCitations ){\n-                JsonObjectBuilder candidateObj = Json.createObjectBuilder();\n-                /**\n-                 * In the future we can imagine storing and presenting more\n-                 * information about the citation such as the title of the paper\n-                 * and the names of the authors. For now, we'll at least give\n-                 * the URL of the citation so people can click and find out more\n-                 * about the citation.\n-                 */\n-                candidateObj.add(\"citationUrl\", citation.getCitedByUrl());\n-                datasetsCitations.add(candidateObj);\n-            }                       \n-            return ok(datasetsCitations); \n-            \n-        } catch (WrappedResponse wr) {\n-            return wr.getResponse();\n-        }\n \n+    // We don't want to allow setting this to a store id that does not exist:\n+    for (Entry<String, String> store : DataAccess.getStorageDriverLabels().entrySet()) {\n+      if (store.getKey().equals(storageDriverLabel)) {\n+        dataset.setStorageDriverId(store.getValue());\n+        datasetService.merge(dataset);\n+        return ok(\"Storage driver set to: \" + store.getKey() + \"/\" + store.getValue());\n+      }\n     }\n-\n-    @GET\n-    @Path(\"{id}/makeDataCount/{metric}\")\n-    public Response getMakeDataCountMetricCurrentMonth(@PathParam(\"id\") String idSupplied, @PathParam(\"metric\") String metricSupplied, @QueryParam(\"country\") String country) {\n-        String nullCurrentMonth = null;\n-        return getMakeDataCountMetric(idSupplied, metricSupplied, nullCurrentMonth, country);\n+    return error(Response.Status.BAD_REQUEST,\n+      \"No Storage Driver found for : \" + storageDriverLabel);\n+  }\n+\n+  @DELETE\n+  @Path(\"{identifier}/storageDriver\")\n+  public Response resetFileStore(@PathParam(\"identifier\") String dvIdtf,\n+                                 @Context UriInfo uriInfo, @Context HttpHeaders headers) throws WrappedResponse {\n+\n+    // Superuser-only:\n+    AuthenticatedUser user;\n+    try {\n+      user = findAuthenticatedUserOrDie();\n+    } catch (WrappedResponse ex) {\n+      return error(Response.Status.BAD_REQUEST, \"Authentication is required.\");\n     }\n-    \n-    @GET\n-    @Path(\"{identifier}/storagesize\")\n-    public Response getStorageSize(@PathParam(\"identifier\") String dvIdtf,  @QueryParam(\"includeCached\") boolean includeCached,  \n-        @Context UriInfo uriInfo, @Context HttpHeaders headers) throws WrappedResponse {       \n-      \n-        return response(req -> ok(MessageFormat.format(BundleUtil.getStringFromBundle(\"datasets.api.datasize.storage\"),\n-                execCommand(new GetDatasetStorageSizeCommand(req, findDatasetOrDie(dvIdtf), includeCached,GetDatasetStorageSizeCommand.Mode.STORAGE, null)))));\n+    if (!user.isSuperuser()) {\n+      return error(Response.Status.FORBIDDEN, \"Superusers only.\");\n     }\n-    \n-    @GET\n-    @Path(\"{identifier}/versions/{versionId}/downloadsize\")\n-    public Response getDownloadSize(@PathParam(\"identifier\") String dvIdtf, @PathParam(\"versionId\") String version,   \n-        @Context UriInfo uriInfo, @Context HttpHeaders headers) throws WrappedResponse {       \n-      \n-        return response(req -> ok(MessageFormat.format(BundleUtil.getStringFromBundle(\"datasets.api.datasize.download\"),\n-                execCommand(new GetDatasetStorageSizeCommand(req, findDatasetOrDie(dvIdtf), false, GetDatasetStorageSizeCommand.Mode.DOWNLOAD, getDatasetVersionOrDie(req, version , findDatasetOrDie(dvIdtf), uriInfo, headers))))));\n+\n+    Dataset dataset;\n+\n+    try {\n+      dataset = findDatasetOrDie(dvIdtf);\n+    } catch (WrappedResponse ex) {\n+      return error(Response.Status.NOT_FOUND, \"No such dataset\");\n     }\n \n-    @GET\n-    @Path(\"{id}/makeDataCount/{metric}/{yyyymm}\")\n-    public Response getMakeDataCountMetric(@PathParam(\"id\") String idSupplied, @PathParam(\"metric\") String metricSupplied, @PathParam(\"yyyymm\") String yyyymm, @QueryParam(\"country\") String country) {\n-        try {\n-            Dataset dataset = findDatasetOrDie(idSupplied);\n-            NullSafeJsonBuilder jsonObjectBuilder = jsonObjectBuilder();\n-            MakeDataCountUtil.MetricType metricType = null;\n-            try {\n-                metricType = MakeDataCountUtil.MetricType.fromString(metricSupplied);\n-            } catch (IllegalArgumentException ex) {\n-                return error(Response.Status.BAD_REQUEST, ex.getMessage());\n-            }\n-            String monthYear = null;\n-            if (yyyymm != null) {\n-                // We add \"-01\" because we store \"2018-05-01\" rather than \"2018-05\" in the \"monthyear\" column.\n-                // Dates come to us as \"2018-05-01\" in the SUSHI JSON (\"begin-date\") and we decided to store them as-is.\n-                monthYear = MetricsUtil.sanitizeYearMonthUserInput(yyyymm) + \"-01\";\n-            }\n-            if (country != null) {\n-                country = country.toLowerCase();\n-                if (!MakeDataCountUtil.isValidCountryCode(country)) {\n-                    return error(Response.Status.BAD_REQUEST, \"Country must be one of the ISO 1366 Country Codes\");\n-                }\n-            }\n-            DatasetMetrics datasetMetrics = datasetMetricsSvc.getDatasetMetricsByDatasetForDisplay(dataset, monthYear, country);\n-            if (datasetMetrics == null) {\n-                return ok(\"No metrics available for dataset \" + dataset.getId() + \" for \" + yyyymm + \" for country code \" + country + \".\");\n-            } else if (datasetMetrics.getDownloadsTotal() + datasetMetrics.getViewsTotal() == 0) {\n-                return ok(\"No metrics available for dataset \" + dataset.getId() + \" for \" + yyyymm + \" for country code \" + country + \".\");\n-            }\n-            Long viewsTotalRegular = null;\n-            Long viewsUniqueRegular = null;\n-            Long downloadsTotalRegular = null;\n-            Long downloadsUniqueRegular = null;\n-            Long viewsTotalMachine = null;\n-            Long viewsUniqueMachine = null;\n-            Long downloadsTotalMachine = null;\n-            Long downloadsUniqueMachine = null;\n-            Long viewsTotal = null;\n-            Long viewsUnique = null;\n-            Long downloadsTotal = null;\n-            Long downloadsUnique = null;\n-            switch (metricSupplied) {\n-                case \"viewsTotal\":\n-                    viewsTotal = datasetMetrics.getViewsTotal();\n-                    break;\n-                case \"viewsTotalRegular\":\n-                    viewsTotalRegular = datasetMetrics.getViewsTotalRegular();\n-                    break;\n-                case \"viewsTotalMachine\":\n-                    viewsTotalMachine = datasetMetrics.getViewsTotalMachine();\n-                    break;\n-                case \"viewsUnique\":\n-                    viewsUnique = datasetMetrics.getViewsUnique();\n-                    break;\n-                case \"viewsUniqueRegular\":\n-                    viewsUniqueRegular = datasetMetrics.getViewsUniqueRegular();\n-                    break;\n-                case \"viewsUniqueMachine\":\n-                    viewsUniqueMachine = datasetMetrics.getViewsUniqueMachine();\n-                    break;\n-                case \"downloadsTotal\":\n-                    downloadsTotal = datasetMetrics.getDownloadsTotal();\n-                    break;\n-                case \"downloadsTotalRegular\":\n-                    downloadsTotalRegular = datasetMetrics.getDownloadsTotalRegular();\n-                    break;\n-                case \"downloadsTotalMachine\":\n-                    downloadsTotalMachine = datasetMetrics.getDownloadsTotalMachine();\n-                    break;\n-                case \"downloadsUnique\":\n-                    downloadsUnique = datasetMetrics.getDownloadsUnique();\n-                    break;\n-                case \"downloadsUniqueRegular\":\n-                    downloadsUniqueRegular = datasetMetrics.getDownloadsUniqueRegular();\n-                    break;\n-                case \"downloadsUniqueMachine\":\n-                    downloadsUniqueMachine = datasetMetrics.getDownloadsUniqueMachine();\n-                    break;\n-                default:\n-                    break;\n-            }\n-            /**\n-             * TODO: Think more about the JSON output and the API design.\n-             * getDatasetMetricsByDatasetMonthCountry returns a single row right\n-             * now, by country. We could return multiple metrics (viewsTotal,\n-             * viewsUnique, downloadsTotal, and downloadsUnique) by country.\n-             */\n-            jsonObjectBuilder.add(\"viewsTotalRegular\", viewsTotalRegular);\n-            jsonObjectBuilder.add(\"viewsUniqueRegular\", viewsUniqueRegular);\n-            jsonObjectBuilder.add(\"downloadsTotalRegular\", downloadsTotalRegular);\n-            jsonObjectBuilder.add(\"downloadsUniqueRegular\", downloadsUniqueRegular);\n-            jsonObjectBuilder.add(\"viewsTotalMachine\", viewsTotalMachine);\n-            jsonObjectBuilder.add(\"viewsUniqueMachine\", viewsUniqueMachine);\n-            jsonObjectBuilder.add(\"downloadsTotalMachine\", downloadsTotalMachine);\n-            jsonObjectBuilder.add(\"downloadsUniqueMachine\", downloadsUniqueMachine);\n-            jsonObjectBuilder.add(\"viewsTotal\", viewsTotal);\n-            jsonObjectBuilder.add(\"viewsUnique\", viewsUnique);\n-            jsonObjectBuilder.add(\"downloadsTotal\", downloadsTotal);\n-            jsonObjectBuilder.add(\"downloadsUnique\", downloadsUnique);\n-            return ok(jsonObjectBuilder);\n-        } catch (WrappedResponse wr) {\n-            return wr.getResponse();\n-        } catch (Exception e) {\n-            //bad date - caught in sanitize call\n-            return error(BAD_REQUEST, e.getMessage());\n+    dataset.setStorageDriverId(null);\n+    datasetService.merge(dataset);\n+    return ok(\"Storage reset to default: \" + DataAccess.DEFAULT_STORAGE_DRIVER_IDENTIFIER);\n+  }\n+\n+  @GET\n+  @Path(\"{identifier}/timestamps\")\n+  @Produces(MediaType.APPLICATION_JSON)\n+  public Response getTimestamps(@PathParam(\"identifier\") String id) {\n+\n+    Dataset dataset = null;\n+    DateTimeFormatter formatter = DateTimeFormatter.ISO_LOCAL_DATE_TIME;\n+    try {\n+      dataset = findDatasetOrDie(id);\n+      User u = findUserOrDie();\n+      Set<Permission> perms = new HashSet<Permission>();\n+      perms.add(Permission.ViewUnpublishedDataset);\n+      boolean canSeeDraft = permissionSvc.hasPermissionsFor(u, dataset, perms);\n+      JsonObjectBuilder timestamps = Json.createObjectBuilder();\n+      logger.fine(\"CSD: \" + canSeeDraft);\n+      logger.fine(\"IT: \" + dataset.getIndexTime());\n+      logger.fine(\"MT: \" + dataset.getModificationTime());\n+      logger.fine(\"PIT: \" + dataset.getPermissionIndexTime());\n+      logger.fine(\"PMT: \" + dataset.getPermissionModificationTime());\n+      // Basic info if it's released\n+      if (dataset.isReleased() || canSeeDraft) {\n+        timestamps.add(\"createTime\", formatter.format(dataset.getCreateDate().toLocalDateTime()));\n+        if (dataset.getPublicationDate() != null) {\n+          timestamps.add(\"publicationTime\", formatter.format(dataset.getPublicationDate().toLocalDateTime()));\n         }\n-    }\n-    \n-    @GET\n-    @Path(\"{identifier}/storageDriver\")\n-    public Response getFileStore(@PathParam(\"identifier\") String dvIdtf,\n-            @Context UriInfo uriInfo, @Context HttpHeaders headers) throws WrappedResponse { \n-        \n-        Dataset dataset; \n-        \n-        try {\n-            dataset = findDatasetOrDie(dvIdtf);\n-        } catch (WrappedResponse ex) {\n-            return error(Response.Status.NOT_FOUND, \"No such dataset\");\n+\n+        if (dataset.getLastExportTime() != null) {\n+          timestamps.add(\"lastMetadataExportTime\",\n+            formatter.format(dataset.getLastExportTime().toInstant().atZone(ZoneId.systemDefault())));\n         }\n-            \n-        return response(req -> ok(dataset.getEffectiveStorageDriverId()));\n-    }\n-    \n-    @PUT\n-    @Path(\"{identifier}/storageDriver\")\n-    public Response setFileStore(@PathParam(\"identifier\") String dvIdtf,\n-            String storageDriverLabel,\n-            @Context UriInfo uriInfo, @Context HttpHeaders headers) throws WrappedResponse {\n-        \n-        // Superuser-only:\n-        AuthenticatedUser user;\n-        try {\n-            user = findAuthenticatedUserOrDie();\n-        } catch (WrappedResponse ex) {\n-            return error(Response.Status.BAD_REQUEST, \"Authentication is required.\");\n+\n+        if (dataset.getMostRecentMajorVersionReleaseDate() != null) {\n+          timestamps.add(\"lastMajorVersionReleaseTime\", formatter.format(\n+            dataset.getMostRecentMajorVersionReleaseDate().toInstant().atZone(ZoneId.systemDefault())));\n         }\n-        if (!user.isSuperuser()) {\n-            return error(Response.Status.FORBIDDEN, \"Superusers only.\");\n-    \t}\n-        \n-        Dataset dataset; \n-        \n-        try {\n-            dataset = findDatasetOrDie(dvIdtf);\n-        } catch (WrappedResponse ex) {\n-            return error(Response.Status.NOT_FOUND, \"No such dataset\");\n+        // If the modification/permissionmodification time is\n+        // set and the index time is null or is before the mod time, the relevant index is stale\n+        timestamps.add(\"hasStaleIndex\",\n+          (dataset.getModificationTime() != null && (dataset.getIndexTime() == null\n+            || (dataset.getIndexTime().compareTo(dataset.getModificationTime()) <= 0))) ? true\n+            : false);\n+        timestamps.add(\"hasStalePermissionIndex\",\n+          (dataset.getPermissionModificationTime() != null && (dataset.getIndexTime() == null\n+            || (dataset.getIndexTime().compareTo(dataset.getModificationTime()) <= 0))) ? true\n+            : false);\n+      }\n+      // More detail if you can see a draft\n+      if (canSeeDraft) {\n+        timestamps.add(\"lastUpdateTime\", formatter.format(dataset.getModificationTime().toLocalDateTime()));\n+        if (dataset.getIndexTime() != null) {\n+          timestamps.add(\"lastIndexTime\", formatter.format(dataset.getIndexTime().toLocalDateTime()));\n         }\n-        \n-        // We don't want to allow setting this to a store id that does not exist: \n-        for (Entry<String, String> store : DataAccess.getStorageDriverLabels().entrySet()) {\n-            if (store.getKey().equals(storageDriverLabel)) {\n-                dataset.setStorageDriverId(store.getValue());\n-                datasetService.merge(dataset);\n-                return ok(\"Storage driver set to: \" + store.getKey() + \"/\" + store.getValue());\n-            }\n+        if (dataset.getPermissionModificationTime() != null) {\n+          timestamps.add(\"lastPermissionUpdateTime\",\n+            formatter.format(dataset.getPermissionModificationTime().toLocalDateTime()));\n         }\n-    \treturn error(Response.Status.BAD_REQUEST,\n-            \"No Storage Driver found for : \" + storageDriverLabel);\n-    }\n-    \n-    @DELETE\n-    @Path(\"{identifier}/storageDriver\")\n-    public Response resetFileStore(@PathParam(\"identifier\") String dvIdtf,\n-            @Context UriInfo uriInfo, @Context HttpHeaders headers) throws WrappedResponse {\n-    \n-        // Superuser-only:\n-        AuthenticatedUser user;\n-        try {\n-            user = findAuthenticatedUserOrDie();\n-        } catch (WrappedResponse ex) {\n-            return error(Response.Status.BAD_REQUEST, \"Authentication is required.\");\n+        if (dataset.getPermissionIndexTime() != null) {\n+          timestamps.add(\"lastPermissionIndexTime\",\n+            formatter.format(dataset.getPermissionIndexTime().toLocalDateTime()));\n         }\n-        if (!user.isSuperuser()) {\n-            return error(Response.Status.FORBIDDEN, \"Superusers only.\");\n-    \t}\n-        \n-        Dataset dataset; \n-        \n-        try {\n-            dataset = findDatasetOrDie(dvIdtf);\n-        } catch (WrappedResponse ex) {\n-            return error(Response.Status.NOT_FOUND, \"No such dataset\");\n+        if (dataset.getGlobalIdCreateTime() != null) {\n+          timestamps.add(\"globalIdCreateTime\", formatter\n+            .format(dataset.getGlobalIdCreateTime().toInstant().atZone(ZoneId.systemDefault())));\n         }\n-        \n-        dataset.setStorageDriverId(null);\n-        datasetService.merge(dataset);\n-    \treturn ok(\"Storage reset to default: \" + DataAccess.DEFAULT_STORAGE_DRIVER_IDENTIFIER);\n-    }\n-\n-    @GET\n-    @Path(\"{identifier}/timestamps\")\n-    @Produces(MediaType.APPLICATION_JSON)\n-    public Response getTimestamps(@PathParam(\"identifier\") String id) {\n-\n-        Dataset dataset = null;\n-        DateTimeFormatter formatter = DateTimeFormatter.ISO_LOCAL_DATE_TIME;\n-        try {\n-            dataset = findDatasetOrDie(id);\n-            User u = findUserOrDie();\n-            Set<Permission> perms = new HashSet<Permission>();\n-            perms.add(Permission.ViewUnpublishedDataset);\n-            boolean canSeeDraft = permissionSvc.hasPermissionsFor(u, dataset, perms);\n-            JsonObjectBuilder timestamps = Json.createObjectBuilder();\n-            logger.fine(\"CSD: \" + canSeeDraft);\n-            logger.fine(\"IT: \" + dataset.getIndexTime());\n-            logger.fine(\"MT: \" + dataset.getModificationTime());\n-            logger.fine(\"PIT: \" + dataset.getPermissionIndexTime());\n-            logger.fine(\"PMT: \" + dataset.getPermissionModificationTime());\n-            // Basic info if it's released\n-            if (dataset.isReleased() || canSeeDraft) {\n-                timestamps.add(\"createTime\", formatter.format(dataset.getCreateDate().toLocalDateTime()));\n-                if (dataset.getPublicationDate() != null) {\n-                    timestamps.add(\"publicationTime\", formatter.format(dataset.getPublicationDate().toLocalDateTime()));\n-                }\n \n-                if (dataset.getLastExportTime() != null) {\n-                    timestamps.add(\"lastMetadataExportTime\",\n-                            formatter.format(dataset.getLastExportTime().toInstant().atZone(ZoneId.systemDefault())));\n-                }\n-\n-                if (dataset.getMostRecentMajorVersionReleaseDate() != null) {\n-                    timestamps.add(\"lastMajorVersionReleaseTime\", formatter.format(\n-                            dataset.getMostRecentMajorVersionReleaseDate().toInstant().atZone(ZoneId.systemDefault())));\n-                }\n-                // If the modification/permissionmodification time is\n-                // set and the index time is null or is before the mod time, the relevant index is stale\n-                timestamps.add(\"hasStaleIndex\",\n-                        (dataset.getModificationTime() != null && (dataset.getIndexTime() == null\n-                                || (dataset.getIndexTime().compareTo(dataset.getModificationTime()) <= 0))) ? true\n-                                        : false);\n-                timestamps.add(\"hasStalePermissionIndex\",\n-                        (dataset.getPermissionModificationTime() != null && (dataset.getIndexTime() == null\n-                                || (dataset.getIndexTime().compareTo(dataset.getModificationTime()) <= 0))) ? true\n-                                        : false);\n-            }\n-            // More detail if you can see a draft\n-            if (canSeeDraft) {\n-                timestamps.add(\"lastUpdateTime\", formatter.format(dataset.getModificationTime().toLocalDateTime()));\n-                if (dataset.getIndexTime() != null) {\n-                    timestamps.add(\"lastIndexTime\", formatter.format(dataset.getIndexTime().toLocalDateTime()));\n-                }\n-                if (dataset.getPermissionModificationTime() != null) {\n-                    timestamps.add(\"lastPermissionUpdateTime\",\n-                            formatter.format(dataset.getPermissionModificationTime().toLocalDateTime()));\n-                }\n-                if (dataset.getPermissionIndexTime() != null) {\n-                    timestamps.add(\"lastPermissionIndexTime\",\n-                            formatter.format(dataset.getPermissionIndexTime().toLocalDateTime()));\n-                }\n-                if (dataset.getGlobalIdCreateTime() != null) {\n-                    timestamps.add(\"globalIdCreateTime\", formatter\n-                            .format(dataset.getGlobalIdCreateTime().toInstant().atZone(ZoneId.systemDefault())));\n-                }\n-\n-            }\n-            return ok(timestamps);\n-        } catch (WrappedResponse wr) {\n-            return wr.getResponse();\n-        }\n+      }\n+      return ok(timestamps);\n+    } catch (WrappedResponse wr) {\n+      return wr.getResponse();\n     }\n+  }\n }\n",
            "diff_size": 3633
        },
        {
            "tool": "naturalize",
            "errors": null,
            "diff": null
        },
        {
            "tool": "codebuff",
            "errors": null,
            "diff": null
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "1564",
                    "column": "1",
                    "severity": "error",
                    "message": "File contains tab characters (this is the first instance).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.FileTabCharacterCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/0/Datasets.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_random/0/Datasets.java\nindex 226b4092078..8f9b71dc210 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/0/Datasets.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_random/0/Datasets.java\n@@ -1560,7 +1560,7 @@ public class Datasets extends AbstractApiBean {\n @Path(\"{id}/uploadsid\")\n @Deprecated\n public Response getUploadUrl(@PathParam(\"id\") String idSupplied) {\n-\ttry {\n+try {\n \t\tDataset dataset = findDatasetOrDie(idSupplied);\n \n \t\tboolean canUpdateDataset = false;\n",
            "diff_size": 1
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "1564",
                    "column": "1",
                    "severity": "error",
                    "message": "File contains tab characters (this is the first instance).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.FileTabCharacterCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/0/Datasets.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_three_grams/0/Datasets.java\nindex 226b4092078..1c861178fd3 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/0/Datasets.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_three_grams/0/Datasets.java\n@@ -1560,7 +1560,7 @@ public class Datasets extends AbstractApiBean {\n @Path(\"{id}/uploadsid\")\n @Deprecated\n public Response getUploadUrl(@PathParam(\"id\") String idSupplied) {\n-\ttry {\n+    try {\n \t\tDataset dataset = findDatasetOrDie(idSupplied);\n \n \t\tboolean canUpdateDataset = false;\n",
            "diff_size": 1
        }
    ],
    "repaired_by": [
        "intellij"
    ],
    "not_repaired_by": [
        "styler",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ]
}