{
    "project_name": "NationalSecurityAgency-datawave",
    "error_id": "99",
    "information": {
        "errors": [
            {
                "line": "30",
                "severity": "error",
                "message": "Accumulo non-public classes imported",
                "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
            }
        ]
    },
    "source_code": "import datawave.util.cli.PasswordConverter;\n\nimport org.apache.accumulo.core.Constants;\nimport org.apache.accumulo.core.client.AccumuloException;\nimport org.apache.accumulo.core.client.AccumuloSecurityException;\nimport org.apache.accumulo.core.client.ClientConfiguration;",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "30",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "intellij",
            "errors": [
                {
                    "line": "30",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/99/IngestJob.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/intellij/99/IngestJob.java\nindex 1f5b6fd985f..b10b3c58836 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/99/IngestJob.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/intellij/99/IngestJob.java\n@@ -122,1386 +122,1431 @@ import java.util.Set;\n  * {@code <workDir>/mapFiles} directory For live ingest the AccumuloOutputFormat is then used to apply the mutations directly to accumulo\n  */\n public class IngestJob implements Tool {\n-    \n-    public static final String DAEMON_PROCESSES_PROPERTY = \"accumulo.ingest.daemons\";\n-    public static final String REDUCE_TASKS_ARG_PREFIX = \"-mapreduce.job.reduces=\";\n-    \n-    protected boolean eventProcessingError = false;\n-    protected Logger log = Logger.getLogger(\"datawave.ingest\");\n-    private ConsoleAppender ca = new ConsoleAppender(new PatternLayout(\"%p [%c{1}] %m%n\"));\n-    \n-    protected ArrayList<String[]> confOverrides = new ArrayList<>();\n-    protected int reduceTasks = 0;\n-    protected String inputPaths = null;\n-    // inputFileLists denotes whether the files in inputPaths are the files to process, or lists of files to process (one per line).\n-    protected boolean inputFileLists = false;\n-    // inputFileListMarker if set is a marker line to look for before treating the lines as files. Useful for our flag files\n-    // where the initial part of the file is our script line and the remainder contains the actual file list\n-    protected String inputFileListMarker = null;\n-    protected String idFilterFsts = null;\n-    \n-    protected String workDir = null;\n-    protected String[] tableNames = null;\n-    protected String flagFile = null;\n-    protected String flagFileDir = null;\n-    protected String flagFilePattern = null;\n-    protected String cacheBaseDir = \"/data/BulkIngest/jobCache\";\n-    protected float markerFileReducePercentage = 0.33f;\n-    protected boolean markerFileFIFO = true;\n-    protected boolean generateMarkerFile = true;\n-    protected String pipelineId = null;\n-    protected boolean outputMutations = false;\n-    protected boolean useMapOnly = false;\n-    protected boolean useCombiner = false;\n-    protected boolean useInlineCombiner = false;\n-    protected boolean verboseCounters = false;\n-    protected boolean tableCounters = false;\n-    protected boolean fileNameCounters = true;\n-    protected boolean contextWriterCounters = false;\n-    protected boolean disableSpeculativeExecution = false;\n-    protected boolean enableBloomFilters = false;\n-    protected boolean collectDistributionStats = false;\n-    protected boolean createTables = false;\n-    protected boolean metricsOutputEnabled = true;\n-    private String metricsLabelOverride = null;\n-    protected boolean generateMapFileRowKeys = false;\n-    protected String compressionType = null;\n-    protected final Set<String> compressionTableBlackList = new HashSet<>();\n-    protected int maxRFileEntries = 0;\n-    protected long maxRFileSize = 0;\n-    @SuppressWarnings(\"rawtypes\")\n-    protected Class<? extends InputFormat> inputFormat = EventSequenceFileInputFormat.class;\n-    @SuppressWarnings(\"rawtypes\")\n-    protected Class<? extends Mapper> mapper = EventMapper.class;\n-    \n-    protected String instanceName = null;\n-    protected String zooKeepers = null;\n-    protected String userName = null;\n-    protected byte[] password = null;\n-    \n-    protected URI srcHdfs = null;\n-    protected URI destHdfs = null;\n-    protected String distCpConfDir = null;\n-    protected int distCpBandwidth = 512;\n-    protected int distCpMaxMaps = 200;\n-    protected String distCpStrategy = \"dynamic\";\n-    protected boolean deleteAfterDistCp = true;\n-    \n-    protected ArrayList<Path> jobDependencies = new ArrayList<>();\n-    \n-    protected boolean writeDirectlyToDest = false;\n-    \n-    private Configuration hadoopConfiguration;\n-    private List<Observer> jobObservers = new ArrayList<>();\n-    private JobObservable jobObservable;\n-    \n-    public static void main(String[] args) throws Exception {\n-        System.out.println(\"Running main\");\n-        System.exit(ToolRunner.run(null, new IngestJob(), args));\n-    }\n-    \n-    protected void printUsage() {\n-        System.out.println(\"Usage: \" + getClass().getSimpleName() + \" inputpath configfile configfile\");\n-        System.out.println(\"                     -user username -pass password -instance instanceName\");\n-        System.out.println(\"                     -zookeepers host[,host,host] -workDir directoryName -flagFileDir directoryName\");\n-        System.out.println(\"                     [-inputFileLists] [-inputFileListMarker marker]\");\n-        System.out.println(\"                     [-srcHdfs srcFileSystemURI] [-destHdfs destFileSystemURI]\");\n-        System.out.println(\"                     [-distCpConfDir distCpHadoopConfDir] [-distCpBandwidth bandwidth]\");\n-        System.out.println(\"                     [-distCpMaxMaps maps] [-distCpStrategy strategy]\");\n-        System.out.println(\"                     [-writeDirectlyToDest]\");\n-        System.out.println(\"                     [-createTables]\");\n-        System.out.println(\"                     [-doNotDeleteAfterDistCp]\");\n-        System.out.println(\"                     [-idFilterFsts comma-separated-list-of-files]\");\n-        System.out.println(\"                     [-inputFormat inputFormatClass]\");\n-        System.out.println(\"                     [-mapper mapperClass]\");\n-        System.out.println(\"                     [-splitsCacheTimeoutMs timeout]\");\n-        System.out.println(\"                     [-disableRefreshSplits]\");\n-        System.out.println(\"                     [-splitsCacheDir /path/to/directory]\");\n-        System.out.println(\"                     [-accumuloConfigCachePath /path/to/file]\");\n-        System.out.println(\"                     [-cacheBaseDir baseDir] [-cacheJars jar,jar,...]\");\n-        System.out.println(\"                     [-multipleNumShardsCacheDir /path/to/directory]\");\n-        System.out.println(\"                     [-skipMarkerFileGeneration] [-markerFileLIFO]\");\n-        System.out.println(\"                     [-markerFileReducePercentage float_in_0_to_1]\");\n-        System.out.println(\"                     [-pipelineId id]\");\n-        System.out.println(\"                     [-flagFile flagFile]\");\n-        System.out.println(\"                     [-flagFilePattern flagFilePattern]\");\n-        System.out.println(\"                     [-outputMutations]\");\n-        System.out.println(\"                     [-mapreduce.job.reduces=numReducers]\");\n-        System.out.println(\"                     [-disableSpeculativeExecution] [-mapOnly] [-useCombiner] [-useInlineCombiner]\");\n-        System.out.println(\"                     [-verboseCounters]\");\n-        System.out.println(\"                     [-tableCounters] [-contextWriterCounters] [-noFileNameCounters]\");\n-        System.out.println(\"                     [-generateMapFileRowKeys]\");\n-        System.out.println(\"                     [-enableBloomFilters]\");\n-        System.out.println(\"                     [-collectDistributionStats]\");\n-        System.out.println(\"                     [-ingestMetricsDisabled]\");\n-        System.out.println(\"                     [-ingestMetricsLabel label]\");\n-        System.out.println(\"                     [-compressionType lzo|gz]\");\n-        System.out.println(\"                     [-compressionTableBlackList table,table,...\");\n-        System.out.println(\"                     [-maxRFileUndeduppedEntries maxEntries]\");\n-        System.out.println(\"                     [-maxRFileUncompressedSize maxSize]\");\n-        System.out.println(\"                     [-jobObservers jobObserverClasses]\");\n-        System.out.println(\"                     [-shardedMapFiles table1=/hdfs/path/table1splits.seq[,table2=/hdfs/path/table2splits.seq] ]\");\n-    }\n-    \n-    @Override\n-    public int run(String[] args) throws Exception {\n-        \n-        Logger.getLogger(TypeRegistry.class).setLevel(Level.ALL);\n-        \n-        ca.setThreshold(Level.INFO);\n-        log.addAppender(ca);\n-        log.setLevel(Level.INFO);\n-        \n-        // Initialize the markings file helper so we get the right markings file\n-        MarkingFunctions.Factory.createMarkingFunctions();\n-        TypeRegistry.reset();\n-        \n-        // Parse the job arguments\n-        Configuration conf = parseArguments(args, this.getConf());\n-        \n-        if (conf == null) {\n-            printUsage();\n-            return -1;\n-        }\n-        \n-        updateConfWithOverrides(conf);\n-        \n-        jobObservable = new JobObservable(srcHdfs != null ? getFileSystem(conf, srcHdfs) : null);\n-        for (Observer observer : jobObservers) {\n-            this.jobObservable.addObserver(observer);\n-            if (observer instanceof Configurable) {\n-                log.info(\"Applying configuration to observer\");\n-                ((Configurable) observer).setConf(conf);\n-            }\n-        }\n-        \n-        AccumuloHelper cbHelper = new AccumuloHelper();\n-        cbHelper.setup(conf);\n-        \n-        TypeRegistry.getInstance(conf);\n-        \n-        log.info(conf.toString());\n-        log.info(String.format(\"getStrings('%s') = %s\", TypeRegistry.INGEST_DATA_TYPES, conf.get(TypeRegistry.INGEST_DATA_TYPES)));\n-        log.info(String.format(\"getStrings('data.name') = %s\", conf.get(\"data.name\")));\n-        int index = 0;\n-        for (String name : TypeRegistry.getTypeNames()) {\n-            log.info(String.format(\"name[%d] = '%s'\", index++, name));\n-        }\n-        \n-        if (TypeRegistry.getTypes().isEmpty()) {\n-            log.error(\"No data types were configured\");\n-            return -1;\n-        }\n-        \n-        TableConfigurationUtil tableConfigUtil = new TableConfigurationUtil(conf);\n-        tableNames = tableConfigUtil.getTableNames();\n-        \n-        if (createTables) {\n-            boolean wasConfigureTablesSuccessful = tableConfigUtil.configureTables(conf);\n-            if (!wasConfigureTablesSuccessful) {\n-                return -1;\n-            } else\n-                log.info(\"Created tables: \" + tableNames + \" successfully!\");\n-        }\n-        \n-        try {\n-            tableConfigUtil.serializeAggregatorConfiguration(cbHelper, conf, log);\n-        } catch (TableNotFoundException tnf) {\n-            log.error(\"One or more configured DataWave tables are missing in Accumulo. If this is a new system or if new tables have recently been introduced, run a job using the '-createTables' flag before attempting to ingest more data\",\n-                            tnf);\n-            return -1;\n-        }\n-        \n-        // get the source and output hadoop file systems\n-        FileSystem inputFs = getFileSystem(conf, srcHdfs);\n-        FileSystem outputFs = (writeDirectlyToDest ? getFileSystem(conf, destHdfs) : inputFs);\n-        conf.set(\"output.fs.uri\", outputFs.getUri().toString());\n-        \n-        // get the qualified work directory path\n-        Path unqualifiedWorkPath = Path.getPathWithoutSchemeAndAuthority(new Path(workDir));\n-        conf.set(\"ingest.work.dir.unqualified\", unqualifiedWorkPath.toString());\n-        Path workDirPath = new Path(new Path(writeDirectlyToDest ? destHdfs : srcHdfs), unqualifiedWorkPath);\n-        conf.set(\"ingest.work.dir.qualified\", workDirPath.toString());\n-        \n-        // Create the Job\n-        Job job = Job.getInstance(conf);\n-        // Job copies the configuration, so any changes made after this point don't get captured in the job.\n-        // Use the job's configuration from this point.\n-        conf = job.getConfiguration();\n-        if (!useMapOnly || !outputMutations) {\n-            // Calculate the sampled splits, splits file, and set up the partitioner, but not if only doing only a map phase and outputting mutations\n-            // if not outputting mutations and only doing a map phase, we still need to go through this logic as the MultiRFileOutputFormatter\n-            // depends on this.\n-            try {\n-                configureBulkPartitionerAndOutputFormatter(job, cbHelper, conf, outputFs);\n-            } catch (Exception e) {\n-                log.error(e);\n-                return -1;\n-            }\n-        }\n-        \n-        job.setJarByClass(this.getClass());\n-        for (Path inputPath : getFilesToProcess(inputFs, inputFileLists, inputFileListMarker, inputPaths)) {\n-            FileInputFormat.addInputPath(job, inputPath);\n-        }\n-        for (Path dependency : jobDependencies)\n-            job.addFileToClassPath(dependency);\n-        \n-        configureInputFormat(job, cbHelper, conf);\n-        \n-        configureJob(job, conf, workDirPath, outputFs);\n-        \n-        // Log configuration\n-        log.info(\"Types: \" + TypeRegistry.getTypeNames());\n-        log.info(\"Tables: \" + Arrays.toString(tableNames));\n-        log.info(\"InputFormat: \" + job.getInputFormatClass().getName());\n-        log.info(\"Mapper: \" + job.getMapperClass().getName());\n-        log.info(\"Reduce tasks: \" + (useMapOnly ? 0 : reduceTasks));\n-        log.info(\"Split File: \" + workDirPath + \"/splits.txt\");\n-        \n-        // Note that if we run any other jobs in the same vm (such as a sampler), then we may\n-        // need to catch and throw away an exception here\n-        URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory(conf));\n-        \n-        startDaemonProcesses(conf);\n-        long start = System.currentTimeMillis();\n-        job.submit();\n-        JobID jobID = job.getJobID();\n-        log.info(\"JOB ID: \" + jobID);\n-        \n-        createFileWithRetries(outputFs, new Path(workDirPath, jobID.toString()));\n-        \n-        // Wait for reduce progress to pass the 30% mark and then\n-        // kick off the next job of this type.\n-        boolean done = false;\n-        while (generateMarkerFile && !done && !job.isComplete()) {\n-            if (job.reduceProgress() > markerFileReducePercentage) {\n-                File flagDir = new File(flagFileDir);\n-                if (flagDir.isDirectory()) {\n-                    // Find flag files that start with this datatype\n-                    RegexFileFilter filter;\n-                    if (flagFilePattern != null) {\n-                        filter = new RegexFileFilter(flagFilePattern);\n-                    } else {\n-                        filter = new RegexFileFilter(outputMutations ? \".*_(live|fivemin)_.*\\\\.flag\" : \".*_(bulk|onehr)_.*\\\\.flag\");\n-                    }\n-                    File[] flagFiles = flagDir.listFiles((FilenameFilter) filter);\n-                    if (flagFiles.length > 0) {\n-                        // Reverse sort by time to get the earliest file\n-                        Comparator<File> comparator = LastModifiedFileComparator.LASTMODIFIED_COMPARATOR;\n-                        if (!markerFileFIFO) {\n-                            comparator = LastModifiedFileComparator.LASTMODIFIED_REVERSE;\n-                        }\n-                        Arrays.sort(flagFiles, comparator);\n-                        // Just grab the first one and rename it to .marker\n-                        File flag = flagFiles[0];\n-                        File targetFile = new File(flag.getAbsolutePath() + (pipelineId == null ? \"\" : '.' + pipelineId) + \".marker\");\n-                        if (!flag.renameTo(targetFile)) {\n-                            log.error(\"Unable to rename flag file: \" + flag.getAbsolutePath());\n-                            continue;\n-                        }\n-                        log.info(\"Renamed flag file \" + flag + \" to \" + targetFile);\n-                    } else {\n-                        log.info(\"No more flag files to process\");\n-                        // + datatype);\n-                    }\n-                } else {\n-                    log.error(\"Flag file directory does not exist: \" + flagFileDir);\n-                }\n-                done = true;\n-            } else {\n-                try {\n-                    Thread.sleep(3000);\n-                } catch (InterruptedException ie) {\n-                    // do nothing\n-                }\n-            }\n-        }\n-        \n-        job.waitForCompletion(true);\n-        long stop = System.currentTimeMillis();\n-        \n-        // output the counters to the log\n-        Counters counters = job.getCounters();\n-        log.info(counters);\n-        JobClient jobClient = new JobClient((org.apache.hadoop.mapred.JobConf) job.getConfiguration());\n-        RunningJob runningJob = jobClient.getJob(new org.apache.hadoop.mapred.JobID(jobID.getJtIdentifier(), jobID.getId()));\n-        \n-        // If the job failed, then don't bring the map files online.\n-        if (!job.isSuccessful()) {\n-            return jobFailed(job, runningJob, outputFs, workDirPath);\n-        }\n-        \n-        // determine if we had processing errors\n-        if (counters.findCounter(IngestProcess.RUNTIME_EXCEPTION).getValue() > 0) {\n-            eventProcessingError = true;\n-            log.error(\"Found Runtime Exceptions in the counters\");\n-            long numExceptions = counters.findCounter(IngestProcess.RUNTIME_EXCEPTION).getValue();\n-            long numRecords = counters.findCounter(IngestOutput.EVENTS_PROCESSED).getValue();\n-            long percentError = (numExceptions / (numRecords + numExceptions)) * 100;\n-            log.info(\"Percent Error: \" + percentError);\n-            if (conf.getInt(\"job.percent.error.threshold\", 101) <= percentError) {\n-                return jobFailed(job, runningJob, outputFs, workDirPath);\n-            }\n-        }\n-        if (counters.findCounter(IngestInput.EVENT_FATAL_ERROR).getValue() > 0) {\n-            eventProcessingError = true;\n-            log.error(\"Found Fatal Errors in the counters\");\n-        }\n-        \n-        // If we're doing \"live\" ingest (sending mutations to accumulo rather than\n-        // bringing map files online), then simply delete the workDir since it\n-        // doesn't contain anything we need. If we are doing bulk ingest, then\n-        // write out a marker file to indicate that the job is complete and a\n-        // separate process will bulk import the map files.\n-        if (outputMutations) {\n-            markFilesLoaded(inputFs, FileInputFormat.getInputPaths(job), job.getJobID());\n-            boolean deleted = outputFs.delete(workDirPath, true);\n-            if (!deleted) {\n-                log.error(\"Unable to remove job working directory: \" + workDirPath);\n-            }\n-        } else {\n-            // now move the job directory over to the warehouse if needed\n-            FileSystem destFs = getFileSystem(conf, destHdfs);\n-            \n-            if (!inputFs.equals(destFs) && !writeDirectlyToDest) {\n-                Configuration distCpConf = conf;\n-                // Use the configuration dir specified on the command-line for DistCP if necessary.\n-                // Basically this means pulling in all of the *-site.xml config files from the specified\n-                // directory. By adding these resources last, their properties will override those in the\n-                // current config.\n-                if (distCpConfDir != null) {\n-                    distCpConf = new Configuration(false);\n-                    FilenameFilter ff = (dir, name) -> name.toLowerCase().endsWith(\"-site.xml\");\n-                    for (String file : new File(distCpConfDir).list(ff)) {\n-                        Path path = new Path(distCpConfDir, file);\n-                        distCpConf.addResource(file.replace(\"-site\", \"-default\"));\n-                        distCpConf.addResource(path);\n-                    }\n-                }\n-                \n-                log.info(\"Moving (using distcp) \" + unqualifiedWorkPath + \" from \" + inputFs.getUri() + \" to \" + destFs.getUri());\n-                try {\n-                    distCpDirectory(unqualifiedWorkPath, inputFs, destFs, distCpConf, deleteAfterDistCp);\n-                } catch (Exception e) {\n-                    log.error(\"Failed to move job directory over to the warehouse.\", e);\n-                    return -3;\n-                }\n-            }\n-            \n-            Path destWorkDirPath = FileSystem.get(destHdfs, conf).makeQualified(unqualifiedWorkPath);\n-            boolean marked = markJobComplete(destFs, destWorkDirPath);\n-            if (!marked) {\n-                log.error(\"Failed to create marker file indicating job completion.\");\n-                return -3;\n+\n+  public static final String DAEMON_PROCESSES_PROPERTY = \"accumulo.ingest.daemons\";\n+  public static final String REDUCE_TASKS_ARG_PREFIX = \"-mapreduce.job.reduces=\";\n+\n+  protected boolean eventProcessingError = false;\n+  protected Logger log = Logger.getLogger(\"datawave.ingest\");\n+  private ConsoleAppender ca = new ConsoleAppender(new PatternLayout(\"%p [%c{1}] %m%n\"));\n+\n+  protected ArrayList<String[]> confOverrides = new ArrayList<>();\n+  protected int reduceTasks = 0;\n+  protected String inputPaths = null;\n+  // inputFileLists denotes whether the files in inputPaths are the files to process, or lists of files to process (one per line).\n+  protected boolean inputFileLists = false;\n+  // inputFileListMarker if set is a marker line to look for before treating the lines as files. Useful for our flag files\n+  // where the initial part of the file is our script line and the remainder contains the actual file list\n+  protected String inputFileListMarker = null;\n+  protected String idFilterFsts = null;\n+\n+  protected String workDir = null;\n+  protected String[] tableNames = null;\n+  protected String flagFile = null;\n+  protected String flagFileDir = null;\n+  protected String flagFilePattern = null;\n+  protected String cacheBaseDir = \"/data/BulkIngest/jobCache\";\n+  protected float markerFileReducePercentage = 0.33f;\n+  protected boolean markerFileFIFO = true;\n+  protected boolean generateMarkerFile = true;\n+  protected String pipelineId = null;\n+  protected boolean outputMutations = false;\n+  protected boolean useMapOnly = false;\n+  protected boolean useCombiner = false;\n+  protected boolean useInlineCombiner = false;\n+  protected boolean verboseCounters = false;\n+  protected boolean tableCounters = false;\n+  protected boolean fileNameCounters = true;\n+  protected boolean contextWriterCounters = false;\n+  protected boolean disableSpeculativeExecution = false;\n+  protected boolean enableBloomFilters = false;\n+  protected boolean collectDistributionStats = false;\n+  protected boolean createTables = false;\n+  protected boolean metricsOutputEnabled = true;\n+  private String metricsLabelOverride = null;\n+  protected boolean generateMapFileRowKeys = false;\n+  protected String compressionType = null;\n+  protected final Set<String> compressionTableBlackList = new HashSet<>();\n+  protected int maxRFileEntries = 0;\n+  protected long maxRFileSize = 0;\n+  @SuppressWarnings(\"rawtypes\")\n+  protected Class<? extends InputFormat> inputFormat = EventSequenceFileInputFormat.class;\n+  @SuppressWarnings(\"rawtypes\")\n+  protected Class<? extends Mapper> mapper = EventMapper.class;\n+\n+  protected String instanceName = null;\n+  protected String zooKeepers = null;\n+  protected String userName = null;\n+  protected byte[] password = null;\n+\n+  protected URI srcHdfs = null;\n+  protected URI destHdfs = null;\n+  protected String distCpConfDir = null;\n+  protected int distCpBandwidth = 512;\n+  protected int distCpMaxMaps = 200;\n+  protected String distCpStrategy = \"dynamic\";\n+  protected boolean deleteAfterDistCp = true;\n+\n+  protected ArrayList<Path> jobDependencies = new ArrayList<>();\n+\n+  protected boolean writeDirectlyToDest = false;\n+\n+  private Configuration hadoopConfiguration;\n+  private List<Observer> jobObservers = new ArrayList<>();\n+  private JobObservable jobObservable;\n+\n+  public static void main(String[] args) throws Exception {\n+    System.out.println(\"Running main\");\n+    System.exit(ToolRunner.run(null, new IngestJob(), args));\n+  }\n+\n+  protected void printUsage() {\n+    System.out.println(\"Usage: \" + getClass().getSimpleName() + \" inputpath configfile configfile\");\n+    System.out.println(\"                     -user username -pass password -instance instanceName\");\n+    System.out\n+        .println(\"                     -zookeepers host[,host,host] -workDir directoryName -flagFileDir directoryName\");\n+    System.out.println(\"                     [-inputFileLists] [-inputFileListMarker marker]\");\n+    System.out.println(\"                     [-srcHdfs srcFileSystemURI] [-destHdfs destFileSystemURI]\");\n+    System.out.println(\"                     [-distCpConfDir distCpHadoopConfDir] [-distCpBandwidth bandwidth]\");\n+    System.out.println(\"                     [-distCpMaxMaps maps] [-distCpStrategy strategy]\");\n+    System.out.println(\"                     [-writeDirectlyToDest]\");\n+    System.out.println(\"                     [-createTables]\");\n+    System.out.println(\"                     [-doNotDeleteAfterDistCp]\");\n+    System.out.println(\"                     [-idFilterFsts comma-separated-list-of-files]\");\n+    System.out.println(\"                     [-inputFormat inputFormatClass]\");\n+    System.out.println(\"                     [-mapper mapperClass]\");\n+    System.out.println(\"                     [-splitsCacheTimeoutMs timeout]\");\n+    System.out.println(\"                     [-disableRefreshSplits]\");\n+    System.out.println(\"                     [-splitsCacheDir /path/to/directory]\");\n+    System.out.println(\"                     [-accumuloConfigCachePath /path/to/file]\");\n+    System.out.println(\"                     [-cacheBaseDir baseDir] [-cacheJars jar,jar,...]\");\n+    System.out.println(\"                     [-multipleNumShardsCacheDir /path/to/directory]\");\n+    System.out.println(\"                     [-skipMarkerFileGeneration] [-markerFileLIFO]\");\n+    System.out.println(\"                     [-markerFileReducePercentage float_in_0_to_1]\");\n+    System.out.println(\"                     [-pipelineId id]\");\n+    System.out.println(\"                     [-flagFile flagFile]\");\n+    System.out.println(\"                     [-flagFilePattern flagFilePattern]\");\n+    System.out.println(\"                     [-outputMutations]\");\n+    System.out.println(\"                     [-mapreduce.job.reduces=numReducers]\");\n+    System.out\n+        .println(\"                     [-disableSpeculativeExecution] [-mapOnly] [-useCombiner] [-useInlineCombiner]\");\n+    System.out.println(\"                     [-verboseCounters]\");\n+    System.out.println(\"                     [-tableCounters] [-contextWriterCounters] [-noFileNameCounters]\");\n+    System.out.println(\"                     [-generateMapFileRowKeys]\");\n+    System.out.println(\"                     [-enableBloomFilters]\");\n+    System.out.println(\"                     [-collectDistributionStats]\");\n+    System.out.println(\"                     [-ingestMetricsDisabled]\");\n+    System.out.println(\"                     [-ingestMetricsLabel label]\");\n+    System.out.println(\"                     [-compressionType lzo|gz]\");\n+    System.out.println(\"                     [-compressionTableBlackList table,table,...\");\n+    System.out.println(\"                     [-maxRFileUndeduppedEntries maxEntries]\");\n+    System.out.println(\"                     [-maxRFileUncompressedSize maxSize]\");\n+    System.out.println(\"                     [-jobObservers jobObserverClasses]\");\n+    System.out.println(\n+        \"                     [-shardedMapFiles table1=/hdfs/path/table1splits.seq[,table2=/hdfs/path/table2splits.seq] ]\");\n+  }\n+\n+  @Override\n+  public int run(String[] args) throws Exception {\n+\n+    Logger.getLogger(TypeRegistry.class).setLevel(Level.ALL);\n+\n+    ca.setThreshold(Level.INFO);\n+    log.addAppender(ca);\n+    log.setLevel(Level.INFO);\n+\n+    // Initialize the markings file helper so we get the right markings file\n+    MarkingFunctions.Factory.createMarkingFunctions();\n+    TypeRegistry.reset();\n+\n+    // Parse the job arguments\n+    Configuration conf = parseArguments(args, this.getConf());\n+\n+    if (conf == null) {\n+      printUsage();\n+      return -1;\n+    }\n+\n+    updateConfWithOverrides(conf);\n+\n+    jobObservable = new JobObservable(srcHdfs != null ? getFileSystem(conf, srcHdfs) : null);\n+    for (Observer observer : jobObservers) {\n+      this.jobObservable.addObserver(observer);\n+      if (observer instanceof Configurable) {\n+        log.info(\"Applying configuration to observer\");\n+        ((Configurable) observer).setConf(conf);\n+      }\n+    }\n+\n+    AccumuloHelper cbHelper = new AccumuloHelper();\n+    cbHelper.setup(conf);\n+\n+    TypeRegistry.getInstance(conf);\n+\n+    log.info(conf.toString());\n+    log.info(String\n+        .format(\"getStrings('%s') = %s\", TypeRegistry.INGEST_DATA_TYPES, conf.get(TypeRegistry.INGEST_DATA_TYPES)));\n+    log.info(String.format(\"getStrings('data.name') = %s\", conf.get(\"data.name\")));\n+    int index = 0;\n+    for (String name : TypeRegistry.getTypeNames()) {\n+      log.info(String.format(\"name[%d] = '%s'\", index++, name));\n+    }\n+\n+    if (TypeRegistry.getTypes().isEmpty()) {\n+      log.error(\"No data types were configured\");\n+      return -1;\n+    }\n+\n+    TableConfigurationUtil tableConfigUtil = new TableConfigurationUtil(conf);\n+    tableNames = tableConfigUtil.getTableNames();\n+\n+    if (createTables) {\n+      boolean wasConfigureTablesSuccessful = tableConfigUtil.configureTables(conf);\n+      if (!wasConfigureTablesSuccessful) {\n+        return -1;\n+      } else {\n+        log.info(\"Created tables: \" + tableNames + \" successfully!\");\n+      }\n+    }\n+\n+    try {\n+      tableConfigUtil.serializeAggregatorConfiguration(cbHelper, conf, log);\n+    } catch (TableNotFoundException tnf) {\n+      log.error(\n+          \"One or more configured DataWave tables are missing in Accumulo. If this is a new system or if new tables have recently been introduced, run a job using the '-createTables' flag before attempting to ingest more data\",\n+          tnf);\n+      return -1;\n+    }\n+\n+    // get the source and output hadoop file systems\n+    FileSystem inputFs = getFileSystem(conf, srcHdfs);\n+    FileSystem outputFs = (writeDirectlyToDest ? getFileSystem(conf, destHdfs) : inputFs);\n+    conf.set(\"output.fs.uri\", outputFs.getUri().toString());\n+\n+    // get the qualified work directory path\n+    Path unqualifiedWorkPath = Path.getPathWithoutSchemeAndAuthority(new Path(workDir));\n+    conf.set(\"ingest.work.dir.unqualified\", unqualifiedWorkPath.toString());\n+    Path workDirPath = new Path(new Path(writeDirectlyToDest ? destHdfs : srcHdfs), unqualifiedWorkPath);\n+    conf.set(\"ingest.work.dir.qualified\", workDirPath.toString());\n+\n+    // Create the Job\n+    Job job = Job.getInstance(conf);\n+    // Job copies the configuration, so any changes made after this point don't get captured in the job.\n+    // Use the job's configuration from this point.\n+    conf = job.getConfiguration();\n+    if (!useMapOnly || !outputMutations) {\n+      // Calculate the sampled splits, splits file, and set up the partitioner, but not if only doing only a map phase and outputting mutations\n+      // if not outputting mutations and only doing a map phase, we still need to go through this logic as the MultiRFileOutputFormatter\n+      // depends on this.\n+      try {\n+        configureBulkPartitionerAndOutputFormatter(job, cbHelper, conf, outputFs);\n+      } catch (Exception e) {\n+        log.error(e);\n+        return -1;\n+      }\n+    }\n+\n+    job.setJarByClass(this.getClass());\n+    for (Path inputPath : getFilesToProcess(inputFs, inputFileLists, inputFileListMarker, inputPaths)) {\n+      FileInputFormat.addInputPath(job, inputPath);\n+    }\n+    for (Path dependency : jobDependencies) {\n+      job.addFileToClassPath(dependency);\n+    }\n+\n+    configureInputFormat(job, cbHelper, conf);\n+\n+    configureJob(job, conf, workDirPath, outputFs);\n+\n+    // Log configuration\n+    log.info(\"Types: \" + TypeRegistry.getTypeNames());\n+    log.info(\"Tables: \" + Arrays.toString(tableNames));\n+    log.info(\"InputFormat: \" + job.getInputFormatClass().getName());\n+    log.info(\"Mapper: \" + job.getMapperClass().getName());\n+    log.info(\"Reduce tasks: \" + (useMapOnly ? 0 : reduceTasks));\n+    log.info(\"Split File: \" + workDirPath + \"/splits.txt\");\n+\n+    // Note that if we run any other jobs in the same vm (such as a sampler), then we may\n+    // need to catch and throw away an exception here\n+    URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory(conf));\n+\n+    startDaemonProcesses(conf);\n+    long start = System.currentTimeMillis();\n+    job.submit();\n+    JobID jobID = job.getJobID();\n+    log.info(\"JOB ID: \" + jobID);\n+\n+    createFileWithRetries(outputFs, new Path(workDirPath, jobID.toString()));\n+\n+    // Wait for reduce progress to pass the 30% mark and then\n+    // kick off the next job of this type.\n+    boolean done = false;\n+    while (generateMarkerFile && !done && !job.isComplete()) {\n+      if (job.reduceProgress() > markerFileReducePercentage) {\n+        File flagDir = new File(flagFileDir);\n+        if (flagDir.isDirectory()) {\n+          // Find flag files that start with this datatype\n+          RegexFileFilter filter;\n+          if (flagFilePattern != null) {\n+            filter = new RegexFileFilter(flagFilePattern);\n+          } else {\n+            filter = new RegexFileFilter(outputMutations ? \".*_(live|fivemin)_.*\\\\.flag\" : \".*_(bulk|onehr)_.*\\\\.flag\");\n+          }\n+          File[] flagFiles = flagDir.listFiles((FilenameFilter) filter);\n+          if (flagFiles.length > 0) {\n+            // Reverse sort by time to get the earliest file\n+            Comparator<File> comparator = LastModifiedFileComparator.LASTMODIFIED_COMPARATOR;\n+            if (!markerFileFIFO) {\n+              comparator = LastModifiedFileComparator.LASTMODIFIED_REVERSE;\n             }\n-        }\n-        \n-        // if we had a failure writing the metrics, or we have event processing errors, then return -5\n-        // this should result in administrators getting an email, but the job will be considered successful\n-        \n-        if (metricsOutputEnabled) {\n-            log.info(\"Writing Stats\");\n-            Path statsDir = new Path(unqualifiedWorkPath.getParent(), \"IngestMetrics\");\n-            if (!writeStats(log, job, jobID, counters, start, stop, outputMutations, inputFs, statsDir, this.metricsLabelOverride)) {\n-                log.warn(\"Failed to output statistics for the job\");\n-                return -5;\n+            Arrays.sort(flagFiles, comparator);\n+            // Just grab the first one and rename it to .marker\n+            File flag = flagFiles[0];\n+            File targetFile =\n+                new File(flag.getAbsolutePath() + (pipelineId == null ? \"\" : '.' + pipelineId) + \".marker\");\n+            if (!flag.renameTo(targetFile)) {\n+              log.error(\"Unable to rename flag file: \" + flag.getAbsolutePath());\n+              continue;\n             }\n+            log.info(\"Renamed flag file \" + flag + \" to \" + targetFile);\n+          } else {\n+            log.info(\"No more flag files to process\");\n+            // + datatype);\n+          }\n         } else {\n-            log.info(\"Ingest stats output disabled via 'ingestMetricsDisabled' flag\");\n-        }\n-        \n-        if (eventProcessingError) {\n-            log.warn(\"Job had processing errors.  See counters for more information\");\n-            return -5;\n-        }\n-        \n-        return 0;\n-    }\n-    \n-    protected Configuration interpolateEnvironment(Configuration conf) {\n-        // We have set up the Configuration, now replace all instances of ${DATAWAVE_INGEST_HOME} with\n-        // the value that is set in the environment.\n-        String ingestHomeValue = System.getenv(\"DATAWAVE_INGEST_HOME\");\n-        if (null == ingestHomeValue)\n-            throw new IllegalArgumentException(\"DATAWAVE_INGEST_HOME must be set in the environment.\");\n-        \n-        log.info(\"Replacing ${DATAWAVE_INGEST_HOME} with \" + ingestHomeValue);\n-        \n-        return ConfigurationHelper.interpolate(conf, \"\\\\$\\\\{DATAWAVE_INGEST_HOME\\\\}\", ingestHomeValue);\n-        \n-    }\n-    \n-    /**\n-     * Parse the arguments and update the configuration as needed\n-     *\n-     * @param args\n-     * @param conf\n-     * @throws ClassNotFoundException\n-     * @throws URISyntaxException\n-     */\n-    protected Configuration parseArguments(String[] args, Configuration conf) throws ClassNotFoundException, URISyntaxException, IllegalArgumentException {\n-        List<String> activeResources = new ArrayList<>();\n-        \n-        inputPaths = args[0];\n-        log.info(\"InputPaths is \" + inputPaths);\n-        for (int i = 1; i < args.length; i++) {\n-            if (args[i].equals(\"-inputFileLists\")) {\n-                inputFileLists = true;\n-            } else if (args[i].equals(\"-inputFileListMarker\")) {\n-                inputFileListMarker = args[++i];\n-            } else if (args[i].equals(\"-instance\")) {\n-                instanceName = args[++i];\n-                AccumuloHelper.setInstanceName(conf, instanceName);\n-            } else if (args[i].equals(\"-zookeepers\")) {\n-                zooKeepers = args[++i];\n-                AccumuloHelper.setZooKeepers(conf, zooKeepers);\n-            } else if (args[i].equals(\"-workDir\")) {\n-                workDir = args[++i];\n-                if (!workDir.endsWith(Path.SEPARATOR)) {\n-                    workDir = workDir + Path.SEPARATOR;\n-                }\n-            } else if (args[i].equals(\"-user\")) {\n-                userName = args[++i];\n-                AccumuloHelper.setUsername(conf, userName);\n-            } else if (args[i].equals(\"-pass\")) {\n-                password = PasswordConverter.parseArg(args[++i]).getBytes();\n-                AccumuloHelper.setPassword(conf, password);\n-            } else if (args[i].equals(\"-flagFile\")) {\n-                flagFile = args[++i];\n-            } else if (args[i].equals(\"-flagFileDir\")) {\n-                flagFileDir = args[++i];\n-            } else if (args[i].equals(\"-flagFilePattern\")) {\n-                flagFilePattern = args[++i];\n-            } else if (\"-srcHdfs\".equalsIgnoreCase(args[i])) {\n-                srcHdfs = new URI(args[++i]);\n-            } else if (\"-destHdfs\".equalsIgnoreCase(args[i])) {\n-                destHdfs = new URI(args[++i]);\n-            } else if (\"-distCpConfDir\".equalsIgnoreCase(args[i])) {\n-                distCpConfDir = args[++i];\n-            } else if (\"-distCpBandwidth\".equalsIgnoreCase(args[i])) {\n-                distCpBandwidth = Integer.parseInt(args[++i]);\n-            } else if (\"-distCpMaxMaps\".equalsIgnoreCase(args[i])) {\n-                distCpMaxMaps = Integer.parseInt(args[++i]);\n-            } else if (\"-distCpStrategy\".equalsIgnoreCase(args[i])) {\n-                distCpStrategy = args[++i];\n-            } else if (\"-doNotDeleteAfterDistCp\".equalsIgnoreCase(args[i])) {\n-                deleteAfterDistCp = false;\n-            } else if (\"-writeDirectlyToDest\".equalsIgnoreCase(args[i])) {\n-                writeDirectlyToDest = true;\n-            } else if (\"-filterFsts\".equalsIgnoreCase(args[i])) {\n-                idFilterFsts = args[++i];\n-            } else if (args[i].equals(\"-inputFormat\")) {\n-                inputFormat = Class.forName(args[++i]).asSubclass(InputFormat.class);\n-            } else if (args[i].equals(\"-mapper\")) {\n-                mapper = Class.forName(args[++i]).asSubclass(Mapper.class);\n-            } else if (args[i].equals(\"-splitsCacheTimeoutMs\")) {\n-                conf.set(TableSplitsCacheStatus.SPLITS_CACHE_TIMEOUT_MS, args[++i]);\n-            } else if (args[i].equals(\"-disableRefreshSplits\")) {\n-                conf.setBoolean(TableSplitsCache.REFRESH_SPLITS, false);\n-            } else if (args[i].equals(\"-splitsCacheDir\")) {\n-                conf.set(TableSplitsCache.SPLITS_CACHE_DIR, args[++i]);\n-            } else if (args[i].equals(\"-multipleNumShardsCacheDir\")) {\n-                conf.set(NumShards.MULTIPLE_NUMSHARDS_CACHE_PATH, args[++i]);\n-            } else if (args[i].equals(\"-enableAccumuloConfigCache\")) {\n-                conf.setBoolean(TableConfigCache.ACCUMULO_CONFIG_CACHE_ENABLE_PROPERTY, true);\n-            } else if (args[i].equalsIgnoreCase(\"-accumuloConfigCachePath\")) {\n-                conf.set(TableConfigCache.ACCUMULO_CONFIG_CACHE_PATH_PROPERTY, args[++i]);\n-                conf.setBoolean(TableConfigCache.ACCUMULO_CONFIG_CACHE_ENABLE_PROPERTY, true);\n-            } else if (args[i].equals(\"-disableSpeculativeExecution\")) {\n-                disableSpeculativeExecution = true;\n-            } else if (args[i].equals(\"-skipMarkerFileGeneration\")) {\n-                generateMarkerFile = false;\n-            } else if (args[i].equals(\"-outputMutations\")) {\n-                outputMutations = true;\n-            } else if (args[i].equals(\"-mapOnly\")) {\n-                useMapOnly = true;\n-                generateMarkerFile = false;\n-            } else if (args[i].equals(\"-useCombiner\")) {\n-                useCombiner = true;\n-            } else if (args[i].equals(\"-useInlineCombiner\")) {\n-                useInlineCombiner = true;\n-            } else if (args[i].equals(\"-pipelineId\")) {\n-                pipelineId = args[++i];\n-            } else if (args[i].equals(\"-markerFileReducePercentage\")) {\n-                try {\n-                    markerFileReducePercentage = Float.parseFloat(args[++i]);\n-                } catch (NumberFormatException e) {\n-                    log.error(\"ERROR: marker file reduce percentage must be a float in [0.0,1.0]\");\n-                    return null;\n-                }\n-            } else if (args[i].equals(\"-markerFileLIFO\")) {\n-                markerFileFIFO = false;\n-            } else if (args[i].equals(\"-cacheBaseDir\")) {\n-                cacheBaseDir = args[++i];\n-            } else if (args[i].equals(\"-cacheJars\")) {\n-                String[] jars = StringUtils.trimAndRemoveEmptyStrings(args[++i].split(\"\\\\s*,\\\\s*\"));\n-                for (String jarString : jars) {\n-                    File jar = new File(jarString);\n-                    Path file = new Path(cacheBaseDir, jar.getName());\n-                    log.info(\"Adding \" + file + \" to job class path via distributed cache.\");\n-                    jobDependencies.add(file);\n-                }\n-            } else if (args[i].equals(\"-verboseCounters\")) {\n-                verboseCounters = true;\n-            } else if (args[i].equals(\"-tableCounters\")) {\n-                tableCounters = true;\n-            } else if (args[i].equals(\"-noFileNameCounters\")) {\n-                fileNameCounters = false;\n-            } else if (args[i].equals(\"-contextWriterCounters\")) {\n-                contextWriterCounters = true;\n-            } else if (args[i].equals(\"-enableBloomFilters\")) {\n-                enableBloomFilters = true;\n-            } else if (args[i].equals(\"-collectDistributionStats\")) {\n-                conf.setBoolean(MultiTableRangePartitioner.PARTITION_STATS, true);\n-            } else if (args[i].equals(\"-ingestMetricsLabel\")) {\n-                this.metricsLabelOverride = args[++i];\n-            } else if (args[i].equals(\"-ingestMetricsDisabled\")) {\n-                this.metricsOutputEnabled = false;\n-            } else if (args[i].equals(\"-generateMapFileRowKeys\")) {\n-                generateMapFileRowKeys = true;\n-            } else if (args[i].equals(\"-compressionType\")) {\n-                compressionType = args[++i];\n-            } else if (args[i].equals(\"-compressionTableBlackList\")) {\n-                String[] tables = StringUtils.split(args[++i], ',');\n-                compressionTableBlackList.addAll(Arrays.asList(tables));\n-            } else if (args[i].equals(\"-maxRFileUndeduppedEntries\")) {\n-                maxRFileEntries = Integer.parseInt(args[++i]);\n-            } else if (args[i].equals(\"-maxRFileUncompressedSize\")) {\n-                maxRFileSize = Long.parseLong(args[++i]);\n-            } else if (args[i].equals(\"-shardedMapFiles\")) {\n-                conf.set(ShardedTableMapFile.SHARDED_MAP_FILE_PATHS_RAW, args[++i]);\n-                ShardedTableMapFile.extractShardedTableMapFilePaths(conf);\n-            } else if (args[i].equals(\"-createTables\")) {\n-                createTables = true;\n-            } else if (args[i].startsWith(REDUCE_TASKS_ARG_PREFIX)) {\n-                try {\n-                    reduceTasks = Integer.parseInt(args[i].substring(REDUCE_TASKS_ARG_PREFIX.length(), args[i].length()));\n-                } catch (NumberFormatException e) {\n-                    log.error(\"ERROR: mapred.reduce.tasks must be set to an integer (\" + REDUCE_TASKS_ARG_PREFIX + \"#)\");\n-                    return null;\n-                }\n-            } else if (args[i].equals(\"-jobObservers\")) {\n-                if (i + 2 > args.length) {\n-                    log.error(\"-jobObservers must be followed by a class name\");\n-                    System.exit(-2);\n-                }\n-                String jobObserverClasses = args[++i];\n-                try {\n-                    String[] classes = jobObserverClasses.split(\",\");\n-                    for (String jobObserverClass : classes) {\n-                        log.info(\"Adding job observer: \" + jobObserverClass);\n-                        Class clazz = Class.forName(jobObserverClass);\n-                        Observer o = (Observer) clazz.newInstance();\n-                        jobObservers.add(o);\n-                    }\n-                } catch (ClassNotFoundException | IllegalAccessException | InstantiationException e) {\n-                    log.error(\"cannot instantiate job observer class '\" + jobObserverClasses + \"'\", e);\n-                    System.exit(-2);\n-                } catch (ClassCastException e) {\n-                    log.error(\"cannot cast '\" + jobObserverClasses + \"' to Observer\", e);\n-                    System.exit(-2);\n-                }\n-            } else if (args[i].startsWith(\"-\")) {\n-                // Configuration key/value entries can be overridden via the command line\n-                // (taking precedence over entries in *conf.xml files)\n-                addConfOverride(args[i].substring(1));\n-            } else {\n-                log.info(\"Adding resource \" + args[i]);\n-                conf.addResource(args[i]);\n-                activeResources.add(args[i]);\n-            }\n+          log.error(\"Flag file directory does not exist: \" + flagFileDir);\n         }\n-        \n-        conf = interpolateEnvironment(conf);\n-        \n-        for (String resource : activeResources) {\n-            conf.addResource(resource);\n+        done = true;\n+      } else {\n+        try {\n+          Thread.sleep(3000);\n+        } catch (InterruptedException ie) {\n+          // do nothing\n         }\n-        \n-        // To enable passing the MONITOR_SERVER_HOME environment variable through to the monitor,\n-        // pull it into the configuration\n-        String monitorHostValue = System.getenv(\"MONITOR_SERVER_HOST\");\n-        log.info(\"Setting MONITOR_SERVER_HOST to \" + monitorHostValue);\n-        if (null != monitorHostValue) {\n-            conf.set(\"MONITOR_SERVER_HOST\", monitorHostValue);\n+      }\n+    }\n+\n+    job.waitForCompletion(true);\n+    long stop = System.currentTimeMillis();\n+\n+    // output the counters to the log\n+    Counters counters = job.getCounters();\n+    log.info(counters);\n+    JobClient jobClient = new JobClient((org.apache.hadoop.mapred.JobConf) job.getConfiguration());\n+    RunningJob runningJob =\n+        jobClient.getJob(new org.apache.hadoop.mapred.JobID(jobID.getJtIdentifier(), jobID.getId()));\n+\n+    // If the job failed, then don't bring the map files online.\n+    if (!job.isSuccessful()) {\n+      return jobFailed(job, runningJob, outputFs, workDirPath);\n+    }\n+\n+    // determine if we had processing errors\n+    if (counters.findCounter(IngestProcess.RUNTIME_EXCEPTION).getValue() > 0) {\n+      eventProcessingError = true;\n+      log.error(\"Found Runtime Exceptions in the counters\");\n+      long numExceptions = counters.findCounter(IngestProcess.RUNTIME_EXCEPTION).getValue();\n+      long numRecords = counters.findCounter(IngestOutput.EVENTS_PROCESSED).getValue();\n+      long percentError = (numExceptions / (numRecords + numExceptions)) * 100;\n+      log.info(\"Percent Error: \" + percentError);\n+      if (conf.getInt(\"job.percent.error.threshold\", 101) <= percentError) {\n+        return jobFailed(job, runningJob, outputFs, workDirPath);\n+      }\n+    }\n+    if (counters.findCounter(IngestInput.EVENT_FATAL_ERROR).getValue() > 0) {\n+      eventProcessingError = true;\n+      log.error(\"Found Fatal Errors in the counters\");\n+    }\n+\n+    // If we're doing \"live\" ingest (sending mutations to accumulo rather than\n+    // bringing map files online), then simply delete the workDir since it\n+    // doesn't contain anything we need. If we are doing bulk ingest, then\n+    // write out a marker file to indicate that the job is complete and a\n+    // separate process will bulk import the map files.\n+    if (outputMutations) {\n+      markFilesLoaded(inputFs, FileInputFormat.getInputPaths(job), job.getJobID());\n+      boolean deleted = outputFs.delete(workDirPath, true);\n+      if (!deleted) {\n+        log.error(\"Unable to remove job working directory: \" + workDirPath);\n+      }\n+    } else {\n+      // now move the job directory over to the warehouse if needed\n+      FileSystem destFs = getFileSystem(conf, destHdfs);\n+\n+      if (!inputFs.equals(destFs) && !writeDirectlyToDest) {\n+        Configuration distCpConf = conf;\n+        // Use the configuration dir specified on the command-line for DistCP if necessary.\n+        // Basically this means pulling in all of the *-site.xml config files from the specified\n+        // directory. By adding these resources last, their properties will override those in the\n+        // current config.\n+        if (distCpConfDir != null) {\n+          distCpConf = new Configuration(false);\n+          FilenameFilter ff = (dir, name) -> name.toLowerCase().endsWith(\"-site.xml\");\n+          for (String file : new File(distCpConfDir).list(ff)) {\n+            Path path = new Path(distCpConfDir, file);\n+            distCpConf.addResource(file.replace(\"-site\", \"-default\"));\n+            distCpConf.addResource(path);\n+          }\n         }\n-        \n-        if (workDir == null) {\n-            log.error(\"ERROR: Must provide a working directory name\");\n-            return null;\n+\n+        log.info(\n+            \"Moving (using distcp) \" + unqualifiedWorkPath + \" from \" + inputFs.getUri() + \" to \" + destFs.getUri());\n+        try {\n+          distCpDirectory(unqualifiedWorkPath, inputFs, destFs, distCpConf, deleteAfterDistCp);\n+        } catch (Exception e) {\n+          log.error(\"Failed to move job directory over to the warehouse.\", e);\n+          return -3;\n         }\n-        \n-        if ((!useMapOnly) && (reduceTasks == 0)) {\n-            log.error(\"ERROR: -mapred.reduce.tasks must be set\");\n-            return null;\n+      }\n+\n+      Path destWorkDirPath = FileSystem.get(destHdfs, conf).makeQualified(unqualifiedWorkPath);\n+      boolean marked = markJobComplete(destFs, destWorkDirPath);\n+      if (!marked) {\n+        log.error(\"Failed to create marker file indicating job completion.\");\n+        return -3;\n+      }\n+    }\n+\n+    // if we had a failure writing the metrics, or we have event processing errors, then return -5\n+    // this should result in administrators getting an email, but the job will be considered successful\n+\n+    if (metricsOutputEnabled) {\n+      log.info(\"Writing Stats\");\n+      Path statsDir = new Path(unqualifiedWorkPath.getParent(), \"IngestMetrics\");\n+      if (!writeStats(log, job, jobID, counters, start, stop, outputMutations, inputFs, statsDir,\n+          this.metricsLabelOverride)) {\n+        log.warn(\"Failed to output statistics for the job\");\n+        return -5;\n+      }\n+    } else {\n+      log.info(\"Ingest stats output disabled via 'ingestMetricsDisabled' flag\");\n+    }\n+\n+    if (eventProcessingError) {\n+      log.warn(\"Job had processing errors.  See counters for more information\");\n+      return -5;\n+    }\n+\n+    return 0;\n+  }\n+\n+  protected Configuration interpolateEnvironment(Configuration conf) {\n+    // We have set up the Configuration, now replace all instances of ${DATAWAVE_INGEST_HOME} with\n+    // the value that is set in the environment.\n+    String ingestHomeValue = System.getenv(\"DATAWAVE_INGEST_HOME\");\n+    if (null == ingestHomeValue) {\n+      throw new IllegalArgumentException(\"DATAWAVE_INGEST_HOME must be set in the environment.\");\n+    }\n+\n+    log.info(\"Replacing ${DATAWAVE_INGEST_HOME} with \" + ingestHomeValue);\n+\n+    return ConfigurationHelper.interpolate(conf, \"\\\\$\\\\{DATAWAVE_INGEST_HOME\\\\}\", ingestHomeValue);\n+\n+  }\n+\n+  /**\n+   * Parse the arguments and update the configuration as needed\n+   *\n+   * @param args\n+   * @param conf\n+   * @throws ClassNotFoundException\n+   * @throws URISyntaxException\n+   */\n+  protected Configuration parseArguments(String[] args, Configuration conf)\n+      throws ClassNotFoundException, URISyntaxException, IllegalArgumentException {\n+    List<String> activeResources = new ArrayList<>();\n+\n+    inputPaths = args[0];\n+    log.info(\"InputPaths is \" + inputPaths);\n+    for (int i = 1; i < args.length; i++) {\n+      if (args[i].equals(\"-inputFileLists\")) {\n+        inputFileLists = true;\n+      } else if (args[i].equals(\"-inputFileListMarker\")) {\n+        inputFileListMarker = args[++i];\n+      } else if (args[i].equals(\"-instance\")) {\n+        instanceName = args[++i];\n+        AccumuloHelper.setInstanceName(conf, instanceName);\n+      } else if (args[i].equals(\"-zookeepers\")) {\n+        zooKeepers = args[++i];\n+        AccumuloHelper.setZooKeepers(conf, zooKeepers);\n+      } else if (args[i].equals(\"-workDir\")) {\n+        workDir = args[++i];\n+        if (!workDir.endsWith(Path.SEPARATOR)) {\n+          workDir = workDir + Path.SEPARATOR;\n         }\n-        \n-        if (flagFileDir == null && generateMarkerFile) {\n-            log.error(\"ERROR: -flagFileDir must be set\");\n-            return null;\n+      } else if (args[i].equals(\"-user\")) {\n+        userName = args[++i];\n+        AccumuloHelper.setUsername(conf, userName);\n+      } else if (args[i].equals(\"-pass\")) {\n+        password = PasswordConverter.parseArg(args[++i]).getBytes();\n+        AccumuloHelper.setPassword(conf, password);\n+      } else if (args[i].equals(\"-flagFile\")) {\n+        flagFile = args[++i];\n+      } else if (args[i].equals(\"-flagFileDir\")) {\n+        flagFileDir = args[++i];\n+      } else if (args[i].equals(\"-flagFilePattern\")) {\n+        flagFilePattern = args[++i];\n+      } else if (\"-srcHdfs\".equalsIgnoreCase(args[i])) {\n+        srcHdfs = new URI(args[++i]);\n+      } else if (\"-destHdfs\".equalsIgnoreCase(args[i])) {\n+        destHdfs = new URI(args[++i]);\n+      } else if (\"-distCpConfDir\".equalsIgnoreCase(args[i])) {\n+        distCpConfDir = args[++i];\n+      } else if (\"-distCpBandwidth\".equalsIgnoreCase(args[i])) {\n+        distCpBandwidth = Integer.parseInt(args[++i]);\n+      } else if (\"-distCpMaxMaps\".equalsIgnoreCase(args[i])) {\n+        distCpMaxMaps = Integer.parseInt(args[++i]);\n+      } else if (\"-distCpStrategy\".equalsIgnoreCase(args[i])) {\n+        distCpStrategy = args[++i];\n+      } else if (\"-doNotDeleteAfterDistCp\".equalsIgnoreCase(args[i])) {\n+        deleteAfterDistCp = false;\n+      } else if (\"-writeDirectlyToDest\".equalsIgnoreCase(args[i])) {\n+        writeDirectlyToDest = true;\n+      } else if (\"-filterFsts\".equalsIgnoreCase(args[i])) {\n+        idFilterFsts = args[++i];\n+      } else if (args[i].equals(\"-inputFormat\")) {\n+        inputFormat = Class.forName(args[++i]).asSubclass(InputFormat.class);\n+      } else if (args[i].equals(\"-mapper\")) {\n+        mapper = Class.forName(args[++i]).asSubclass(Mapper.class);\n+      } else if (args[i].equals(\"-splitsCacheTimeoutMs\")) {\n+        conf.set(TableSplitsCacheStatus.SPLITS_CACHE_TIMEOUT_MS, args[++i]);\n+      } else if (args[i].equals(\"-disableRefreshSplits\")) {\n+        conf.setBoolean(TableSplitsCache.REFRESH_SPLITS, false);\n+      } else if (args[i].equals(\"-splitsCacheDir\")) {\n+        conf.set(TableSplitsCache.SPLITS_CACHE_DIR, args[++i]);\n+      } else if (args[i].equals(\"-multipleNumShardsCacheDir\")) {\n+        conf.set(NumShards.MULTIPLE_NUMSHARDS_CACHE_PATH, args[++i]);\n+      } else if (args[i].equals(\"-enableAccumuloConfigCache\")) {\n+        conf.setBoolean(TableConfigCache.ACCUMULO_CONFIG_CACHE_ENABLE_PROPERTY, true);\n+      } else if (args[i].equalsIgnoreCase(\"-accumuloConfigCachePath\")) {\n+        conf.set(TableConfigCache.ACCUMULO_CONFIG_CACHE_PATH_PROPERTY, args[++i]);\n+        conf.setBoolean(TableConfigCache.ACCUMULO_CONFIG_CACHE_ENABLE_PROPERTY, true);\n+      } else if (args[i].equals(\"-disableSpeculativeExecution\")) {\n+        disableSpeculativeExecution = true;\n+      } else if (args[i].equals(\"-skipMarkerFileGeneration\")) {\n+        generateMarkerFile = false;\n+      } else if (args[i].equals(\"-outputMutations\")) {\n+        outputMutations = true;\n+      } else if (args[i].equals(\"-mapOnly\")) {\n+        useMapOnly = true;\n+        generateMarkerFile = false;\n+      } else if (args[i].equals(\"-useCombiner\")) {\n+        useCombiner = true;\n+      } else if (args[i].equals(\"-useInlineCombiner\")) {\n+        useInlineCombiner = true;\n+      } else if (args[i].equals(\"-pipelineId\")) {\n+        pipelineId = args[++i];\n+      } else if (args[i].equals(\"-markerFileReducePercentage\")) {\n+        try {\n+          markerFileReducePercentage = Float.parseFloat(args[++i]);\n+        } catch (NumberFormatException e) {\n+          log.error(\"ERROR: marker file reduce percentage must be a float in [0.0,1.0]\");\n+          return null;\n         }\n-        \n-        if (useMapOnly && !outputMutations) {\n-            log.error(\"ERROR: Cannot do bulk ingest mapOnly (i.e. without the reduce phase).  Bulk ingest required sorted keys.\");\n-            return null;\n+      } else if (args[i].equals(\"-markerFileLIFO\")) {\n+        markerFileFIFO = false;\n+      } else if (args[i].equals(\"-cacheBaseDir\")) {\n+        cacheBaseDir = args[++i];\n+      } else if (args[i].equals(\"-cacheJars\")) {\n+        String[] jars = StringUtils.trimAndRemoveEmptyStrings(args[++i].split(\"\\\\s*,\\\\s*\"));\n+        for (String jarString : jars) {\n+          File jar = new File(jarString);\n+          Path file = new Path(cacheBaseDir, jar.getName());\n+          log.info(\"Adding \" + file + \" to job class path via distributed cache.\");\n+          jobDependencies.add(file);\n         }\n-        \n-        if (!outputMutations && destHdfs == null) {\n-            log.error(\"ERROR: -destHdfs must be specified for bulk ingest\");\n-            return null;\n+      } else if (args[i].equals(\"-verboseCounters\")) {\n+        verboseCounters = true;\n+      } else if (args[i].equals(\"-tableCounters\")) {\n+        tableCounters = true;\n+      } else if (args[i].equals(\"-noFileNameCounters\")) {\n+        fileNameCounters = false;\n+      } else if (args[i].equals(\"-contextWriterCounters\")) {\n+        contextWriterCounters = true;\n+      } else if (args[i].equals(\"-enableBloomFilters\")) {\n+        enableBloomFilters = true;\n+      } else if (args[i].equals(\"-collectDistributionStats\")) {\n+        conf.setBoolean(MultiTableRangePartitioner.PARTITION_STATS, true);\n+      } else if (args[i].equals(\"-ingestMetricsLabel\")) {\n+        this.metricsLabelOverride = args[++i];\n+      } else if (args[i].equals(\"-ingestMetricsDisabled\")) {\n+        this.metricsOutputEnabled = false;\n+      } else if (args[i].equals(\"-generateMapFileRowKeys\")) {\n+        generateMapFileRowKeys = true;\n+      } else if (args[i].equals(\"-compressionType\")) {\n+        compressionType = args[++i];\n+      } else if (args[i].equals(\"-compressionTableBlackList\")) {\n+        String[] tables = StringUtils.split(args[++i], ',');\n+        compressionTableBlackList.addAll(Arrays.asList(tables));\n+      } else if (args[i].equals(\"-maxRFileUndeduppedEntries\")) {\n+        maxRFileEntries = Integer.parseInt(args[++i]);\n+      } else if (args[i].equals(\"-maxRFileUncompressedSize\")) {\n+        maxRFileSize = Long.parseLong(args[++i]);\n+      } else if (args[i].equals(\"-shardedMapFiles\")) {\n+        conf.set(ShardedTableMapFile.SHARDED_MAP_FILE_PATHS_RAW, args[++i]);\n+        ShardedTableMapFile.extractShardedTableMapFilePaths(conf);\n+      } else if (args[i].equals(\"-createTables\")) {\n+        createTables = true;\n+      } else if (args[i].startsWith(REDUCE_TASKS_ARG_PREFIX)) {\n+        try {\n+          reduceTasks = Integer.parseInt(args[i].substring(REDUCE_TASKS_ARG_PREFIX.length(), args[i].length()));\n+        } catch (NumberFormatException e) {\n+          log.error(\"ERROR: mapred.reduce.tasks must be set to an integer (\" + REDUCE_TASKS_ARG_PREFIX + \"#)\");\n+          return null;\n         }\n-        \n-        return conf;\n-    }\n-    \n-    /**\n-     * Configure the partitioner and the output formatter.\n-     *\n-     * @param job\n-     * @param cbHelper\n-     * @param conf\n-     * @param outputFs\n-     * @throws AccumuloSecurityException\n-     * @throws AccumuloException\n-     * @throws IOException\n-     * @throws URISyntaxException\n-     * @throws TableExistsException\n-     * @throws TableNotFoundException\n-     */\n-    protected void configureBulkPartitionerAndOutputFormatter(Job job, AccumuloHelper cbHelper, Configuration conf, FileSystem outputFs)\n-                    throws AccumuloSecurityException, AccumuloException, IOException, URISyntaxException, TableExistsException, TableNotFoundException {\n-        if (null == conf.get(\"split.work.dir\")) {\n-            conf.set(\"split.work.dir\", conf.get(\"ingest.work.dir.qualified\"));\n+      } else if (args[i].equals(\"-jobObservers\")) {\n+        if (i + 2 > args.length) {\n+          log.error(\"-jobObservers must be followed by a class name\");\n+          System.exit(-2);\n         }\n-        conf.setInt(\"splits.num.reduce\", this.reduceTasks);\n-        // used by the output formatter and the sharded partitioner\n-        ShardedTableMapFile.setupFile(conf);\n-        \n-        conf.setInt(MultiRFileOutputFormatter.EVENT_PARTITION_COUNT, this.reduceTasks * 2);\n-        configureMultiRFileOutputFormatter(conf, compressionType, compressionTableBlackList, maxRFileEntries, maxRFileSize, generateMapFileRowKeys);\n-        \n-        DelegatingPartitioner.configurePartitioner(job, conf, tableNames); // sets the partitioner\n-    }\n-    \n-    protected void configureInputFormat(Job job, AccumuloHelper cbHelper, Configuration conf) throws Exception {\n-        // see if we need to do any accumulo setup on the input format class (initial for EventProcessingErrorTableFileInputFormat)\n-        if (inputFormat != null) {\n-            try {\n-                log.info(\"Looking for \" + inputFormat.getName() + \".setup(\" + JobContext.class.getName() + \", \" + AccumuloHelper.class.getName() + \")\");\n-                Method setup = inputFormat.getMethod(\"setup\", JobContext.class, AccumuloHelper.class);\n-                log.info(\"Calling \" + inputFormat.getName() + \".setup(\" + JobContext.class.getName() + \", \" + AccumuloHelper.class.getName() + \")\");\n-                setup.invoke(null, job, cbHelper);\n-            } catch (NoSuchMethodException nsme) {\n-                // no problem, nothing to call\n-            }\n-            job.setInputFormatClass(inputFormat);\n+        String jobObserverClasses = args[++i];\n+        try {\n+          String[] classes = jobObserverClasses.split(\",\");\n+          for (String jobObserverClass : classes) {\n+            log.info(\"Adding job observer: \" + jobObserverClass);\n+            Class clazz = Class.forName(jobObserverClass);\n+            Observer o = (Observer) clazz.newInstance();\n+            jobObservers.add(o);\n+          }\n+        } catch (ClassNotFoundException | IllegalAccessException | InstantiationException e) {\n+          log.error(\"cannot instantiate job observer class '\" + jobObserverClasses + \"'\", e);\n+          System.exit(-2);\n+        } catch (ClassCastException e) {\n+          log.error(\"cannot cast '\" + jobObserverClasses + \"' to Observer\", e);\n+          System.exit(-2);\n         }\n+      } else if (args[i].startsWith(\"-\")) {\n+        // Configuration key/value entries can be overridden via the command line\n+        // (taking precedence over entries in *conf.xml files)\n+        addConfOverride(args[i].substring(1));\n+      } else {\n+        log.info(\"Adding resource \" + args[i]);\n+        conf.addResource(args[i]);\n+        activeResources.add(args[i]);\n+      }\n     }\n-    \n-    protected void configureJob(Job job, Configuration conf, Path workDirPath, FileSystem outputFs) throws Exception {\n-        // create a job name\n-        SimpleDateFormat format = new SimpleDateFormat(\"yyyyMMddHHmmss.SSS\");\n-        job.setJobName(IngestJob.class.getSimpleName() + \"_\" + format.format(new Date()));\n-        \n-        // if doing this as a bulk job, create the job.paths file and the flag file if supplied\n-        if (!outputMutations) {\n-            writeInputPathsFile(outputFs, workDirPath, FileInputFormat.getInputPaths(job));\n-            if (flagFile != null) {\n-                writeFlagFile(outputFs, workDirPath, flagFile);\n-            }\n-        }\n-        \n-        // Setup the Mapper\n-        job.setMapperClass(mapper);\n-        job.setSortComparatorClass(BulkIngestKey.Comparator.class);\n-        \n-        if (idFilterFsts != null) {\n-            job.getConfiguration().set(EventMapper.ID_FILTER_FSTS, idFilterFsts);\n-        }\n-        \n-        // Setup the context writer counters boolean\n-        job.getConfiguration().setBoolean(AbstractContextWriter.CONTEXT_WRITER_COUNTERS, contextWriterCounters);\n-        job.getConfiguration().setBoolean(EventMapper.FILE_NAME_COUNTERS, fileNameCounters);\n-        \n-        // if we are using the combiner, then ensure the flag is set\n-        if (useCombiner || useInlineCombiner) {\n-            job.getConfiguration().setBoolean(BulkIngestKeyDedupeCombiner.USING_COMBINER, true);\n-            if (useCombiner && useInlineCombiner) {\n-                log.warn(\"Using both an inline combiner AND a map-reduce combiner...perhaps only one is needed\");\n-            } else if (useCombiner) {\n-                log.info(\"Using a combiner.  Consider using 'useInlineCombiner' instead\");\n-            } else {\n-                log.info(\"Using an inline combiner\");\n-            }\n+\n+    conf = interpolateEnvironment(conf);\n+\n+    for (String resource : activeResources) {\n+      conf.addResource(resource);\n+    }\n+\n+    // To enable passing the MONITOR_SERVER_HOME environment variable through to the monitor,\n+    // pull it into the configuration\n+    String monitorHostValue = System.getenv(\"MONITOR_SERVER_HOST\");\n+    log.info(\"Setting MONITOR_SERVER_HOST to \" + monitorHostValue);\n+    if (null != monitorHostValue) {\n+      conf.set(\"MONITOR_SERVER_HOST\", monitorHostValue);\n+    }\n+\n+    if (workDir == null) {\n+      log.error(\"ERROR: Must provide a working directory name\");\n+      return null;\n+    }\n+\n+    if ((!useMapOnly) && (reduceTasks == 0)) {\n+      log.error(\"ERROR: -mapred.reduce.tasks must be set\");\n+      return null;\n+    }\n+\n+    if (flagFileDir == null && generateMarkerFile) {\n+      log.error(\"ERROR: -flagFileDir must be set\");\n+      return null;\n+    }\n+\n+    if (useMapOnly && !outputMutations) {\n+      log.error(\n+          \"ERROR: Cannot do bulk ingest mapOnly (i.e. without the reduce phase).  Bulk ingest required sorted keys.\");\n+      return null;\n+    }\n+\n+    if (!outputMutations && destHdfs == null) {\n+      log.error(\"ERROR: -destHdfs must be specified for bulk ingest\");\n+      return null;\n+    }\n+\n+    return conf;\n+  }\n+\n+  /**\n+   * Configure the partitioner and the output formatter.\n+   *\n+   * @param job\n+   * @param cbHelper\n+   * @param conf\n+   * @param outputFs\n+   * @throws AccumuloSecurityException\n+   * @throws AccumuloException\n+   * @throws IOException\n+   * @throws URISyntaxException\n+   * @throws TableExistsException\n+   * @throws TableNotFoundException\n+   */\n+  protected void configureBulkPartitionerAndOutputFormatter(Job job, AccumuloHelper cbHelper, Configuration conf,\n+                                                            FileSystem outputFs)\n+      throws AccumuloSecurityException, AccumuloException, IOException, URISyntaxException, TableExistsException,\n+      TableNotFoundException {\n+    if (null == conf.get(\"split.work.dir\")) {\n+      conf.set(\"split.work.dir\", conf.get(\"ingest.work.dir.qualified\"));\n+    }\n+    conf.setInt(\"splits.num.reduce\", this.reduceTasks);\n+    // used by the output formatter and the sharded partitioner\n+    ShardedTableMapFile.setupFile(conf);\n+\n+    conf.setInt(MultiRFileOutputFormatter.EVENT_PARTITION_COUNT, this.reduceTasks * 2);\n+    configureMultiRFileOutputFormatter(conf, compressionType, compressionTableBlackList, maxRFileEntries, maxRFileSize,\n+        generateMapFileRowKeys);\n+\n+    DelegatingPartitioner.configurePartitioner(job, conf, tableNames); // sets the partitioner\n+  }\n+\n+  protected void configureInputFormat(Job job, AccumuloHelper cbHelper, Configuration conf) throws Exception {\n+    // see if we need to do any accumulo setup on the input format class (initial for EventProcessingErrorTableFileInputFormat)\n+    if (inputFormat != null) {\n+      try {\n+        log.info(\"Looking for \" + inputFormat.getName() + \".setup(\" + JobContext.class.getName() + \", \" +\n+            AccumuloHelper.class.getName() + \")\");\n+        Method setup = inputFormat.getMethod(\"setup\", JobContext.class, AccumuloHelper.class);\n+        log.info(\"Calling \" + inputFormat.getName() + \".setup(\" + JobContext.class.getName() + \", \" +\n+            AccumuloHelper.class.getName() + \")\");\n+        setup.invoke(null, job, cbHelper);\n+      } catch (NoSuchMethodException nsme) {\n+        // no problem, nothing to call\n+      }\n+      job.setInputFormatClass(inputFormat);\n+    }\n+  }\n+\n+  protected void configureJob(Job job, Configuration conf, Path workDirPath, FileSystem outputFs) throws Exception {\n+    // create a job name\n+    SimpleDateFormat format = new SimpleDateFormat(\"yyyyMMddHHmmss.SSS\");\n+    job.setJobName(IngestJob.class.getSimpleName() + \"_\" + format.format(new Date()));\n+\n+    // if doing this as a bulk job, create the job.paths file and the flag file if supplied\n+    if (!outputMutations) {\n+      writeInputPathsFile(outputFs, workDirPath, FileInputFormat.getInputPaths(job));\n+      if (flagFile != null) {\n+        writeFlagFile(outputFs, workDirPath, flagFile);\n+      }\n+    }\n+\n+    // Setup the Mapper\n+    job.setMapperClass(mapper);\n+    job.setSortComparatorClass(BulkIngestKey.Comparator.class);\n+\n+    if (idFilterFsts != null) {\n+      job.getConfiguration().set(EventMapper.ID_FILTER_FSTS, idFilterFsts);\n+    }\n+\n+    // Setup the context writer counters boolean\n+    job.getConfiguration().setBoolean(AbstractContextWriter.CONTEXT_WRITER_COUNTERS, contextWriterCounters);\n+    job.getConfiguration().setBoolean(EventMapper.FILE_NAME_COUNTERS, fileNameCounters);\n+\n+    // if we are using the combiner, then ensure the flag is set\n+    if (useCombiner || useInlineCombiner) {\n+      job.getConfiguration().setBoolean(BulkIngestKeyDedupeCombiner.USING_COMBINER, true);\n+      if (useCombiner && useInlineCombiner) {\n+        log.warn(\"Using both an inline combiner AND a map-reduce combiner...perhaps only one is needed\");\n+      } else if (useCombiner) {\n+        log.info(\"Using a combiner.  Consider using 'useInlineCombiner' instead\");\n+      } else {\n+        log.info(\"Using an inline combiner\");\n+      }\n+    }\n+\n+    // Setup the job output and reducer classes\n+    if (outputMutations) {\n+      job.setOutputKeyClass(Text.class);\n+      job.setOutputValueClass(Mutation.class);\n+\n+      if (!useMapOnly) {\n+        job.setMapOutputKeyClass(BulkIngestKey.class);\n+        job.setMapOutputValueClass(Value.class);\n+\n+        if (useCombiner) {\n+          // Dedupe combiner will remove dupes and reset timestamps\n+          // Note: to guarantee the combiner runs we need the min combine splits to be 1\n+          job.getConfiguration().setInt(\"min.num.spills.for.combine\", 1);\n+          job.getConfiguration()\n+              .setClass(BulkIngestKeyDedupeCombiner.CONTEXT_WRITER_CLASS, BulkContextWriter.class, ContextWriter.class);\n+          job.setCombinerClass(BulkIngestKeyDedupeCombiner.class);\n         }\n-        \n-        // Setup the job output and reducer classes\n-        if (outputMutations) {\n-            job.setOutputKeyClass(Text.class);\n-            job.setOutputValueClass(Mutation.class);\n-            \n-            if (!useMapOnly) {\n-                job.setMapOutputKeyClass(BulkIngestKey.class);\n-                job.setMapOutputValueClass(Value.class);\n-                \n-                if (useCombiner) {\n-                    // Dedupe combiner will remove dupes and reset timestamps\n-                    // Note: to guarantee the combiner runs we need the min combine splits to be 1\n-                    job.getConfiguration().setInt(\"min.num.spills.for.combine\", 1);\n-                    job.getConfiguration().setClass(BulkIngestKeyDedupeCombiner.CONTEXT_WRITER_CLASS, BulkContextWriter.class, ContextWriter.class);\n-                    job.setCombinerClass(BulkIngestKeyDedupeCombiner.class);\n-                }\n-                \n-                if (useInlineCombiner) {\n-                    // The dedupe context writer invokes the BulkIngestKeyDedupeCombiner.\n-                    // We are running the DedupeContextWriter in the context writer stream instead of using a combiner for performance reasons\n-                    job.getConfiguration().setClass(EventMapper.CONTEXT_WRITER_CLASS, DedupeContextWriter.class, ChainedContextWriter.class);\n-                    job.getConfiguration().setClass(DedupeContextWriter.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ContextWriter.class);\n-                } else {\n-                    job.getConfiguration().setClass(EventMapper.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ChainedContextWriter.class);\n-                }\n-                job.getConfiguration().setClass(TableCachingContextWriter.CONTEXT_WRITER_CLASS, BulkContextWriter.class, ContextWriter.class);\n-                \n-                // Aggregating reducer will remove dupes for each reduce task and reset the reset timestamps\n-                // The reducer will take care of translating from BulkIngestKeys to Mutations by using the LiveContextWriter\n-                job.getConfiguration().setClass(BulkIngestKeyAggregatingReducer.CONTEXT_WRITER_CLASS, LiveContextWriter.class, ContextWriter.class);\n-                job.getConfiguration().setBoolean(BulkIngestKeyAggregatingReducer.CONTEXT_WRITER_OUTPUT_TABLE_COUNTERS, tableCounters);\n-                job.setReducerClass(BulkIngestKeyAggregatingReducer.class);\n-            } else {\n-                job.setMapOutputKeyClass(Text.class);\n-                job.setMapOutputValueClass(Mutation.class);\n-                \n-                // The dedupe context writer invokes the BulkIngestKeyDedupeCombiner, and the aggregating context writer\n-                // invokes the BulkIngestKeyAggregatingReducer. The LiveContextWriter will take care of translating from BulkIngestKeys to Mutations\n-                job.getConfiguration().setBoolean(EventMapper.CONTEXT_WRITER_OUTPUT_TABLE_COUNTERS, tableCounters);\n-                \n-                if (useCombiner || useInlineCombiner) {\n-                    job.getConfiguration().setClass(EventMapper.CONTEXT_WRITER_CLASS, DedupeContextWriter.class, ChainedContextWriter.class);\n-                    job.getConfiguration().setClass(DedupeContextWriter.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ContextWriter.class);\n-                } else {\n-                    job.getConfiguration().setClass(EventMapper.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ChainedContextWriter.class);\n-                }\n-                \n-                job.getConfiguration().setClass(TableCachingContextWriter.CONTEXT_WRITER_CLASS, AggregatingContextWriter.class, ContextWriter.class);\n-                job.getConfiguration().setClass(AggregatingContextWriter.CONTEXT_WRITER_CLASS, LiveContextWriter.class, ContextWriter.class);\n-            }\n-            \n+\n+        if (useInlineCombiner) {\n+          // The dedupe context writer invokes the BulkIngestKeyDedupeCombiner.\n+          // We are running the DedupeContextWriter in the context writer stream instead of using a combiner for performance reasons\n+          job.getConfiguration()\n+              .setClass(EventMapper.CONTEXT_WRITER_CLASS, DedupeContextWriter.class, ChainedContextWriter.class);\n+          job.getConfiguration()\n+              .setClass(DedupeContextWriter.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ContextWriter.class);\n         } else {\n-            job.setMapOutputKeyClass(BulkIngestKey.class);\n-            job.setMapOutputValueClass(Value.class);\n-            job.setOutputKeyClass(BulkIngestKey.class);\n-            job.setOutputValueClass(Value.class);\n-            \n-            if (!useMapOnly) {\n-                if (useCombiner) {\n-                    // Dedupe combiner will remove dupes and reset timestamps\n-                    // Note: to guarantee the combiner runs we need the min combine splits to be 1\n-                    job.getConfiguration().setInt(\"min.num.spills.for.combine\", 1);\n-                    job.getConfiguration().setClass(BulkIngestKeyDedupeCombiner.CONTEXT_WRITER_CLASS, BulkContextWriter.class, ContextWriter.class);\n-                    job.setCombinerClass(BulkIngestKeyDedupeCombiner.class);\n-                }\n-                \n-                if (useInlineCombiner) {\n-                    // The dedupe context writer invokes the BulkIngestKeyDedupeCombiner.\n-                    // We are running the DedupeContextWriter in the context writer stream instead of using a combiner for performance reasons\n-                    job.getConfiguration().setClass(EventMapper.CONTEXT_WRITER_CLASS, DedupeContextWriter.class, ChainedContextWriter.class);\n-                    job.getConfiguration().setClass(DedupeContextWriter.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ContextWriter.class);\n-                } else {\n-                    job.getConfiguration().setClass(EventMapper.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ChainedContextWriter.class);\n-                }\n-                job.getConfiguration().setClass(TableCachingContextWriter.CONTEXT_WRITER_CLASS, BulkContextWriter.class, ContextWriter.class);\n-                \n-                // Aggregating reducer will remove dupes for each reduce task and reset the reset timestamps\n-                job.getConfiguration().setClass(BulkIngestKeyAggregatingReducer.CONTEXT_WRITER_CLASS, BulkContextWriter.class, ContextWriter.class);\n-                job.getConfiguration().setBoolean(BulkIngestKeyAggregatingReducer.CONTEXT_WRITER_OUTPUT_TABLE_COUNTERS, tableCounters);\n-                job.setReducerClass(BulkIngestKeyAggregatingReducer.class);\n-            } else {\n-                // The dedupe context writer invokes the BulkIngestKeyDedupeCombiner, and the aggregating context writer\n-                // invokes the BulkIngestKeyAggregatingReducer. The LiveContextWriter will take care of translating from BulkIngestKeys to Mutations\n-                job.getConfiguration().setBoolean(EventMapper.CONTEXT_WRITER_OUTPUT_TABLE_COUNTERS, tableCounters);\n-                \n-                if (useCombiner || useInlineCombiner) {\n-                    job.getConfiguration().setClass(EventMapper.CONTEXT_WRITER_CLASS, DedupeContextWriter.class, ChainedContextWriter.class);\n-                    job.getConfiguration().setClass(DedupeContextWriter.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ContextWriter.class);\n-                } else {\n-                    job.getConfiguration().setClass(EventMapper.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ChainedContextWriter.class);\n-                }\n-                \n-                job.getConfiguration().setClass(TableCachingContextWriter.CONTEXT_WRITER_CLASS, AggregatingContextWriter.class, ContextWriter.class);\n-                job.getConfiguration().setClass(AggregatingContextWriter.CONTEXT_WRITER_CLASS, BulkContextWriter.class, ContextWriter.class);\n-            }\n+          job.getConfiguration()\n+              .setClass(EventMapper.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ChainedContextWriter.class);\n         }\n-        \n-        // If only doing a map phase, then no reduce tasks to run\n-        if (useMapOnly) {\n-            job.setNumReduceTasks(0);\n-            setReduceSpeculativeExecution(job.getConfiguration(), false);\n+        job.getConfiguration()\n+            .setClass(TableCachingContextWriter.CONTEXT_WRITER_CLASS, BulkContextWriter.class, ContextWriter.class);\n+\n+        // Aggregating reducer will remove dupes for each reduce task and reset the reset timestamps\n+        // The reducer will take care of translating from BulkIngestKeys to Mutations by using the LiveContextWriter\n+        job.getConfiguration().setClass(BulkIngestKeyAggregatingReducer.CONTEXT_WRITER_CLASS, LiveContextWriter.class,\n+            ContextWriter.class);\n+        job.getConfiguration()\n+            .setBoolean(BulkIngestKeyAggregatingReducer.CONTEXT_WRITER_OUTPUT_TABLE_COUNTERS, tableCounters);\n+        job.setReducerClass(BulkIngestKeyAggregatingReducer.class);\n+      } else {\n+        job.setMapOutputKeyClass(Text.class);\n+        job.setMapOutputValueClass(Mutation.class);\n+\n+        // The dedupe context writer invokes the BulkIngestKeyDedupeCombiner, and the aggregating context writer\n+        // invokes the BulkIngestKeyAggregatingReducer. The LiveContextWriter will take care of translating from BulkIngestKeys to Mutations\n+        job.getConfiguration().setBoolean(EventMapper.CONTEXT_WRITER_OUTPUT_TABLE_COUNTERS, tableCounters);\n+\n+        if (useCombiner || useInlineCombiner) {\n+          job.getConfiguration()\n+              .setClass(EventMapper.CONTEXT_WRITER_CLASS, DedupeContextWriter.class, ChainedContextWriter.class);\n+          job.getConfiguration()\n+              .setClass(DedupeContextWriter.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ContextWriter.class);\n         } else {\n-            job.setNumReduceTasks(reduceTasks);\n+          job.getConfiguration()\n+              .setClass(EventMapper.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ChainedContextWriter.class);\n         }\n-        \n-        // Turn off speculative execution if we're using live ingest.\n-        // We don't want to send the same mutations to accumulo multiple times.\n-        // Normally for bulk we use speculative execution since there are no\n-        // direct accumulo writes. However, the user can turn off speculative\n-        // execution if they want. This may, for example, increase overall\n-        // throughput if the system is fully loaded.\n-        if (disableSpeculativeExecution || outputMutations) {\n-            setMapSpeculativeExecution(job.getConfiguration(), false);\n-            setReduceSpeculativeExecution(job.getConfiguration(), false);\n+\n+        job.getConfiguration().setClass(TableCachingContextWriter.CONTEXT_WRITER_CLASS, AggregatingContextWriter.class,\n+            ContextWriter.class);\n+        job.getConfiguration()\n+            .setClass(AggregatingContextWriter.CONTEXT_WRITER_CLASS, LiveContextWriter.class, ContextWriter.class);\n+      }\n+\n+    } else {\n+      job.setMapOutputKeyClass(BulkIngestKey.class);\n+      job.setMapOutputValueClass(Value.class);\n+      job.setOutputKeyClass(BulkIngestKey.class);\n+      job.setOutputValueClass(Value.class);\n+\n+      if (!useMapOnly) {\n+        if (useCombiner) {\n+          // Dedupe combiner will remove dupes and reset timestamps\n+          // Note: to guarantee the combiner runs we need the min combine splits to be 1\n+          job.getConfiguration().setInt(\"min.num.spills.for.combine\", 1);\n+          job.getConfiguration()\n+              .setClass(BulkIngestKeyDedupeCombiner.CONTEXT_WRITER_CLASS, BulkContextWriter.class, ContextWriter.class);\n+          job.setCombinerClass(BulkIngestKeyDedupeCombiner.class);\n         }\n-        \n-        // Setup the Output\n-        job.setWorkingDirectory(workDirPath);\n-        if (outputMutations) {\n-            CBMutationOutputFormatter.setZooKeeperInstance(job, ClientConfiguration.loadDefault().withInstance(instanceName).withZkHosts(zooKeepers));\n-            CBMutationOutputFormatter.setOutputInfo(job, userName, password, true, null);\n-            job.setOutputFormatClass(CBMutationOutputFormatter.class);\n+\n+        if (useInlineCombiner) {\n+          // The dedupe context writer invokes the BulkIngestKeyDedupeCombiner.\n+          // We are running the DedupeContextWriter in the context writer stream instead of using a combiner for performance reasons\n+          job.getConfiguration()\n+              .setClass(EventMapper.CONTEXT_WRITER_CLASS, DedupeContextWriter.class, ChainedContextWriter.class);\n+          job.getConfiguration()\n+              .setClass(DedupeContextWriter.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ContextWriter.class);\n         } else {\n-            FileOutputFormat.setOutputPath(job, new Path(workDirPath, \"mapFiles\"));\n-            job.setOutputFormatClass(MultiRFileOutputFormatter.class);\n+          job.getConfiguration()\n+              .setClass(EventMapper.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ChainedContextWriter.class);\n         }\n-        \n-        // Setup the location for the history output (old and new property names)\n-        job.getConfiguration().setIfUnset(\"hadoop.job.history.user.location\", workDirPath + \"/mapFiles\");\n-        job.getConfiguration().setIfUnset(\"mapreduce.job.userhistorylocation\", workDirPath + \"/mapFiles\");\n-        \n-        // verbose counters will add counters showing the output in the EventMapper, and the input to the reducers\n-        if (verboseCounters) {\n-            job.getConfiguration().setBoolean(\"verboseCounters\", true);\n-        }\n-        \n-        // we always want the job to use our jars instead of the ones in $HADOOP_HOME/lib\n-        job.getConfiguration().setBoolean(\"mapreduce.job.user.classpath.first\", true);\n-        \n-        // fetch the multiple numshards cache, if necessary\n-        if (job.getConfiguration().getBoolean(NumShards.ENABLE_MULTIPLE_NUMSHARDS, false)) {\n-            NumShards numShards = new NumShards(job.getConfiguration());\n-            String multipleNumShardsConfig = numShards.readMultipleNumShardsConfig();\n-            \n-            // this could return empty string, if the feature is enabled, but no entries in the metadata table\n-            // if it didn't throw RuntimeException, it found a valid cache file\n-            job.getConfiguration().set(NumShards.PREFETCHED_MULTIPLE_NUMSHARDS_CONFIGURATION, multipleNumShardsConfig == null ? \"\" : multipleNumShardsConfig);\n-        }\n-    }\n-    \n-    /**\n-     * @param keyValue\n-     *            of format 'key=value'\n-     */\n-    protected void addConfOverride(String keyValue) {\n-        String[] strArr = keyValue.split(\"=\", 2);\n-        if (strArr.length != 2) {\n-            log.error(\"WARN: skipping bad property configuration \" + keyValue);\n+        job.getConfiguration()\n+            .setClass(TableCachingContextWriter.CONTEXT_WRITER_CLASS, BulkContextWriter.class, ContextWriter.class);\n+\n+        // Aggregating reducer will remove dupes for each reduce task and reset the reset timestamps\n+        job.getConfiguration().setClass(BulkIngestKeyAggregatingReducer.CONTEXT_WRITER_CLASS, BulkContextWriter.class,\n+            ContextWriter.class);\n+        job.getConfiguration()\n+            .setBoolean(BulkIngestKeyAggregatingReducer.CONTEXT_WRITER_OUTPUT_TABLE_COUNTERS, tableCounters);\n+        job.setReducerClass(BulkIngestKeyAggregatingReducer.class);\n+      } else {\n+        // The dedupe context writer invokes the BulkIngestKeyDedupeCombiner, and the aggregating context writer\n+        // invokes the BulkIngestKeyAggregatingReducer. The LiveContextWriter will take care of translating from BulkIngestKeys to Mutations\n+        job.getConfiguration().setBoolean(EventMapper.CONTEXT_WRITER_OUTPUT_TABLE_COUNTERS, tableCounters);\n+\n+        if (useCombiner || useInlineCombiner) {\n+          job.getConfiguration()\n+              .setClass(EventMapper.CONTEXT_WRITER_CLASS, DedupeContextWriter.class, ChainedContextWriter.class);\n+          job.getConfiguration()\n+              .setClass(DedupeContextWriter.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ContextWriter.class);\n         } else {\n-            log.info(\"Setting \" + strArr[0] + \" = \\\"\" + strArr[1] + '\"');\n-            confOverrides.add(strArr);\n+          job.getConfiguration()\n+              .setClass(EventMapper.CONTEXT_WRITER_CLASS, TableCachingContextWriter.class, ChainedContextWriter.class);\n         }\n+\n+        job.getConfiguration().setClass(TableCachingContextWriter.CONTEXT_WRITER_CLASS, AggregatingContextWriter.class,\n+            ContextWriter.class);\n+        job.getConfiguration()\n+            .setClass(AggregatingContextWriter.CONTEXT_WRITER_CLASS, BulkContextWriter.class, ContextWriter.class);\n+      }\n     }\n-    \n-    private void updateConfWithOverrides(Configuration conf) {\n-        for (String[] conOverride : confOverrides) {\n-            conf.set(conOverride[0], conOverride[1]);\n-        }\n+\n+    // If only doing a map phase, then no reduce tasks to run\n+    if (useMapOnly) {\n+      job.setNumReduceTasks(0);\n+      setReduceSpeculativeExecution(job.getConfiguration(), false);\n+    } else {\n+      job.setNumReduceTasks(reduceTasks);\n     }\n-    \n-    protected int jobFailed(Job job, RunningJob runningJob, FileSystem fs, Path workDir) throws IOException {\n-        log.error(\"Map Reduce job \" + job.getJobName() + \" was unsuccessful. Check the logs.\");\n-        log.error(\"Since job was not successful, deleting work directory: \" + workDir);\n-        boolean deleted = fs.delete(workDir, true);\n-        if (!deleted) {\n-            log.error(\"Unable to remove job working directory: \" + workDir);\n-        }\n-        if (runningJob.getJobState() == JobStatus.KILLED) {\n-            log.warn(\"Job was killed\");\n-            return -2;\n-        } else {\n-            log.error(\"Job failed with a jobstate of \" + runningJob.getJobState());\n-            return -3;\n-        }\n+\n+    // Turn off speculative execution if we're using live ingest.\n+    // We don't want to send the same mutations to accumulo multiple times.\n+    // Normally for bulk we use speculative execution since there are no\n+    // direct accumulo writes. However, the user can turn off speculative\n+    // execution if they want. This may, for example, increase overall\n+    // throughput if the system is fully loaded.\n+    if (disableSpeculativeExecution || outputMutations) {\n+      setMapSpeculativeExecution(job.getConfiguration(), false);\n+      setReduceSpeculativeExecution(job.getConfiguration(), false);\n     }\n-    \n-    protected boolean createFileWithRetries(FileSystem fs, Path file) throws IOException, InterruptedException {\n-        return createFileWithRetries(fs, file, file);\n-    }\n-    \n-    protected boolean createFileWithRetries(FileSystem fs, Path file, Path verification) throws IOException, InterruptedException {\n-        Exception exception = null;\n-        // we will attempt this 10 times at most....\n-        for (int i = 0; i < 10; i++) {\n-            try {\n-                exception = null;\n-                // create the file....ignoring the return value as we will be checking ourselves anyway....\n-                log.info(\"Creating \" + file);\n-                fs.createNewFile(file);\n-            } catch (Exception e) {\n-                exception = e;\n-            }\n-            // check to see if the file exists in which case we are good to go\n-            try {\n-                log.info(\"Verifying \" + file + \" with \" + verification);\n-                FileStatus[] files = fs.globStatus(verification);\n-                if (files == null || files.length == 0) {\n-                    throw new FileNotFoundException(\"Failed to get status for \" + file);\n-                }\n-                // we found the file!\n-                log.info(\"Created \" + file);\n-                return true;\n-            } catch (Exception e) {\n-                log.warn(\"Trying again to create \" + file + \" in one second\");\n-                // now this is getting frustrating....\n-                // wait a sec and try again\n-                Thread.sleep(1000);\n-            }\n-        }\n-        // log the exception if any\n-        if (exception != null) {\n-            log.error(\"Failed to create \" + file, exception);\n+\n+    // Setup the Output\n+    job.setWorkingDirectory(workDirPath);\n+    if (outputMutations) {\n+      CBMutationOutputFormatter.setZooKeeperInstance(job,\n+          ClientConfiguration.loadDefault().withInstance(instanceName).withZkHosts(zooKeepers));\n+      CBMutationOutputFormatter.setOutputInfo(job, userName, password, true, null);\n+      job.setOutputFormatClass(CBMutationOutputFormatter.class);\n+    } else {\n+      FileOutputFormat.setOutputPath(job, new Path(workDirPath, \"mapFiles\"));\n+      job.setOutputFormatClass(MultiRFileOutputFormatter.class);\n+    }\n+\n+    // Setup the location for the history output (old and new property names)\n+    job.getConfiguration().setIfUnset(\"hadoop.job.history.user.location\", workDirPath + \"/mapFiles\");\n+    job.getConfiguration().setIfUnset(\"mapreduce.job.userhistorylocation\", workDirPath + \"/mapFiles\");\n+\n+    // verbose counters will add counters showing the output in the EventMapper, and the input to the reducers\n+    if (verboseCounters) {\n+      job.getConfiguration().setBoolean(\"verboseCounters\", true);\n+    }\n+\n+    // we always want the job to use our jars instead of the ones in $HADOOP_HOME/lib\n+    job.getConfiguration().setBoolean(\"mapreduce.job.user.classpath.first\", true);\n+\n+    // fetch the multiple numshards cache, if necessary\n+    if (job.getConfiguration().getBoolean(NumShards.ENABLE_MULTIPLE_NUMSHARDS, false)) {\n+      NumShards numShards = new NumShards(job.getConfiguration());\n+      String multipleNumShardsConfig = numShards.readMultipleNumShardsConfig();\n+\n+      // this could return empty string, if the feature is enabled, but no entries in the metadata table\n+      // if it didn't throw RuntimeException, it found a valid cache file\n+      job.getConfiguration().set(NumShards.PREFETCHED_MULTIPLE_NUMSHARDS_CONFIGURATION,\n+          multipleNumShardsConfig == null ? \"\" : multipleNumShardsConfig);\n+    }\n+  }\n+\n+  /**\n+   * @param keyValue of format 'key=value'\n+   */\n+  protected void addConfOverride(String keyValue) {\n+    String[] strArr = keyValue.split(\"=\", 2);\n+    if (strArr.length != 2) {\n+      log.error(\"WARN: skipping bad property configuration \" + keyValue);\n+    } else {\n+      log.info(\"Setting \" + strArr[0] + \" = \\\"\" + strArr[1] + '\"');\n+      confOverrides.add(strArr);\n+    }\n+  }\n+\n+  private void updateConfWithOverrides(Configuration conf) {\n+    for (String[] conOverride : confOverrides) {\n+      conf.set(conOverride[0], conOverride[1]);\n+    }\n+  }\n+\n+  protected int jobFailed(Job job, RunningJob runningJob, FileSystem fs, Path workDir) throws IOException {\n+    log.error(\"Map Reduce job \" + job.getJobName() + \" was unsuccessful. Check the logs.\");\n+    log.error(\"Since job was not successful, deleting work directory: \" + workDir);\n+    boolean deleted = fs.delete(workDir, true);\n+    if (!deleted) {\n+      log.error(\"Unable to remove job working directory: \" + workDir);\n+    }\n+    if (runningJob.getJobState() == JobStatus.KILLED) {\n+      log.warn(\"Job was killed\");\n+      return -2;\n+    } else {\n+      log.error(\"Job failed with a jobstate of \" + runningJob.getJobState());\n+      return -3;\n+    }\n+  }\n+\n+  protected boolean createFileWithRetries(FileSystem fs, Path file) throws IOException, InterruptedException {\n+    return createFileWithRetries(fs, file, file);\n+  }\n+\n+  protected boolean createFileWithRetries(FileSystem fs, Path file, Path verification)\n+      throws IOException, InterruptedException {\n+    Exception exception = null;\n+    // we will attempt this 10 times at most....\n+    for (int i = 0; i < 10; i++) {\n+      try {\n+        exception = null;\n+        // create the file....ignoring the return value as we will be checking ourselves anyway....\n+        log.info(\"Creating \" + file);\n+        fs.createNewFile(file);\n+      } catch (Exception e) {\n+        exception = e;\n+      }\n+      // check to see if the file exists in which case we are good to go\n+      try {\n+        log.info(\"Verifying \" + file + \" with \" + verification);\n+        FileStatus[] files = fs.globStatus(verification);\n+        if (files == null || files.length == 0) {\n+          throw new FileNotFoundException(\"Failed to get status for \" + file);\n         }\n-        return false;\n-        \n-    }\n-    \n-    protected boolean markJobComplete(FileSystem fs, Path workDir) throws IOException, InterruptedException {\n-        return createFileWithRetries(fs, new Path(workDir, \"job.complete\"), new Path(workDir, \"job.[^p]*\"));\n-    }\n-    \n-    /**\n-     * Get the files to process\n-     *\n-     * @param fs\n-     *            used by extending classes such as MapFileMergeJob\n-     * @param inputFileLists\n-     * @param inputFileListMarker\n-     * @param inputPaths\n-     * @return\n-     * @throws IOException\n-     */\n-    protected Path[] getFilesToProcess(FileSystem fs, boolean inputFileLists, String inputFileListMarker, String inputPaths) throws IOException {\n-        String[] paths = StringUtils.trimAndRemoveEmptyStrings(StringUtils.split(inputPaths, ','));\n-        List<Path> inputPathList = new ArrayList<>(inputFileLists ? paths.length * 100 : paths.length);\n-        for (String inputPath : paths) {\n-            // if we are to treat the input paths as file lists, then expand here\n-            if (inputFileLists) {\n-                FileInputStream in = new FileInputStream(inputPath);\n-                try (BufferedReader r = new BufferedReader(new InputStreamReader(in))) {\n-                    String line = r.readLine();\n-                    boolean useit = (inputFileListMarker == null);\n-                    while (line != null) {\n-                        if (useit) {\n-                            inputPathList.add(new Path(line));\n-                        } else {\n-                            if (line.equals(inputFileListMarker)) {\n-                                useit = true;\n-                            }\n-                        }\n-                        line = r.readLine();\n-                    }\n-                } finally {\n-                    in.close();\n-                }\n+        // we found the file!\n+        log.info(\"Created \" + file);\n+        return true;\n+      } catch (Exception e) {\n+        log.warn(\"Trying again to create \" + file + \" in one second\");\n+        // now this is getting frustrating....\n+        // wait a sec and try again\n+        Thread.sleep(1000);\n+      }\n+    }\n+    // log the exception if any\n+    if (exception != null) {\n+      log.error(\"Failed to create \" + file, exception);\n+    }\n+    return false;\n+\n+  }\n+\n+  protected boolean markJobComplete(FileSystem fs, Path workDir) throws IOException, InterruptedException {\n+    return createFileWithRetries(fs, new Path(workDir, \"job.complete\"), new Path(workDir, \"job.[^p]*\"));\n+  }\n+\n+  /**\n+   * Get the files to process\n+   *\n+   * @param fs                  used by extending classes such as MapFileMergeJob\n+   * @param inputFileLists\n+   * @param inputFileListMarker\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  protected Path[] getFilesToProcess(FileSystem fs, boolean inputFileLists, String inputFileListMarker,\n+                                     String inputPaths) throws IOException {\n+    String[] paths = StringUtils.trimAndRemoveEmptyStrings(StringUtils.split(inputPaths, ','));\n+    List<Path> inputPathList = new ArrayList<>(inputFileLists ? paths.length * 100 : paths.length);\n+    for (String inputPath : paths) {\n+      // if we are to treat the input paths as file lists, then expand here\n+      if (inputFileLists) {\n+        FileInputStream in = new FileInputStream(inputPath);\n+        try (BufferedReader r = new BufferedReader(new InputStreamReader(in))) {\n+          String line = r.readLine();\n+          boolean useit = (inputFileListMarker == null);\n+          while (line != null) {\n+            if (useit) {\n+              inputPathList.add(new Path(line));\n             } else {\n-                inputPathList.add(new Path(inputPath));\n+              if (line.equals(inputFileListMarker)) {\n+                useit = true;\n+              }\n             }\n+            line = r.readLine();\n+          }\n+        } finally {\n+          in.close();\n         }\n-        // log the input path list if we had to expand file lists\n-        if (inputFileLists) {\n-            log.info(\"inputPathList is \" + inputPathList);\n-        }\n-        return inputPathList.toArray(new Path[inputPathList.size()]);\n-    }\n-    \n-    protected FileSystem getFileSystem(Configuration conf, URI uri) throws IOException {\n-        return (uri == null ? FileSystem.get(conf) : FileSystem.get(uri, conf));\n-    }\n-    \n-    /**\n-     * Writes the input paths for this job into the work directory in a file named \"job.paths\"\n-     */\n-    protected void writeInputPathsFile(FileSystem fs, Path workDir, Path[] inputPaths) throws IOException {\n-        FSDataOutputStream os = fs.create(new Path(workDir, \"job.paths\"));\n-        PrintStream ps = new PrintStream(new BufferedOutputStream(os));\n-        for (Path p : inputPaths) {\n-            ps.println(new Path(p.toUri().getPath()));\n-        }\n-        ps.close();\n-    }\n-    \n-    /**\n-     * Writes the flag file for this job into the work directory in a file with the same name\n-     */\n-    protected void writeFlagFile(FileSystem fs, Path workDir, String flagFileName) throws IOException {\n-        File flagFile = new File(flagFileName);\n-        if (!flagFile.exists() || !flagFile.isFile() || !flagFile.canRead()) {\n-            throw new IOException(\"Unable to access \" + flagFile + \" for copying into hdfs \" + workDir + \" directory\");\n-        }\n-        \n-        try (InputStream is = new BufferedInputStream(new FileInputStream(flagFile))) {\n-            String flagFileBase = getBaseFlagFileName(flagFile.getName());\n-            Path destFile = new Path(workDir, flagFileBase);\n-            log.info(\"Copying flag file into \" + destFile);\n-            try (OutputStream os = new BufferedOutputStream(fs.create(destFile))) {\n-                \n-                byte[] buffer = new byte[2048];\n-                int bytesRead = is.read(buffer);\n-                while (bytesRead >= 0) {\n-                    if (bytesRead > 0) {\n-                        os.write(buffer, 0, bytesRead);\n-                    }\n-                    bytesRead = is.read(buffer);\n-                }\n-            }\n+      } else {\n+        inputPathList.add(new Path(inputPath));\n+      }\n+    }\n+    // log the input path list if we had to expand file lists\n+    if (inputFileLists) {\n+      log.info(\"inputPathList is \" + inputPathList);\n+    }\n+    return inputPathList.toArray(new Path[inputPathList.size()]);\n+  }\n+\n+  protected FileSystem getFileSystem(Configuration conf, URI uri) throws IOException {\n+    return (uri == null ? FileSystem.get(conf) : FileSystem.get(uri, conf));\n+  }\n+\n+  /**\n+   * Writes the input paths for this job into the work directory in a file named \"job.paths\"\n+   */\n+  protected void writeInputPathsFile(FileSystem fs, Path workDir, Path[] inputPaths) throws IOException {\n+    FSDataOutputStream os = fs.create(new Path(workDir, \"job.paths\"));\n+    PrintStream ps = new PrintStream(new BufferedOutputStream(os));\n+    for (Path p : inputPaths) {\n+      ps.println(new Path(p.toUri().getPath()));\n+    }\n+    ps.close();\n+  }\n+\n+  /**\n+   * Writes the flag file for this job into the work directory in a file with the same name\n+   */\n+  protected void writeFlagFile(FileSystem fs, Path workDir, String flagFileName) throws IOException {\n+    File flagFile = new File(flagFileName);\n+    if (!flagFile.exists() || !flagFile.isFile() || !flagFile.canRead()) {\n+      throw new IOException(\"Unable to access \" + flagFile + \" for copying into hdfs \" + workDir + \" directory\");\n+    }\n+\n+    try (InputStream is = new BufferedInputStream(new FileInputStream(flagFile))) {\n+      String flagFileBase = getBaseFlagFileName(flagFile.getName());\n+      Path destFile = new Path(workDir, flagFileBase);\n+      log.info(\"Copying flag file into \" + destFile);\n+      try (OutputStream os = new BufferedOutputStream(fs.create(destFile))) {\n+\n+        byte[] buffer = new byte[2048];\n+        int bytesRead = is.read(buffer);\n+        while (bytesRead >= 0) {\n+          if (bytesRead > 0) {\n+            os.write(buffer, 0, bytesRead);\n+          }\n+          bytesRead = is.read(buffer);\n         }\n+      }\n     }\n-    \n-    /*\n-     * Get the base flag filename. The name supplied may have something line '.inprogress' tagged on the end. Return everything up to and including .flag.\n-     */\n-    protected String getBaseFlagFileName(String filename) {\n-        if (filename.endsWith(\".flag\")) {\n-            return filename;\n+  }\n+\n+  /*\n+   * Get the base flag filename. The name supplied may have something line '.inprogress' tagged on the end. Return everything up to and including .flag.\n+   */\n+  protected String getBaseFlagFileName(String filename) {\n+    if (filename.endsWith(\".flag\")) {\n+      return filename;\n+    } else {\n+      int index = filename.lastIndexOf(\".flag\");\n+      if (index < 0) {\n+        return filename;\n+      } else {\n+        return filename.substring(0, index + \".flag\".length());\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Marks the input files given to this job as loaded by moving them from the \"flagged\" directory to the \"loaded\" directory.\n+   */\n+  protected void markFilesLoaded(FileSystem fs, Path[] inputPaths, JobID jobID) throws IOException {\n+    for (Path src : inputPaths) {\n+      String ssrc = src.toString();\n+      if (ssrc.contains(\"/flagged/\")) {\n+        Path dst = new Path(ssrc.replaceFirst(\"/flagged/\", \"/loaded/\"));\n+        boolean mkdirs = fs.mkdirs(dst.getParent());\n+        if (mkdirs) {\n+          boolean renamed = fs.rename(src, dst);\n+          if (!renamed) {\n+            throw new IOException(\"Unable to rename \" + src + \" to \" + dst);\n+          }\n         } else {\n-            int index = filename.lastIndexOf(\".flag\");\n-            if (index < 0) {\n-                return filename;\n-            } else {\n-                return filename.substring(0, index + \".flag\".length());\n-            }\n+          throw new IOException(\"Unable to create parent dir: \" + dst.getParent());\n         }\n+      }\n     }\n-    \n-    /**\n-     * Marks the input files given to this job as loaded by moving them from the \"flagged\" directory to the \"loaded\" directory.\n-     */\n-    protected void markFilesLoaded(FileSystem fs, Path[] inputPaths, JobID jobID) throws IOException {\n-        for (Path src : inputPaths) {\n-            String ssrc = src.toString();\n-            if (ssrc.contains(\"/flagged/\")) {\n-                Path dst = new Path(ssrc.replaceFirst(\"/flagged/\", \"/loaded/\"));\n-                boolean mkdirs = fs.mkdirs(dst.getParent());\n-                if (mkdirs) {\n-                    boolean renamed = fs.rename(src, dst);\n-                    if (!renamed) {\n-                        throw new IOException(\"Unable to rename \" + src + \" to \" + dst);\n-                    }\n-                } else {\n-                    throw new IOException(\"Unable to create parent dir: \" + dst.getParent());\n-                }\n-            }\n-        }\n-        \n-        // notify observers\n-        jobObservable.setJobId(jobID.toString());\n-    }\n-    \n-    /**\n-     * Some properties cannot be set using the new API. However, we know internally that the configuration Hadoop uses is really just the old JobConf which\n-     * exposes the methods we want. In particular, we have to turn off speculative execution since we are loading data and don't want Hadoop to spawn many\n-     * speculative tasks that will load duplicate data.\n-     */\n-    protected void setMapSpeculativeExecution(Configuration conf, boolean value) {\n-        if (conf instanceof org.apache.hadoop.mapred.JobConf) {\n-            org.apache.hadoop.mapred.JobConf jobConf = (org.apache.hadoop.mapred.JobConf) conf;\n-            jobConf.setMapSpeculativeExecution(value);\n-        }\n+\n+    // notify observers\n+    jobObservable.setJobId(jobID.toString());\n+  }\n+\n+  /**\n+   * Some properties cannot be set using the new API. However, we know internally that the configuration Hadoop uses is really just the old JobConf which\n+   * exposes the methods we want. In particular, we have to turn off speculative execution since we are loading data and don't want Hadoop to spawn many\n+   * speculative tasks that will load duplicate data.\n+   */\n+  protected void setMapSpeculativeExecution(Configuration conf, boolean value) {\n+    if (conf instanceof org.apache.hadoop.mapred.JobConf) {\n+      org.apache.hadoop.mapred.JobConf jobConf = (org.apache.hadoop.mapred.JobConf) conf;\n+      jobConf.setMapSpeculativeExecution(value);\n     }\n-    \n-    /**\n-     * Some properties cannot be set using the new API. However, we know internally that the configuration Hadoop uses is really just the old JobConf which\n-     * exposes the methods we want. In particular, we have to turn off speculative execution since we are loading data and don't want Hadoop to spawn many\n-     * speculative tasks that will load duplicate data.\n-     */\n-    protected void setReduceSpeculativeExecution(Configuration conf, boolean value) {\n-        if (conf instanceof org.apache.hadoop.mapred.JobConf) {\n-            org.apache.hadoop.mapred.JobConf jobConf = (org.apache.hadoop.mapred.JobConf) conf;\n-            jobConf.setReduceSpeculativeExecution(value);\n-        }\n+  }\n+\n+  /**\n+   * Some properties cannot be set using the new API. However, we know internally that the configuration Hadoop uses is really just the old JobConf which\n+   * exposes the methods we want. In particular, we have to turn off speculative execution since we are loading data and don't want Hadoop to spawn many\n+   * speculative tasks that will load duplicate data.\n+   */\n+  protected void setReduceSpeculativeExecution(Configuration conf, boolean value) {\n+    if (conf instanceof org.apache.hadoop.mapred.JobConf) {\n+      org.apache.hadoop.mapred.JobConf jobConf = (org.apache.hadoop.mapred.JobConf) conf;\n+      jobConf.setReduceSpeculativeExecution(value);\n     }\n-    \n-    /**\n-     * Configures the output formatter with the correct accumulo instance information, splits file, and shard table.\n-     *\n-     * @param config\n-     *            hadoop configuration\n-     * @param compressionType\n-     *            type of compression to use for the output format\n-     * @param compressionTableBlackList\n-     *            a set of table names for which we will not compress the rfile output\n-     */\n-    public static void configureMultiRFileOutputFormatter(Configuration config, String compressionType, Set<String> compressionTableBlackList, int maxEntries,\n-                    long maxSize) {\n-        IngestJob.configureMultiRFileOutputFormatter(config, compressionType, compressionTableBlackList, maxEntries, maxSize, false);\n-    }\n-    \n-    public static void configureMultiRFileOutputFormatter(Configuration config, String compressionType, Set<String> compressionTableBlackList, int maxEntries,\n-                    long maxSize, boolean generateMapFileRowKeys) {\n-        MultiRFileOutputFormatter.setAccumuloConfiguration(config);\n-        if (compressionType != null) {\n-            MultiRFileOutputFormatter.setCompressionType(config, compressionType);\n-        }\n-        if (compressionTableBlackList != null) {\n-            MultiRFileOutputFormatter.setCompressionTableBlackList(config, compressionTableBlackList);\n-        }\n-        MultiRFileOutputFormatter.setRFileLimits(config, maxEntries, maxSize);\n-        MultiRFileOutputFormatter.setGenerateMapFileRowKeys(config, generateMapFileRowKeys);\n-    }\n-    \n-    protected void startDaemonProcesses(Configuration configuration) {\n-        String daemonClassNames = configuration.get(DAEMON_PROCESSES_PROPERTY);\n-        if (daemonClassNames == null) {\n-            return;\n-        }\n-        for (String className : StringUtils.split(daemonClassNames, ',')) {\n-            try {\n-                @SuppressWarnings(\"unchecked\")\n-                Class<? extends Runnable> daemonClass = (Class<? extends Runnable>) Class.forName(className.trim());\n-                Runnable daemon = daemonClass.newInstance();\n-                if (daemon instanceof Configurable) {\n-                    Configurable configurable = (Configurable) daemon;\n-                    configurable.setConf(configuration);\n-                }\n-                Thread daemonThread = new Thread(daemon);\n-                daemonThread.setDaemon(true);\n-                daemonThread.start();\n-            } catch (ClassNotFoundException | IllegalAccessException | InstantiationException e) {\n-                throw new IllegalArgumentException(e);\n-            }\n-        }\n+  }\n+\n+  /**\n+   * Configures the output formatter with the correct accumulo instance information, splits file, and shard table.\n+   *\n+   * @param config                    hadoop configuration\n+   * @param compressionType           type of compression to use for the output format\n+   * @param compressionTableBlackList a set of table names for which we will not compress the rfile output\n+   */\n+  public static void configureMultiRFileOutputFormatter(Configuration config, String compressionType,\n+                                                        Set<String> compressionTableBlackList, int maxEntries,\n+                                                        long maxSize) {\n+    IngestJob\n+        .configureMultiRFileOutputFormatter(config, compressionType, compressionTableBlackList, maxEntries, maxSize,\n+            false);\n+  }\n+\n+  public static void configureMultiRFileOutputFormatter(Configuration config, String compressionType,\n+                                                        Set<String> compressionTableBlackList, int maxEntries,\n+                                                        long maxSize, boolean generateMapFileRowKeys) {\n+    MultiRFileOutputFormatter.setAccumuloConfiguration(config);\n+    if (compressionType != null) {\n+      MultiRFileOutputFormatter.setCompressionType(config, compressionType);\n     }\n-    \n-    protected void distCpDirectory(Path workDir, FileSystem src, FileSystem dest, Configuration distcpConfig, boolean deleteAfterDistCp) throws Exception {\n-        Path srcPath = src.makeQualified(workDir);\n-        Path destPath = dest.makeQualified(workDir);\n-        Path logPath = new Path(destPath, \"logs\");\n-        \n-        // Make sure the destination path doesn't already exist, so that distcp won't\n-        // complain. We could add -i to the distcp command, but we don't want to hide\n-        // any other failures that we might care about (such as map files failing to\n-        // copy). We know the distcp target shouldn't exist, so if it does, it could\n-        // only be from a previous failed attempt.\n-        dest.delete(destPath, true);\n-        \n-        // NOTE: be careful with the preserve option. We only want to preserve user, group, and permissions but\n-        // not carry block size or replication across. This is especially important because by default the\n-        // MapReduce jobs produce output with the replication set to 1 and we definitely don't want to preserve\n-        // that when copying across clusters.\n-        DistCpOptions options = new DistCpOptions(Collections.singletonList(srcPath), destPath);\n-        options.setLogPath(logPath);\n-        options.setMapBandwidth(distCpBandwidth);\n-        options.setMaxMaps(distCpMaxMaps);\n-        options.setCopyStrategy(distCpStrategy);\n-        options.setSyncFolder(true);\n-        options.preserve(DistCpOptions.FileAttribute.USER);\n-        options.preserve(DistCpOptions.FileAttribute.GROUP);\n-        options.preserve(DistCpOptions.FileAttribute.PERMISSION);\n-        options.preserve(DistCpOptions.FileAttribute.BLOCKSIZE);\n-        options.preserve(DistCpOptions.FileAttribute.CHECKSUMTYPE);\n-        options.setBlocking(true);\n-        \n-        DistCp cp = new DistCp(distcpConfig, options);\n-        log.info(\"Starting distcp from \" + srcPath + \" to \" + destPath + \" with configuration: \" + options);\n-        try {\n-            cp.execute();\n-        } catch (Exception e) {\n-            throw new RuntimeException(\"Distcp failed.\", e);\n-        }\n-        // verify the data was copied\n-        Map<String,FileStatus> destFiles = new HashMap<>();\n-        for (FileStatus destFile : dest.listStatus(destPath)) {\n-            destFiles.put(destFile.getPath().getName(), destFile);\n-        }\n-        \n-        for (FileStatus srcFile : src.listStatus(srcPath)) {\n-            FileStatus destFile = destFiles.get(srcFile.getPath().getName());\n-            if (destFile == null || destFile.getLen() != srcFile.getLen()) {\n-                throw new RuntimeException(\"DistCp failed to copy \" + srcFile.getPath());\n-            }\n-        }\n-        \n-        // now we can clean up the src job directory\n-        if (deleteAfterDistCp) {\n-            src.delete(srcPath, true);\n-        }\n+    if (compressionTableBlackList != null) {\n+      MultiRFileOutputFormatter.setCompressionTableBlackList(config, compressionTableBlackList);\n     }\n-    \n-    protected boolean writeStats(Logger log, Job job, JobID jobId, Counters counters, long start, long stop, boolean outputMutations, FileSystem fs,\n-                    Path statsDir, String metricsLabelOverride) throws IOException, InterruptedException {\n-        \n-        Configuration conf = job.getConfiguration();\n-        \n-        // We are going to serialize the counters into a file in HDFS.\n-        // The context was set in the processKeyValues method below, and should not be null. We'll guard against NPE anyway\n-        try (RawLocalFileSystem rawFS = new RawLocalFileSystem()) {\n-            rawFS.setConf(conf);\n-            CompressionCodec cc = new GzipCodec();\n-            CompressionType ct = CompressionType.BLOCK;\n-            \n-            // Add additional counters\n-            if (!outputMutations) {\n-                counters.findCounter(IngestProcess.OUTPUT_DIRECTORY.name(), job.getWorkingDirectory().getName()).increment(1);\n-            } else {\n-                counters.findCounter(IngestProcess.LIVE_INGEST).increment(1);\n-            }\n-            counters.findCounter(IngestProcess.START_TIME).increment(start);\n-            counters.findCounter(IngestProcess.END_TIME).increment(stop);\n-            \n-            if (metricsLabelOverride != null) {\n-                counters.getGroup(IngestProcess.METRICS_LABEL_OVERRIDE.name()).findCounter(metricsLabelOverride).increment(1);\n-            }\n-            \n-            // Serialize the counters to a file in HDFS.\n-            Path src = new Path(\"/tmp\" + File.separator + job.getJobName() + \".metrics\");\n-            src = rawFS.makeQualified(src);\n-            createFileWithRetries(rawFS, src);\n-            try (Writer writer = SequenceFile.createWriter(conf, Writer.file(src), Writer.keyClass(Text.class), Writer.valueClass(Counters.class),\n-                            Writer.compression(ct, cc))) {\n-                writer.append(new Text(jobId.toString()), counters);\n-            }\n-            \n-            // Now we will try to move the file to HDFS.\n-            // Copy the file to the temp dir\n-            try {\n-                if (!fs.exists(statsDir))\n-                    fs.mkdirs(statsDir);\n-                Path dst = new Path(statsDir, src.getName());\n-                log.info(\"Copying file \" + src + \" to \" + dst);\n-                fs.copyFromLocalFile(false, true, src, dst);\n-                // If this worked, then remove the local file\n-                rawFS.delete(src, false);\n-                // also remove the residual crc file\n-                rawFS.delete(getCrcFile(src), false);\n-            } catch (IOException e) {\n-                // If an error occurs in the copy, then we will leave in the local metrics directory.\n-                log.error(\"Error copying metrics file into HDFS, will remain in metrics directory.\", e);\n-                return false;\n-            }\n-        }\n-        // now if configured, lets write the stats out to statsd\n-        CounterToStatsDConfiguration statsDConfig = new CounterToStatsDConfiguration(conf);\n-        if (statsDConfig.isConfigured()) {\n-            log.info(\"Sending final counters via statsd: \" + statsDConfig);\n-            CounterStatsDClient statsd = statsDConfig.getClient();\n-            try {\n-                statsd.sendFinalStats(counters);\n-            } finally {\n-                statsd.close();\n-            }\n-        }\n-        \n-        return true;\n+    MultiRFileOutputFormatter.setRFileLimits(config, maxEntries, maxSize);\n+    MultiRFileOutputFormatter.setGenerateMapFileRowKeys(config, generateMapFileRowKeys);\n+  }\n+\n+  protected void startDaemonProcesses(Configuration configuration) {\n+    String daemonClassNames = configuration.get(DAEMON_PROCESSES_PROPERTY);\n+    if (daemonClassNames == null) {\n+      return;\n     }\n-    \n-    private Path getCrcFile(Path path) {\n-        return new Path(path.getParent(), \".\" + path.getName() + \".crc\");\n-    }\n-    \n-    /**\n-     * Output some verbose counters\n-     *\n-     * @param context\n-     *            hadoop task context for writing counter values\n-     * @param tableName\n-     *            the table name to write in the counter\n-     * @param mutation\n-     *            a Mutation containing the key-value pairs to log to counters\n-     */\n-    @SuppressWarnings(\"rawtypes\")\n-    public static void verboseCounters(TaskInputOutputContext context, String location, Text tableName, Mutation mutation) {\n-        for (KeyValue keyValue : getKeyValues(mutation)) {\n-            verboseCounter(context, location, tableName, keyValue.getKey().getRow().getBytes(), keyValue.getKey().getColumnFamily().getBytes(), keyValue\n-                            .getKey().getColumnQualifier().getBytes(), keyValue.getKey().getColumnVisibility(), keyValue.getValue().get());\n+    for (String className : StringUtils.split(daemonClassNames, ',')) {\n+      try {\n+        @SuppressWarnings(\"unchecked\")\n+        Class<? extends Runnable> daemonClass = (Class<? extends Runnable>) Class.forName(className.trim());\n+        Runnable daemon = daemonClass.newInstance();\n+        if (daemon instanceof Configurable) {\n+          Configurable configurable = (Configurable) daemon;\n+          configurable.setConf(configuration);\n         }\n+        Thread daemonThread = new Thread(daemon);\n+        daemonThread.setDaemon(true);\n+        daemonThread.start();\n+      } catch (ClassNotFoundException | IllegalAccessException | InstantiationException e) {\n+        throw new IllegalArgumentException(e);\n+      }\n     }\n-    \n-    /**\n-     * Output some verbose counters. Since the input is an iterable, this will cache the values in a list and return the new iterable.\n-     *\n-     * @param context\n-     * @param key\n-     * @param values\n-     */\n-    @SuppressWarnings(\"rawtypes\")\n-    public static Iterable<Value> verboseCounters(TaskInputOutputContext context, String location, BulkIngestKey key, Iterable<Value> values) {\n-        List<Value> valueList = new ArrayList<>();\n-        for (Value value : values) {\n-            valueList.add(value);\n-            verboseCounters(context, location, key, value);\n-        }\n-        return valueList;\n-    }\n-    \n-    /**\n-     * Output some verbose counters\n-     *\n-     * @param context\n-     *            hadoop task context for writing counter values\n-     * @param key\n-     *            hadoop key to log all key-value pairs to counters\n-     * @param value\n-     *            the value that goes with {@code key}\n-     */\n-    @SuppressWarnings(\"rawtypes\")\n-    public static void verboseCounters(TaskInputOutputContext context, String location, BulkIngestKey key, Value value) {\n-        verboseCounter(context, location, key.getTableName(), key.getKey().getRow().getBytes(), key.getKey().getColumnFamily().getBytes(), key.getKey()\n-                        .getColumnQualifier().getBytes(), key.getKey().getColumnVisibility(), value.get());\n-    }\n-    \n-    /**\n-     * Output a verbose counter\n-     *\n-     * @param context\n-     *            hadoop task context for writing counter values\n-     * @param tableName\n-     *            the table name to write in the counter\n-     * @param row\n-     *            the row of the key to writer in the counter\n-     * @param colFamily\n-     *            the column family of the key to write in the counter\n-     * @param colQualifier\n-     *            the column qualifier of the key to write in the counter\n-     * @param colVis\n-     *            the column visibility of the key to write in the counter\n-     * @param val\n-     *            the value that goes with the supplied key\n-     */\n-    @SuppressWarnings(\"rawtypes\")\n-    public static void verboseCounter(TaskInputOutputContext context, String location, Text tableName, byte[] row, byte[] colFamily, byte[] colQualifier,\n-                    Text colVis, byte[] val) {\n-        String labelString = new ColumnVisibility(colVis).toString();\n-        String s = Key.toPrintableString(row, 0, row.length, Constants.MAX_DATA_TO_PRINT) + \" \"\n-                        + Key.toPrintableString(colFamily, 0, colFamily.length, Constants.MAX_DATA_TO_PRINT) + \":\"\n-                        + Key.toPrintableString(colQualifier, 0, colQualifier.length, Constants.MAX_DATA_TO_PRINT) + \" \" + labelString + \" \"\n-                        + (val == null ? \"null\" : String.valueOf(val.length) + \" value bytes\");\n-        \n-        s = s.replace('\\n', ' ');\n-        \n-        context.getCounter(\"TABLE.KEY.VALUElen\", tableName.toString() + ' ' + location + ' ' + s).increment(1);\n-    }\n-    \n-    /**\n-     * Turn a mutation's column update into a key\n-     *\n-     * @param m\n-     *            the Mutation from which KeyValue pairs should be extracted\n-     * @return a List of KeyValue pairs representing the contents of {@code m}\n-     */\n-    public static List<KeyValue> getKeyValues(Mutation m) {\n-        List<KeyValue> values = new ArrayList<>();\n-        for (ColumnUpdate update : m.getUpdates()) {\n-            values.add(new KeyValue(new Key(m.getRow(), update.getColumnFamily(), update.getColumnQualifier(), update.getColumnVisibility(), (update\n-                            .hasTimestamp() ? update.getTimestamp() : -1), update.isDeleted()), update.getValue()));\n+  }\n+\n+  protected void distCpDirectory(Path workDir, FileSystem src, FileSystem dest, Configuration distcpConfig,\n+                                 boolean deleteAfterDistCp) throws Exception {\n+    Path srcPath = src.makeQualified(workDir);\n+    Path destPath = dest.makeQualified(workDir);\n+    Path logPath = new Path(destPath, \"logs\");\n+\n+    // Make sure the destination path doesn't already exist, so that distcp won't\n+    // complain. We could add -i to the distcp command, but we don't want to hide\n+    // any other failures that we might care about (such as map files failing to\n+    // copy). We know the distcp target shouldn't exist, so if it does, it could\n+    // only be from a previous failed attempt.\n+    dest.delete(destPath, true);\n+\n+    // NOTE: be careful with the preserve option. We only want to preserve user, group, and permissions but\n+    // not carry block size or replication across. This is especially important because by default the\n+    // MapReduce jobs produce output with the replication set to 1 and we definitely don't want to preserve\n+    // that when copying across clusters.\n+    DistCpOptions options = new DistCpOptions(Collections.singletonList(srcPath), destPath);\n+    options.setLogPath(logPath);\n+    options.setMapBandwidth(distCpBandwidth);\n+    options.setMaxMaps(distCpMaxMaps);\n+    options.setCopyStrategy(distCpStrategy);\n+    options.setSyncFolder(true);\n+    options.preserve(DistCpOptions.FileAttribute.USER);\n+    options.preserve(DistCpOptions.FileAttribute.GROUP);\n+    options.preserve(DistCpOptions.FileAttribute.PERMISSION);\n+    options.preserve(DistCpOptions.FileAttribute.BLOCKSIZE);\n+    options.preserve(DistCpOptions.FileAttribute.CHECKSUMTYPE);\n+    options.setBlocking(true);\n+\n+    DistCp cp = new DistCp(distcpConfig, options);\n+    log.info(\"Starting distcp from \" + srcPath + \" to \" + destPath + \" with configuration: \" + options);\n+    try {\n+      cp.execute();\n+    } catch (Exception e) {\n+      throw new RuntimeException(\"Distcp failed.\", e);\n+    }\n+    // verify the data was copied\n+    Map<String, FileStatus> destFiles = new HashMap<>();\n+    for (FileStatus destFile : dest.listStatus(destPath)) {\n+      destFiles.put(destFile.getPath().getName(), destFile);\n+    }\n+\n+    for (FileStatus srcFile : src.listStatus(srcPath)) {\n+      FileStatus destFile = destFiles.get(srcFile.getPath().getName());\n+      if (destFile == null || destFile.getLen() != srcFile.getLen()) {\n+        throw new RuntimeException(\"DistCp failed to copy \" + srcFile.getPath());\n+      }\n+    }\n+\n+    // now we can clean up the src job directory\n+    if (deleteAfterDistCp) {\n+      src.delete(srcPath, true);\n+    }\n+  }\n+\n+  protected boolean writeStats(Logger log, Job job, JobID jobId, Counters counters, long start, long stop,\n+                               boolean outputMutations, FileSystem fs,\n+                               Path statsDir, String metricsLabelOverride) throws IOException, InterruptedException {\n+\n+    Configuration conf = job.getConfiguration();\n+\n+    // We are going to serialize the counters into a file in HDFS.\n+    // The context was set in the processKeyValues method below, and should not be null. We'll guard against NPE anyway\n+    try (RawLocalFileSystem rawFS = new RawLocalFileSystem()) {\n+      rawFS.setConf(conf);\n+      CompressionCodec cc = new GzipCodec();\n+      CompressionType ct = CompressionType.BLOCK;\n+\n+      // Add additional counters\n+      if (!outputMutations) {\n+        counters.findCounter(IngestProcess.OUTPUT_DIRECTORY.name(), job.getWorkingDirectory().getName()).increment(1);\n+      } else {\n+        counters.findCounter(IngestProcess.LIVE_INGEST).increment(1);\n+      }\n+      counters.findCounter(IngestProcess.START_TIME).increment(start);\n+      counters.findCounter(IngestProcess.END_TIME).increment(stop);\n+\n+      if (metricsLabelOverride != null) {\n+        counters.getGroup(IngestProcess.METRICS_LABEL_OVERRIDE.name()).findCounter(metricsLabelOverride).increment(1);\n+      }\n+\n+      // Serialize the counters to a file in HDFS.\n+      Path src = new Path(\"/tmp\" + File.separator + job.getJobName() + \".metrics\");\n+      src = rawFS.makeQualified(src);\n+      createFileWithRetries(rawFS, src);\n+      try (Writer writer = SequenceFile\n+          .createWriter(conf, Writer.file(src), Writer.keyClass(Text.class), Writer.valueClass(Counters.class),\n+              Writer.compression(ct, cc))) {\n+        writer.append(new Text(jobId.toString()), counters);\n+      }\n+\n+      // Now we will try to move the file to HDFS.\n+      // Copy the file to the temp dir\n+      try {\n+        if (!fs.exists(statsDir)) {\n+          fs.mkdirs(statsDir);\n         }\n-        return values;\n+        Path dst = new Path(statsDir, src.getName());\n+        log.info(\"Copying file \" + src + \" to \" + dst);\n+        fs.copyFromLocalFile(false, true, src, dst);\n+        // If this worked, then remove the local file\n+        rawFS.delete(src, false);\n+        // also remove the residual crc file\n+        rawFS.delete(getCrcFile(src), false);\n+      } catch (IOException e) {\n+        // If an error occurs in the copy, then we will leave in the local metrics directory.\n+        log.error(\"Error copying metrics file into HDFS, will remain in metrics directory.\", e);\n+        return false;\n+      }\n     }\n-    \n-    @Override\n-    public Configuration getConf() {\n-        return hadoopConfiguration;\n+    // now if configured, lets write the stats out to statsd\n+    CounterToStatsDConfiguration statsDConfig = new CounterToStatsDConfiguration(conf);\n+    if (statsDConfig.isConfigured()) {\n+      log.info(\"Sending final counters via statsd: \" + statsDConfig);\n+      CounterStatsDClient statsd = statsDConfig.getClient();\n+      try {\n+        statsd.sendFinalStats(counters);\n+      } finally {\n+        statsd.close();\n+      }\n     }\n-    \n-    @Override\n-    public void setConf(Configuration conf) {\n-        this.hadoopConfiguration = conf;\n+\n+    return true;\n+  }\n+\n+  private Path getCrcFile(Path path) {\n+    return new Path(path.getParent(), \".\" + path.getName() + \".crc\");\n+  }\n+\n+  /**\n+   * Output some verbose counters\n+   *\n+   * @param context   hadoop task context for writing counter values\n+   * @param tableName the table name to write in the counter\n+   * @param mutation  a Mutation containing the key-value pairs to log to counters\n+   */\n+  @SuppressWarnings(\"rawtypes\")\n+  public static void verboseCounters(TaskInputOutputContext context, String location, Text tableName,\n+                                     Mutation mutation) {\n+    for (KeyValue keyValue : getKeyValues(mutation)) {\n+      verboseCounter(context, location, tableName, keyValue.getKey().getRow().getBytes(),\n+          keyValue.getKey().getColumnFamily().getBytes(), keyValue\n+              .getKey().getColumnQualifier().getBytes(), keyValue.getKey().getColumnVisibility(),\n+          keyValue.getValue().get());\n+    }\n+  }\n+\n+  /**\n+   * Output some verbose counters. Since the input is an iterable, this will cache the values in a list and return the new iterable.\n+   *\n+   * @param context\n+   * @param key\n+   * @param values\n+   */\n+  @SuppressWarnings(\"rawtypes\")\n+  public static Iterable<Value> verboseCounters(TaskInputOutputContext context, String location, BulkIngestKey key,\n+                                                Iterable<Value> values) {\n+    List<Value> valueList = new ArrayList<>();\n+    for (Value value : values) {\n+      valueList.add(value);\n+      verboseCounters(context, location, key, value);\n     }\n+    return valueList;\n+  }\n+\n+  /**\n+   * Output some verbose counters\n+   *\n+   * @param context hadoop task context for writing counter values\n+   * @param key     hadoop key to log all key-value pairs to counters\n+   * @param value   the value that goes with {@code key}\n+   */\n+  @SuppressWarnings(\"rawtypes\")\n+  public static void verboseCounters(TaskInputOutputContext context, String location, BulkIngestKey key, Value value) {\n+    verboseCounter(context, location, key.getTableName(), key.getKey().getRow().getBytes(),\n+        key.getKey().getColumnFamily().getBytes(), key.getKey()\n+            .getColumnQualifier().getBytes(), key.getKey().getColumnVisibility(), value.get());\n+  }\n+\n+  /**\n+   * Output a verbose counter\n+   *\n+   * @param context      hadoop task context for writing counter values\n+   * @param tableName    the table name to write in the counter\n+   * @param row          the row of the key to writer in the counter\n+   * @param colFamily    the column family of the key to write in the counter\n+   * @param colQualifier the column qualifier of the key to write in the counter\n+   * @param colVis       the column visibility of the key to write in the counter\n+   * @param val          the value that goes with the supplied key\n+   */\n+  @SuppressWarnings(\"rawtypes\")\n+  public static void verboseCounter(TaskInputOutputContext context, String location, Text tableName, byte[] row,\n+                                    byte[] colFamily, byte[] colQualifier,\n+                                    Text colVis, byte[] val) {\n+    String labelString = new ColumnVisibility(colVis).toString();\n+    String s = Key.toPrintableString(row, 0, row.length, Constants.MAX_DATA_TO_PRINT) + \" \"\n+        + Key.toPrintableString(colFamily, 0, colFamily.length, Constants.MAX_DATA_TO_PRINT) + \":\"\n+        + Key.toPrintableString(colQualifier, 0, colQualifier.length, Constants.MAX_DATA_TO_PRINT) + \" \" + labelString +\n+        \" \"\n+        + (val == null ? \"null\" : String.valueOf(val.length) + \" value bytes\");\n+\n+    s = s.replace('\\n', ' ');\n+\n+    context.getCounter(\"TABLE.KEY.VALUElen\", tableName.toString() + ' ' + location + ' ' + s).increment(1);\n+  }\n+\n+  /**\n+   * Turn a mutation's column update into a key\n+   *\n+   * @param m the Mutation from which KeyValue pairs should be extracted\n+   * @return a List of KeyValue pairs representing the contents of {@code m}\n+   */\n+  public static List<KeyValue> getKeyValues(Mutation m) {\n+    List<KeyValue> values = new ArrayList<>();\n+    for (ColumnUpdate update : m.getUpdates()) {\n+      values.add(new KeyValue(\n+          new Key(m.getRow(), update.getColumnFamily(), update.getColumnQualifier(), update.getColumnVisibility(),\n+              (update\n+                  .hasTimestamp() ? update.getTimestamp() : -1), update.isDeleted()), update.getValue()));\n+    }\n+    return values;\n+  }\n+\n+  @Override\n+  public Configuration getConf() {\n+    return hadoopConfiguration;\n+  }\n+\n+  @Override\n+  public void setConf(Configuration conf) {\n+    this.hadoopConfiguration = conf;\n+  }\n }\n",
            "diff_size": 1984
        },
        {
            "tool": "naturalize",
            "errors": [
                {
                    "line": "30",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/99/IngestJob.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/naturalize/99/IngestJob.java\nindex 1f5b6fd985f..29f7a0af4ba 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/99/IngestJob.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/naturalize/99/IngestJob.java\n@@ -1142,9 +1142,8 @@ public class IngestJob implements Tool {\n             Path destFile = new Path(workDir, flagFileBase);\n             log.info(\"Copying flag file into \" + destFile);\n             try (OutputStream os = new BufferedOutputStream(fs.create(destFile))) {\n-                \n-                byte[] buffer = new byte[2048];\n-                int bytesRead = is.read(buffer);\n+    byte[] buffer = new byte[2048];\n+int bytesRead = is.read(buffer);\n                 while (bytesRead >= 0) {\n                     if (bytesRead > 0) {\n                         os.write(buffer, 0, bytesRead);\n@@ -1473,8 +1472,7 @@ public class IngestJob implements Tool {\n                         + Key.toPrintableString(colFamily, 0, colFamily.length, Constants.MAX_DATA_TO_PRINT) + \":\"\n                         + Key.toPrintableString(colQualifier, 0, colQualifier.length, Constants.MAX_DATA_TO_PRINT) + \" \" + labelString + \" \"\n                         + (val == null ? \"null\" : String.valueOf(val.length) + \" value bytes\");\n-        \n-        s = s.replace('\\n', ' ');\n+s = s.replace('\\n', ' ');\n         \n         context.getCounter(\"TABLE.KEY.VALUElen\", tableName.toString() + ' ' + location + ' ' + s).increment(1);\n     }\n@@ -1489,8 +1487,7 @@ public class IngestJob implements Tool {\n     public static List<KeyValue> getKeyValues(Mutation m) {\n         List<KeyValue> values = new ArrayList<>();\n         for (ColumnUpdate update : m.getUpdates()) {\n-            values.add(new KeyValue(new Key(m.getRow(), update.getColumnFamily(), update.getColumnQualifier(), update.getColumnVisibility(), (update\n-                            .hasTimestamp() ? update.getTimestamp() : -1), update.isDeleted()), update.getValue()));\n+            values.add(new KeyValue(new Key(m.getRow(), update.getColumnFamily(), update.getColumnQualifier(), update.getColumnVisibility(), (update.hasTimestamp() ? update.getTimestamp() : -1), update.isDeleted()), update.getValue()));\n         }\n         return values;\n     }\n@@ -1504,4 +1501,4 @@ public class IngestJob implements Tool {\n     public void setConf(Configuration conf) {\n         this.hadoopConfiguration = conf;\n     }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 8
        },
        {
            "tool": "codebuff",
            "errors": null,
            "diff": null
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "30",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "30",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        }
    ],
    "repaired_by": [],
    "not_repaired_by": [
        "styler",
        "intellij",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ]
}