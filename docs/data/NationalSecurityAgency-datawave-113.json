{
    "project_name": "NationalSecurityAgency-datawave",
    "error_id": "113",
    "information": {
        "errors": [
            {
                "line": "26",
                "severity": "error",
                "message": "Accumulo non-public classes imported",
                "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
            }
        ]
    },
    "source_code": "import org.apache.accumulo.core.data.Value;\nimport org.apache.accumulo.core.security.Authorizations;\nimport org.apache.accumulo.core.util.Pair;\nimport org.apache.commons.cli.CommandLine;\nimport org.apache.commons.cli.GnuParser;\nimport org.apache.commons.cli.Option;",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "26",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "intellij",
            "errors": [
                {
                    "line": "26",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/113/MetricsIngester.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/intellij/113/MetricsIngester.java\nindex 710aeefdb17..010ef4f1b41 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/113/MetricsIngester.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/intellij/113/MetricsIngester.java\n@@ -74,292 +74,302 @@ import java.util.concurrent.TimeUnit;\n  * (1) This job is a map only job, because as of now, there's no need to run a reduce task because the metrics are already sorted on per file/job/folder bases.\n  * <p>\n  * (2) This job will create the table specified as an argument if it does not already exist.\n- * \n  */\n public class MetricsIngester extends Configured implements Tool {\n-    private static final boolean createTables = true;\n-    \n-    private static final Logger log = Logger.getLogger(MetricsIngester.class);\n-    \n-    protected static final byte[] emptyBytes = {};\n-    protected static final Value emptyValue = new Value(emptyBytes);\n-    \n-    private static final int MAX_FILES = 2000;\n-    \n-    @Override\n-    public int run(String[] args) throws Exception {\n-        _configure(args);\n-        \n-        final Configuration conf = getConf();\n-        String type = conf.get(MetricsConfig.TYPE);\n-        \n-        /*\n-         * if the type is \"errors\", we want to process all of the errors from the metrics files first and then run the regular ingest metrics process\n-         */\n-        // MetricsServer.setServerConf(conf);\n-        // MetricsServer.initInstance();\n-        if (\"errors\".equals(type)) {\n-            try {\n-                launchErrorsJob(Job.getInstance(conf), conf);\n-            } catch (Exception e) {\n-                log.info(\"Failed to launch errors job\", e);\n-            }\n-            type = \"ingest\";\n-            conf.set(MetricsConfig.TYPE, type);\n-        }\n-        \n-        /* Type logic so I can differeniate between loader and ingest metrics jobs */\n-        Class<? extends Mapper<?,?,?,?>> mapperClass;\n-        String outTable;\n-        \n-        Path inputDirectoryPath = new Path(conf.get(MetricsConfig.INPUT_DIRECTORY));\n-        FileSystem fs = FileSystem.get(inputDirectoryPath.toUri(), conf);\n-        FileStatus[] fstats = fs.listStatus(inputDirectoryPath);\n-        Path[] files = FileUtil.stat2Paths(fstats);\n-        Path[] fileBuffer = new Path[MAX_FILES];\n-        for (int i = 0; i < files.length;) {\n-            Job job = Job.getInstance(getConf());\n-            job.setJarByClass(this.getClass());\n-            \n-            job.getConfiguration().setInt(\"mapred.job.reuse.jvm.num.tasks\", -1);\n-            \n-            if (\"ingest\".equalsIgnoreCase(type)) {\n-                mapperClass = IngestMetricsMapper.class;\n-                outTable = conf.get(MetricsConfig.INGEST_TABLE, MetricsConfig.DEFAULT_INGEST_TABLE);\n-                job.setInputFormatClass(SequenceFileInputFormat.class);\n-            } else if (\"loader\".equalsIgnoreCase(type)) {\n-                mapperClass = LoaderMetricsMapper.class;\n-                outTable = conf.get(MetricsConfig.LOADER_TABLE, MetricsConfig.DEFAULT_LOADER_TABLE);\n-                job.setInputFormatClass(SequenceFileInputFormat.class);\n-            } else if (\"flagmaker\".equalsIgnoreCase(type)) {\n-                mapperClass = FlagMakerMetricsMapper.class;\n-                outTable = conf.get(MetricsConfig.FLAGMAKER_TABLE, MetricsConfig.DEFAULT_FLAGMAKER_TABLE);\n-                job.setInputFormatClass(SequenceFileInputFormat.class);\n-            } else {\n-                log.error(type + \" is not a valid job type. Please use <ingest|loader>.\");\n-                return -1;\n-            }\n-            \n-            job.setJobName(\"MetricsIngester-\" + type);\n-            \n-            if (files.length - i > MAX_FILES) {\n-                System.arraycopy(files, i, fileBuffer, 0, MAX_FILES);\n-                i += MAX_FILES;\n-            } else {\n-                fileBuffer = new Path[files.length - i];\n-                System.arraycopy(files, i, fileBuffer, 0, fileBuffer.length);\n-                i += files.length - i;\n-            }\n-            \n-            SequenceFileInputFormat.setInputPaths(job, fileBuffer);\n-            \n-            job.setMapperClass(mapperClass);\n-            \n-            job.setNumReduceTasks(0);\n-            \n-            job.setOutputFormatClass(AccumuloOutputFormat.class);\n-            AccumuloOutputFormat.setConnectorInfo(job, conf.get(MetricsConfig.USER), new PasswordToken(conf.get(MetricsConfig.PASS, \"\").getBytes()));\n-            AccumuloOutputFormat.setCreateTables(job, createTables);\n-            AccumuloOutputFormat.setDefaultTableName(job, outTable);\n-            log.info(\"zookeepers = \" + conf.get(MetricsConfig.ZOOKEEPERS));\n-            log.info(\"instance = \" + conf.get(MetricsConfig.INSTANCE));\n-            log.info(\"clientConfuguration = \"\n-                            + ClientConfiguration.loadDefault().withInstance(conf.get(MetricsConfig.INSTANCE)).withZkHosts(conf.get(MetricsConfig.ZOOKEEPERS)));\n-            AccumuloOutputFormat.setZooKeeperInstance(job,\n-                            ClientConfiguration.loadDefault().withInstance(conf.get(MetricsConfig.INSTANCE)).withZkHosts(conf.get(MetricsConfig.ZOOKEEPERS)));\n-            AccumuloOutputFormat.setBatchWriterOptions(job, new BatchWriterConfig().setMaxLatency(25, TimeUnit.MILLISECONDS));\n-            \n-            job.submit();\n-            \n-            job.waitForCompletion(true);\n-            \n-            if (job.isSuccessful()) {\n-                for (Path p : fileBuffer) {\n-                    fs.delete(p, true);\n-                }\n-            }\n-        }\n-        \n-        return 0;\n+  private static final boolean createTables = true;\n+\n+  private static final Logger log = Logger.getLogger(MetricsIngester.class);\n+\n+  protected static final byte[] emptyBytes = {};\n+  protected static final Value emptyValue = new Value(emptyBytes);\n+\n+  private static final int MAX_FILES = 2000;\n+\n+  @Override\n+  public int run(String[] args) throws Exception {\n+    _configure(args);\n+\n+    final Configuration conf = getConf();\n+    String type = conf.get(MetricsConfig.TYPE);\n+\n+    /*\n+     * if the type is \"errors\", we want to process all of the errors from the metrics files first and then run the regular ingest metrics process\n+     */\n+    // MetricsServer.setServerConf(conf);\n+    // MetricsServer.initInstance();\n+    if (\"errors\".equals(type)) {\n+      try {\n+        launchErrorsJob(Job.getInstance(conf), conf);\n+      } catch (Exception e) {\n+        log.info(\"Failed to launch errors job\", e);\n+      }\n+      type = \"ingest\";\n+      conf.set(MetricsConfig.TYPE, type);\n     }\n-    \n-    protected int launchErrorsJob(Job job, Configuration conf) throws IOException, InterruptedException, ClassNotFoundException, AccumuloException,\n-                    AccumuloSecurityException, TableNotFoundException {\n-        job.setJobName(\"ErrorMetricsIngest\");\n-        \n-        job.setJarByClass(this.getClass());\n-        \n-        String outTable = conf.get(MetricsConfig.METRICS_TABLE, MetricsConfig.DEFAULT_METRICS_TABLE);\n-        \n-        /*\n-         * This block allows for us to read from a virtual \"snapshot\" of a directory and remove only files we process.\n-         */\n-        Path inputDirectoryPath = new Path(conf.get(MetricsConfig.INPUT_DIRECTORY));\n-        FileSystem fs = FileSystem.get(inputDirectoryPath.toUri(), conf);\n-        FileStatus[] fstats = fs.listStatus(inputDirectoryPath);\n-        Path[] inPaths = {};\n-        if (fstats != null && fstats.length > 0) {\n-            inPaths = FileUtil.stat2Paths(fstats);\n-            FileInputFormat.setInputPaths(job, inPaths);\n-        }\n-        \n-        Collection<Range> ranges = new ArrayList<>();\n-        \n-        BatchWriterConfig bwConfig = new BatchWriterConfig().setMaxLatency(1000L, TimeUnit.MILLISECONDS).setMaxMemory(1024L).setMaxWriteThreads(4);\n-        \n-        // job name is in form\n-        // IngestJob_yyyyMMddHHmmss.553\n-        // 20120829134659.\n-        // 20120829135352\n-        SimpleDateFormat outFormat = new SimpleDateFormat(\"yyyyMMddHH\");\n-        \n-        String jobNamePrefix = \"IngestJob_\";\n-        String date = null;\n-        Date dateObj;\n-        \n-        try (BatchWriter writer = Connections.warehouseConnection(conf).createBatchWriter(\n-                        conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), bwConfig)) {\n-            \n-            Mutation m = new Mutation(\"metrics\");\n-            for (Path path : inPaths) {\n-                \n-                String jobName = path.getName();\n-                if (jobName.contains(\".metrics\")) {\n-                    jobName = jobName.substring(0, jobName.indexOf(\".metrics\"));\n-                }\n-                \n-                if (jobName.startsWith((jobNamePrefix))) {\n-                    int end = jobName.lastIndexOf(\".\");\n-                    if (end < 0)\n-                        end = jobName.length();\n-                    date = jobName.substring(jobNamePrefix.length(), end);\n-                }\n-                \n-                m.put(new Text(date), new Text(\"\"), emptyValue);\n-                \n-            }\n-            \n-            if (m.size() > 0) {\n-                writer.addMutation(m);\n-            }\n-        }\n-        \n-        Collection<Key> keysToRemove = new ArrayList<>();\n-        try (BatchScanner scanner = Connections.metricsConnection(conf).createBatchScanner(\n-                        conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), Authorizations.EMPTY, 8)) {\n-            scanner.setRanges(Collections.singleton(new Range(new Text(\"metrics\"))));\n-            \n-            Iterator<Entry<Key,Value>> iter = scanner.iterator();\n-            String cf;\n-            \n-            long oneDay = (24 * 60 * 60 * 1000);\n-            \n-            Calendar calendar = Calendar.getInstance();\n-            calendar.setTimeInMillis(System.currentTimeMillis() - oneDay);\n-            \n-            Key iterKey;\n-            while (iter.hasNext()) {\n-                iterKey = iter.next().getKey();\n-                cf = iterKey.getColumnFamily().toString();\n-                try {\n-                    dateObj = DateHelper.parseTimeExactToSeconds(cf);\n-                    Date dateObjNext = DateHelper.addHours(dateObj, 1);\n-                    if (calendar.getTime().compareTo(dateObj) > 0) {\n-                        // remove the entries older than 24 hrs. If we are restarting after a long pause,\n-                        // then those entries will be removed following successful access.\n-                        keysToRemove.add(iterKey);\n-                        \n-                    }\n-                    \n-                    ranges.add(new Range(new Key(new Text(\"IngestJob_\" + outFormat.format(dateObj))), new Key(new Text(\"IngestJob_\"\n-                                    + outFormat.format(dateObjNext)))));\n-                    \n-                } catch (DateTimeParseException e) {\n-                    log.error(e);\n-                }\n-                \n-            }\n-        }\n-        \n-        Collection<Pair<Text,Text>> columns = new ArrayList<>();\n-        columns.add(new Pair<>(new Text(\"e\"), null));\n-        columns.add(new Pair<>(new Text(\"info\"), null));\n-        \n-        AccumuloInputFormat.fetchColumns(job, columns);\n-        \n-        AccumuloInputFormat.setRanges(job, ranges);\n-        \n-        job.setMapOutputKeyClass(Text.class);\n-        job.setMapOutputValueClass(Text.class);\n-        \n-        job.setMapperClass(ProcessingErrorsMapper.class);\n-        \n-        job.setReducerClass(ProcessingErrorsReducer.class);\n-        job.setNumReduceTasks(1);\n-        \n-        PasswordToken warehousePW = new PasswordToken(conf.get(MetricsConfig.WAREHOUSE_PASSWORD, \"\"));\n-        ClientConfiguration zkConfig = ClientConfiguration.loadDefault().withInstance(conf.get(MetricsConfig.WAREHOUSE_INSTANCE))\n-                        .withZkHosts(conf.get(MetricsConfig.WAREHOUSE_ZOOKEEPERS));\n-        ZooKeeperInstance instance = new ZooKeeperInstance(zkConfig);\n-        Connector connector = instance.getConnector(conf.get(MetricsConfig.WAREHOUSE_USERNAME), warehousePW);\n-        \n-        AccumuloInputFormat.setZooKeeperInstance(job, zkConfig);\n-        AccumuloInputFormat.setConnectorInfo(job, conf.get(MetricsConfig.WAREHOUSE_USERNAME), warehousePW);\n-        AccumuloInputFormat.setInputTableName(job, conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE));\n-        AccumuloInputFormat.setScanAuthorizations(job, connector.securityOperations().getUserAuthorizations(conf.get(MetricsConfig.WAREHOUSE_USERNAME)));\n-        job.setInputFormatClass(AccumuloInputFormat.class);\n-        job.setOutputFormatClass(AccumuloOutputFormat.class);\n-        AccumuloOutputFormat.setZooKeeperInstance(job, zkConfig);\n-        AccumuloOutputFormat.setConnectorInfo(job, conf.get(MetricsConfig.WAREHOUSE_USERNAME), warehousePW);\n-        AccumuloOutputFormat.setCreateTables(job, createTables);\n-        AccumuloOutputFormat.setDefaultTableName(job, outTable);\n-        AccumuloOutputFormat.setBatchWriterOptions(job, new BatchWriterConfig().setMaxLatency(25, TimeUnit.MILLISECONDS));\n-        \n-        if (job.waitForCompletion(true)) {\n-            if (!keysToRemove.isEmpty()) {\n-                bwConfig = new BatchWriterConfig().setMaxLatency(1024L, TimeUnit.MILLISECONDS).setMaxMemory(1024L).setMaxWriteThreads(8);\n-                try (BatchWriter writer = Connections.metricsConnection(conf).createBatchWriter(\n-                                conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), bwConfig)) {\n-                    \n-                    Mutation m = new Mutation(\"metrics\");\n-                    for (Key key : keysToRemove) {\n-                        \n-                        m.putDelete(key.getColumnFamily(), key.getColumnQualifier());\n-                        \n-                    }\n-                    writer.addMutation(m);\n-                }\n-            }\n+\n+    /* Type logic so I can differeniate between loader and ingest metrics jobs */\n+    Class<? extends Mapper<?, ?, ?, ?>> mapperClass;\n+    String outTable;\n+\n+    Path inputDirectoryPath = new Path(conf.get(MetricsConfig.INPUT_DIRECTORY));\n+    FileSystem fs = FileSystem.get(inputDirectoryPath.toUri(), conf);\n+    FileStatus[] fstats = fs.listStatus(inputDirectoryPath);\n+    Path[] files = FileUtil.stat2Paths(fstats);\n+    Path[] fileBuffer = new Path[MAX_FILES];\n+    for (int i = 0; i < files.length; ) {\n+      Job job = Job.getInstance(getConf());\n+      job.setJarByClass(this.getClass());\n+\n+      job.getConfiguration().setInt(\"mapred.job.reuse.jvm.num.tasks\", -1);\n+\n+      if (\"ingest\".equalsIgnoreCase(type)) {\n+        mapperClass = IngestMetricsMapper.class;\n+        outTable = conf.get(MetricsConfig.INGEST_TABLE, MetricsConfig.DEFAULT_INGEST_TABLE);\n+        job.setInputFormatClass(SequenceFileInputFormat.class);\n+      } else if (\"loader\".equalsIgnoreCase(type)) {\n+        mapperClass = LoaderMetricsMapper.class;\n+        outTable = conf.get(MetricsConfig.LOADER_TABLE, MetricsConfig.DEFAULT_LOADER_TABLE);\n+        job.setInputFormatClass(SequenceFileInputFormat.class);\n+      } else if (\"flagmaker\".equalsIgnoreCase(type)) {\n+        mapperClass = FlagMakerMetricsMapper.class;\n+        outTable = conf.get(MetricsConfig.FLAGMAKER_TABLE, MetricsConfig.DEFAULT_FLAGMAKER_TABLE);\n+        job.setInputFormatClass(SequenceFileInputFormat.class);\n+      } else {\n+        log.error(type + \" is not a valid job type. Please use <ingest|loader>.\");\n+        return -1;\n+      }\n+\n+      job.setJobName(\"MetricsIngester-\" + type);\n+\n+      if (files.length - i > MAX_FILES) {\n+        System.arraycopy(files, i, fileBuffer, 0, MAX_FILES);\n+        i += MAX_FILES;\n+      } else {\n+        fileBuffer = new Path[files.length - i];\n+        System.arraycopy(files, i, fileBuffer, 0, fileBuffer.length);\n+        i += files.length - i;\n+      }\n+\n+      SequenceFileInputFormat.setInputPaths(job, fileBuffer);\n+\n+      job.setMapperClass(mapperClass);\n+\n+      job.setNumReduceTasks(0);\n+\n+      job.setOutputFormatClass(AccumuloOutputFormat.class);\n+      AccumuloOutputFormat.setConnectorInfo(job, conf.get(MetricsConfig.USER),\n+          new PasswordToken(conf.get(MetricsConfig.PASS, \"\").getBytes()));\n+      AccumuloOutputFormat.setCreateTables(job, createTables);\n+      AccumuloOutputFormat.setDefaultTableName(job, outTable);\n+      log.info(\"zookeepers = \" + conf.get(MetricsConfig.ZOOKEEPERS));\n+      log.info(\"instance = \" + conf.get(MetricsConfig.INSTANCE));\n+      log.info(\"clientConfuguration = \"\n+          + ClientConfiguration.loadDefault().withInstance(conf.get(MetricsConfig.INSTANCE))\n+          .withZkHosts(conf.get(MetricsConfig.ZOOKEEPERS)));\n+      AccumuloOutputFormat.setZooKeeperInstance(job,\n+          ClientConfiguration.loadDefault().withInstance(conf.get(MetricsConfig.INSTANCE))\n+              .withZkHosts(conf.get(MetricsConfig.ZOOKEEPERS)));\n+      AccumuloOutputFormat.setBatchWriterOptions(job, new BatchWriterConfig().setMaxLatency(25, TimeUnit.MILLISECONDS));\n+\n+      job.submit();\n+\n+      job.waitForCompletion(true);\n+\n+      if (job.isSuccessful()) {\n+        for (Path p : fileBuffer) {\n+          fs.delete(p, true);\n         }\n-        return 0;\n+      }\n     }\n-    \n+\n+    return 0;\n+  }\n+\n+  protected int launchErrorsJob(Job job, Configuration conf)\n+      throws IOException, InterruptedException, ClassNotFoundException, AccumuloException,\n+      AccumuloSecurityException, TableNotFoundException {\n+    job.setJobName(\"ErrorMetricsIngest\");\n+\n+    job.setJarByClass(this.getClass());\n+\n+    String outTable = conf.get(MetricsConfig.METRICS_TABLE, MetricsConfig.DEFAULT_METRICS_TABLE);\n+\n     /*\n-     * Goes through the arguments and attempts to add relevant values to the configuration\n+     * This block allows for us to read from a virtual \"snapshot\" of a directory and remove only files we process.\n      */\n-    private void _configure(String[] args) {\n-        GnuParser parser = new GnuParser();\n-        CommandLine cmd;\n-        try {\n-            cmd = parser.parse(new MetricsOptions(), args);\n-        } catch (ParseException e) {\n-            log.warn(\"Could not parse command line options. Defaults from metrics.xml will be used.\", e);\n-            return;\n+    Path inputDirectoryPath = new Path(conf.get(MetricsConfig.INPUT_DIRECTORY));\n+    FileSystem fs = FileSystem.get(inputDirectoryPath.toUri(), conf);\n+    FileStatus[] fstats = fs.listStatus(inputDirectoryPath);\n+    Path[] inPaths = {};\n+    if (fstats != null && fstats.length > 0) {\n+      inPaths = FileUtil.stat2Paths(fstats);\n+      FileInputFormat.setInputPaths(job, inPaths);\n+    }\n+\n+    Collection<Range> ranges = new ArrayList<>();\n+\n+    BatchWriterConfig bwConfig =\n+        new BatchWriterConfig().setMaxLatency(1000L, TimeUnit.MILLISECONDS).setMaxMemory(1024L).setMaxWriteThreads(4);\n+\n+    // job name is in form\n+    // IngestJob_yyyyMMddHHmmss.553\n+    // 20120829134659.\n+    // 20120829135352\n+    SimpleDateFormat outFormat = new SimpleDateFormat(\"yyyyMMddHH\");\n+\n+    String jobNamePrefix = \"IngestJob_\";\n+    String date = null;\n+    Date dateObj;\n+\n+    try (BatchWriter writer = Connections.warehouseConnection(conf).createBatchWriter(\n+        conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), bwConfig)) {\n+\n+      Mutation m = new Mutation(\"metrics\");\n+      for (Path path : inPaths) {\n+\n+        String jobName = path.getName();\n+        if (jobName.contains(\".metrics\")) {\n+          jobName = jobName.substring(0, jobName.indexOf(\".metrics\"));\n         }\n-        \n-        Configuration conf = getConf();\n-        URL metricsConfig = MetricsIngester.class.getClassLoader().getResource(\"metrics.xml\");\n-        if (metricsConfig != null) {\n-            conf.addResource(metricsConfig);\n+\n+        if (jobName.startsWith((jobNamePrefix))) {\n+          int end = jobName.lastIndexOf(\".\");\n+          if (end < 0) {\n+            end = jobName.length();\n+          }\n+          date = jobName.substring(jobNamePrefix.length(), end);\n         }\n-        for (Option opt : cmd.getOptions()) {\n-            conf.set(MetricsConfig.MTX + opt.getOpt(), opt.getValue());\n+\n+        m.put(new Text(date), new Text(\"\"), emptyValue);\n+\n+      }\n+\n+      if (m.size() > 0) {\n+        writer.addMutation(m);\n+      }\n+    }\n+\n+    Collection<Key> keysToRemove = new ArrayList<>();\n+    try (BatchScanner scanner = Connections.metricsConnection(conf).createBatchScanner(\n+        conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), Authorizations.EMPTY, 8)) {\n+      scanner.setRanges(Collections.singleton(new Range(new Text(\"metrics\"))));\n+\n+      Iterator<Entry<Key, Value>> iter = scanner.iterator();\n+      String cf;\n+\n+      long oneDay = (24 * 60 * 60 * 1000);\n+\n+      Calendar calendar = Calendar.getInstance();\n+      calendar.setTimeInMillis(System.currentTimeMillis() - oneDay);\n+\n+      Key iterKey;\n+      while (iter.hasNext()) {\n+        iterKey = iter.next().getKey();\n+        cf = iterKey.getColumnFamily().toString();\n+        try {\n+          dateObj = DateHelper.parseTimeExactToSeconds(cf);\n+          Date dateObjNext = DateHelper.addHours(dateObj, 1);\n+          if (calendar.getTime().compareTo(dateObj) > 0) {\n+            // remove the entries older than 24 hrs. If we are restarting after a long pause,\n+            // then those entries will be removed following successful access.\n+            keysToRemove.add(iterKey);\n+\n+          }\n+\n+          ranges\n+              .add(new Range(new Key(new Text(\"IngestJob_\" + outFormat.format(dateObj))), new Key(new Text(\"IngestJob_\"\n+                  + outFormat.format(dateObjNext)))));\n+\n+        } catch (DateTimeParseException e) {\n+          log.error(e);\n+        }\n+\n+      }\n+    }\n+\n+    Collection<Pair<Text, Text>> columns = new ArrayList<>();\n+    columns.add(new Pair<>(new Text(\"e\"), null));\n+    columns.add(new Pair<>(new Text(\"info\"), null));\n+\n+    AccumuloInputFormat.fetchColumns(job, columns);\n+\n+    AccumuloInputFormat.setRanges(job, ranges);\n+\n+    job.setMapOutputKeyClass(Text.class);\n+    job.setMapOutputValueClass(Text.class);\n+\n+    job.setMapperClass(ProcessingErrorsMapper.class);\n+\n+    job.setReducerClass(ProcessingErrorsReducer.class);\n+    job.setNumReduceTasks(1);\n+\n+    PasswordToken warehousePW = new PasswordToken(conf.get(MetricsConfig.WAREHOUSE_PASSWORD, \"\"));\n+    ClientConfiguration zkConfig =\n+        ClientConfiguration.loadDefault().withInstance(conf.get(MetricsConfig.WAREHOUSE_INSTANCE))\n+            .withZkHosts(conf.get(MetricsConfig.WAREHOUSE_ZOOKEEPERS));\n+    ZooKeeperInstance instance = new ZooKeeperInstance(zkConfig);\n+    Connector connector = instance.getConnector(conf.get(MetricsConfig.WAREHOUSE_USERNAME), warehousePW);\n+\n+    AccumuloInputFormat.setZooKeeperInstance(job, zkConfig);\n+    AccumuloInputFormat.setConnectorInfo(job, conf.get(MetricsConfig.WAREHOUSE_USERNAME), warehousePW);\n+    AccumuloInputFormat\n+        .setInputTableName(job, conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE));\n+    AccumuloInputFormat.setScanAuthorizations(job,\n+        connector.securityOperations().getUserAuthorizations(conf.get(MetricsConfig.WAREHOUSE_USERNAME)));\n+    job.setInputFormatClass(AccumuloInputFormat.class);\n+    job.setOutputFormatClass(AccumuloOutputFormat.class);\n+    AccumuloOutputFormat.setZooKeeperInstance(job, zkConfig);\n+    AccumuloOutputFormat.setConnectorInfo(job, conf.get(MetricsConfig.WAREHOUSE_USERNAME), warehousePW);\n+    AccumuloOutputFormat.setCreateTables(job, createTables);\n+    AccumuloOutputFormat.setDefaultTableName(job, outTable);\n+    AccumuloOutputFormat.setBatchWriterOptions(job, new BatchWriterConfig().setMaxLatency(25, TimeUnit.MILLISECONDS));\n+\n+    if (job.waitForCompletion(true)) {\n+      if (!keysToRemove.isEmpty()) {\n+        bwConfig = new BatchWriterConfig().setMaxLatency(1024L, TimeUnit.MILLISECONDS).setMaxMemory(1024L)\n+            .setMaxWriteThreads(8);\n+        try (BatchWriter writer = Connections.metricsConnection(conf).createBatchWriter(\n+            conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), bwConfig)) {\n+\n+          Mutation m = new Mutation(\"metrics\");\n+          for (Key key : keysToRemove) {\n+\n+            m.putDelete(key.getColumnFamily(), key.getColumnQualifier());\n+\n+          }\n+          writer.addMutation(m);\n         }\n+      }\n     }\n-    \n-    public static void main(String[] args) throws Exception {\n-        \n-        System.exit(ToolRunner.run(new MetricsIngester(), args));\n+    return 0;\n+  }\n+\n+  /*\n+   * Goes through the arguments and attempts to add relevant values to the configuration\n+   */\n+  private void _configure(String[] args) {\n+    GnuParser parser = new GnuParser();\n+    CommandLine cmd;\n+    try {\n+      cmd = parser.parse(new MetricsOptions(), args);\n+    } catch (ParseException e) {\n+      log.warn(\"Could not parse command line options. Defaults from metrics.xml will be used.\", e);\n+      return;\n+    }\n+\n+    Configuration conf = getConf();\n+    URL metricsConfig = MetricsIngester.class.getClassLoader().getResource(\"metrics.xml\");\n+    if (metricsConfig != null) {\n+      conf.addResource(metricsConfig);\n     }\n-    \n+    for (Option opt : cmd.getOptions()) {\n+      conf.set(MetricsConfig.MTX + opt.getOpt(), opt.getValue());\n+    }\n+  }\n+\n+  public static void main(String[] args) throws Exception {\n+\n+    System.exit(ToolRunner.run(new MetricsIngester(), args));\n+  }\n+\n }\n",
            "diff_size": 438
        },
        {
            "tool": "naturalize",
            "errors": [
                {
                    "line": "26",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/113/MetricsIngester.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/naturalize/113/MetricsIngester.java\nindex 710aeefdb17..e24b359888a 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/113/MetricsIngester.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/naturalize/113/MetricsIngester.java\n@@ -217,10 +217,8 @@ public class MetricsIngester extends Configured implements Tool {\n         String date = null;\n         Date dateObj;\n         \n-        try (BatchWriter writer = Connections.warehouseConnection(conf).createBatchWriter(\n-                        conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), bwConfig)) {\n-            \n-            Mutation m = new Mutation(\"metrics\");\n+        try (BatchWriter writer = Connections.warehouseConnection(conf).createBatchWriter(conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), bwConfig)) {\n+    Mutation m = new Mutation(\"metrics\");\n             for (Path path : inPaths) {\n                 \n                 String jobName = path.getName();\n@@ -271,10 +269,8 @@ public class MetricsIngester extends Configured implements Tool {\n                         \n                     }\n                     \n-                    ranges.add(new Range(new Key(new Text(\"IngestJob_\" + outFormat.format(dateObj))), new Key(new Text(\"IngestJob_\"\n-                                    + outFormat.format(dateObjNext)))));\n-                    \n-                } catch (DateTimeParseException e) {\n+                    ranges.add(new Range(new Key(new Text(\"IngestJob_\" + outFormat.format(dateObj))), new Key(new Text(\"IngestJob_\" + outFormat.format(dateObjNext)))));\n+    } catch (DateTimeParseException e) {\n                     log.error(e);\n                 }\n                 \n@@ -323,8 +319,7 @@ public class MetricsIngester extends Configured implements Tool {\n                     \n                     Mutation m = new Mutation(\"metrics\");\n                     for (Key key : keysToRemove) {\n-                        \n-                        m.putDelete(key.getColumnFamily(), key.getColumnQualifier());\n+    m.putDelete(key.getColumnFamily(), key.getColumnQualifier());\n                         \n                     }\n                     writer.addMutation(m);\n@@ -362,4 +357,4 @@ public class MetricsIngester extends Configured implements Tool {\n         System.exit(ToolRunner.run(new MetricsIngester(), args));\n     }\n     \n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 11
        },
        {
            "tool": "codebuff",
            "errors": [
                {
                    "line": "26",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/113/MetricsIngester.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/codebuff/113/MetricsIngester.java\nindex 710aeefdb17..72760fd9c32 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/113/MetricsIngester.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/codebuff/113/MetricsIngester.java\n@@ -42,7 +42,6 @@ import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;\n import org.apache.hadoop.util.Tool;\n import org.apache.hadoop.util.ToolRunner;\n import org.apache.log4j.Logger;\n-\n import java.io.IOException;\n import java.net.URL;\n import java.text.SimpleDateFormat;\n@@ -76,20 +75,19 @@ import java.util.concurrent.TimeUnit;\n  * (2) This job will create the table specified as an argument if it does not already exist.\n  * \n  */\n+\n+\n public class MetricsIngester extends Configured implements Tool {\n     private static final boolean createTables = true;\n-    \n     private static final Logger log = Logger.getLogger(MetricsIngester.class);\n-    \n     protected static final byte[] emptyBytes = {};\n     protected static final Value emptyValue = new Value(emptyBytes);\n-    \n     private static final int MAX_FILES = 2000;\n-    \n+\n     @Override\n     public int run(String[] args) throws Exception {\n         _configure(args);\n-        \n+\n         final Configuration conf = getConf();\n         String type = conf.get(MetricsConfig.TYPE);\n         \n@@ -109,9 +107,9 @@ public class MetricsIngester extends Configured implements Tool {\n         }\n         \n         /* Type logic so I can differeniate between loader and ingest metrics jobs */\n+\n         Class<? extends Mapper<?,?,?,?>> mapperClass;\n         String outTable;\n-        \n         Path inputDirectoryPath = new Path(conf.get(MetricsConfig.INPUT_DIRECTORY));\n         FileSystem fs = FileSystem.get(inputDirectoryPath.toUri(), conf);\n         FileStatus[] fstats = fs.listStatus(inputDirectoryPath);\n@@ -120,9 +118,7 @@ public class MetricsIngester extends Configured implements Tool {\n         for (int i = 0; i < files.length;) {\n             Job job = Job.getInstance(getConf());\n             job.setJarByClass(this.getClass());\n-            \n             job.getConfiguration().setInt(\"mapred.job.reuse.jvm.num.tasks\", -1);\n-            \n             if (\"ingest\".equalsIgnoreCase(type)) {\n                 mapperClass = IngestMetricsMapper.class;\n                 outTable = conf.get(MetricsConfig.INGEST_TABLE, MetricsConfig.DEFAULT_INGEST_TABLE);\n@@ -131,17 +127,15 @@ public class MetricsIngester extends Configured implements Tool {\n                 mapperClass = LoaderMetricsMapper.class;\n                 outTable = conf.get(MetricsConfig.LOADER_TABLE, MetricsConfig.DEFAULT_LOADER_TABLE);\n                 job.setInputFormatClass(SequenceFileInputFormat.class);\n-            } else if (\"flagmaker\".equalsIgnoreCase(type)) {\n-                mapperClass = FlagMakerMetricsMapper.class;\n-                outTable = conf.get(MetricsConfig.FLAGMAKER_TABLE, MetricsConfig.DEFAULT_FLAGMAKER_TABLE);\n-                job.setInputFormatClass(SequenceFileInputFormat.class);\n+                                           } else if (\"flagmaker\".equalsIgnoreCase(type)) {\n+                       mapperClass = FlagMakerMetricsMapper.class;\n+                       outTable = conf.get(MetricsConfig.FLAGMAKER_TABLE, MetricsConfig.DEFAULT_FLAGMAKER_TABLE);\n+                       job.setInputFormatClass(SequenceFileInputFormat.class);\n             } else {\n-                log.error(type + \" is not a valid job type. Please use <ingest|loader>.\");\n-                return -1;\n-            }\n-            \n+                       log.error(type + \" is not a valid job type. Please use <ingest|loader>.\");\n+                       return -1;\n+                   }\n             job.setJobName(\"MetricsIngester-\" + type);\n-            \n             if (files.length - i > MAX_FILES) {\n                 System.arraycopy(files, i, fileBuffer, 0, MAX_FILES);\n                 i += MAX_FILES;\n@@ -150,45 +144,34 @@ public class MetricsIngester extends Configured implements Tool {\n                 System.arraycopy(files, i, fileBuffer, 0, fileBuffer.length);\n                 i += files.length - i;\n             }\n-            \n             SequenceFileInputFormat.setInputPaths(job, fileBuffer);\n-            \n             job.setMapperClass(mapperClass);\n-            \n             job.setNumReduceTasks(0);\n-            \n             job.setOutputFormatClass(AccumuloOutputFormat.class);\n             AccumuloOutputFormat.setConnectorInfo(job, conf.get(MetricsConfig.USER), new PasswordToken(conf.get(MetricsConfig.PASS, \"\").getBytes()));\n             AccumuloOutputFormat.setCreateTables(job, createTables);\n             AccumuloOutputFormat.setDefaultTableName(job, outTable);\n             log.info(\"zookeepers = \" + conf.get(MetricsConfig.ZOOKEEPERS));\n             log.info(\"instance = \" + conf.get(MetricsConfig.INSTANCE));\n-            log.info(\"clientConfuguration = \"\n-                            + ClientConfiguration.loadDefault().withInstance(conf.get(MetricsConfig.INSTANCE)).withZkHosts(conf.get(MetricsConfig.ZOOKEEPERS)));\n+            log.info(\"clientConfuguration = \" + ClientConfiguration.loadDefault().withInstance(conf.get(MetricsConfig.INSTANCE)).withZkHosts(conf.get(MetricsConfig.ZOOKEEPERS)));\n             AccumuloOutputFormat.setZooKeeperInstance(job,\n-                            ClientConfiguration.loadDefault().withInstance(conf.get(MetricsConfig.INSTANCE)).withZkHosts(conf.get(MetricsConfig.ZOOKEEPERS)));\n+                ClientConfiguration.loadDefault().withInstance(conf.get(MetricsConfig.INSTANCE)).withZkHosts(conf.get(MetricsConfig.ZOOKEEPERS)));\n             AccumuloOutputFormat.setBatchWriterOptions(job, new BatchWriterConfig().setMaxLatency(25, TimeUnit.MILLISECONDS));\n-            \n             job.submit();\n-            \n             job.waitForCompletion(true);\n-            \n             if (job.isSuccessful()) {\n                 for (Path p : fileBuffer) {\n                     fs.delete(p, true);\n                 }\n             }\n         }\n-        \n         return 0;\n     }\n-    \n-    protected int launchErrorsJob(Job job, Configuration conf) throws IOException, InterruptedException, ClassNotFoundException, AccumuloException,\n-                    AccumuloSecurityException, TableNotFoundException {\n+\n+    protected int launchErrorsJob(Job job, Configuration conf) throws IOException, InterruptedException, ClassNotFoundException, AccumuloException, AccumuloSecurityException, TableNotFoundException {\n         job.setJobName(\"ErrorMetricsIngest\");\n-        \n         job.setJarByClass(this.getClass());\n-        \n+\n         String outTable = conf.get(MetricsConfig.METRICS_TABLE, MetricsConfig.DEFAULT_METRICS_TABLE);\n         \n         /*\n@@ -202,9 +185,8 @@ public class MetricsIngester extends Configured implements Tool {\n             inPaths = FileUtil.stat2Paths(fstats);\n             FileInputFormat.setInputPaths(job, inPaths);\n         }\n-        \n+\n         Collection<Range> ranges = new ArrayList<>();\n-        \n         BatchWriterConfig bwConfig = new BatchWriterConfig().setMaxLatency(1000L, TimeUnit.MILLISECONDS).setMaxMemory(1024L).setMaxWriteThreads(4);\n         \n         // job name is in form\n@@ -212,97 +194,78 @@ public class MetricsIngester extends Configured implements Tool {\n         // 20120829134659.\n         // 20120829135352\n         SimpleDateFormat outFormat = new SimpleDateFormat(\"yyyyMMddHH\");\n-        \n         String jobNamePrefix = \"IngestJob_\";\n         String date = null;\n         Date dateObj;\n-        \n-        try (BatchWriter writer = Connections.warehouseConnection(conf).createBatchWriter(\n-                        conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), bwConfig)) {\n-            \n+        try (BatchWriter writer = Connections.warehouseConnection(conf).createBatchWriter(conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), bwConfig)) {\n             Mutation m = new Mutation(\"metrics\");\n             for (Path path : inPaths) {\n-                \n                 String jobName = path.getName();\n                 if (jobName.contains(\".metrics\")) {\n                     jobName = jobName.substring(0, jobName.indexOf(\".metrics\"));\n                 }\n-                \n+\n+\n                 if (jobName.startsWith((jobNamePrefix))) {\n                     int end = jobName.lastIndexOf(\".\");\n                     if (end < 0)\n                         end = jobName.length();\n                     date = jobName.substring(jobNamePrefix.length(), end);\n                 }\n-                \n                 m.put(new Text(date), new Text(\"\"), emptyValue);\n-                \n             }\n-            \n+\n+\n             if (m.size() > 0) {\n                 writer.addMutation(m);\n             }\n         }\n-        \n+\n         Collection<Key> keysToRemove = new ArrayList<>();\n-        try (BatchScanner scanner = Connections.metricsConnection(conf).createBatchScanner(\n-                        conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), Authorizations.EMPTY, 8)) {\n+        try (BatchScanner scanner = Connections.metricsConnection(conf).createBatchScanner(conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), Authorizations.EMPTY, 8)) {\n             scanner.setRanges(Collections.singleton(new Range(new Text(\"metrics\"))));\n-            \n+\n             Iterator<Entry<Key,Value>> iter = scanner.iterator();\n             String cf;\n-            \n             long oneDay = (24 * 60 * 60 * 1000);\n-            \n             Calendar calendar = Calendar.getInstance();\n             calendar.setTimeInMillis(System.currentTimeMillis() - oneDay);\n-            \n+\n             Key iterKey;\n             while (iter.hasNext()) {\n                 iterKey = iter.next().getKey();\n                 cf = iterKey.getColumnFamily().toString();\n                 try {\n                     dateObj = DateHelper.parseTimeExactToSeconds(cf);\n+\n                     Date dateObjNext = DateHelper.addHours(dateObj, 1);\n                     if (calendar.getTime().compareTo(dateObj) > 0) {\n                         // remove the entries older than 24 hrs. If we are restarting after a long pause,\n                         // then those entries will be removed following successful access.\n                         keysToRemove.add(iterKey);\n-                        \n                     }\n-                    \n-                    ranges.add(new Range(new Key(new Text(\"IngestJob_\" + outFormat.format(dateObj))), new Key(new Text(\"IngestJob_\"\n-                                    + outFormat.format(dateObjNext)))));\n-                    \n+                    ranges.add(new Range(new Key(new Text(\"IngestJob_\" + outFormat.format(dateObj))), new Key(new Text(\"IngestJob_\" + outFormat.format(dateObjNext)))));\n                 } catch (DateTimeParseException e) {\n                     log.error(e);\n                 }\n-                \n             }\n         }\n-        \n+\n         Collection<Pair<Text,Text>> columns = new ArrayList<>();\n         columns.add(new Pair<>(new Text(\"e\"), null));\n         columns.add(new Pair<>(new Text(\"info\"), null));\n-        \n         AccumuloInputFormat.fetchColumns(job, columns);\n-        \n         AccumuloInputFormat.setRanges(job, ranges);\n-        \n         job.setMapOutputKeyClass(Text.class);\n         job.setMapOutputValueClass(Text.class);\n-        \n         job.setMapperClass(ProcessingErrorsMapper.class);\n-        \n         job.setReducerClass(ProcessingErrorsReducer.class);\n         job.setNumReduceTasks(1);\n-        \n+\n         PasswordToken warehousePW = new PasswordToken(conf.get(MetricsConfig.WAREHOUSE_PASSWORD, \"\"));\n-        ClientConfiguration zkConfig = ClientConfiguration.loadDefault().withInstance(conf.get(MetricsConfig.WAREHOUSE_INSTANCE))\n-                        .withZkHosts(conf.get(MetricsConfig.WAREHOUSE_ZOOKEEPERS));\n+        ClientConfiguration zkConfig = ClientConfiguration.loadDefault().withInstance(conf.get(MetricsConfig.WAREHOUSE_INSTANCE)).withZkHosts(conf.get(MetricsConfig.WAREHOUSE_ZOOKEEPERS));\n         ZooKeeperInstance instance = new ZooKeeperInstance(zkConfig);\n         Connector connector = instance.getConnector(conf.get(MetricsConfig.WAREHOUSE_USERNAME), warehousePW);\n-        \n         AccumuloInputFormat.setZooKeeperInstance(job, zkConfig);\n         AccumuloInputFormat.setConnectorInfo(job, conf.get(MetricsConfig.WAREHOUSE_USERNAME), warehousePW);\n         AccumuloInputFormat.setInputTableName(job, conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE));\n@@ -314,18 +277,13 @@ public class MetricsIngester extends Configured implements Tool {\n         AccumuloOutputFormat.setCreateTables(job, createTables);\n         AccumuloOutputFormat.setDefaultTableName(job, outTable);\n         AccumuloOutputFormat.setBatchWriterOptions(job, new BatchWriterConfig().setMaxLatency(25, TimeUnit.MILLISECONDS));\n-        \n         if (job.waitForCompletion(true)) {\n             if (!keysToRemove.isEmpty()) {\n                 bwConfig = new BatchWriterConfig().setMaxLatency(1024L, TimeUnit.MILLISECONDS).setMaxMemory(1024L).setMaxWriteThreads(8);\n-                try (BatchWriter writer = Connections.metricsConnection(conf).createBatchWriter(\n-                                conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), bwConfig)) {\n-                    \n+                try (BatchWriter writer = Connections.metricsConnection(conf).createBatchWriter(conf.get(MetricsConfig.ERRORS_TABLE, MetricsConfig.DEFAULT_ERRORS_TABLE), bwConfig)) {\n                     Mutation m = new Mutation(\"metrics\");\n                     for (Key key : keysToRemove) {\n-                        \n                         m.putDelete(key.getColumnFamily(), key.getColumnQualifier());\n-                        \n                     }\n                     writer.addMutation(m);\n                 }\n@@ -337,6 +295,7 @@ public class MetricsIngester extends Configured implements Tool {\n     /*\n      * Goes through the arguments and attempts to add relevant values to the configuration\n      */\n+\n     private void _configure(String[] args) {\n         GnuParser parser = new GnuParser();\n         CommandLine cmd;\n@@ -346,7 +305,7 @@ public class MetricsIngester extends Configured implements Tool {\n             log.warn(\"Could not parse command line options. Defaults from metrics.xml will be used.\", e);\n             return;\n         }\n-        \n+\n         Configuration conf = getConf();\n         URL metricsConfig = MetricsIngester.class.getClassLoader().getResource(\"metrics.xml\");\n         if (metricsConfig != null) {\n@@ -356,10 +315,8 @@ public class MetricsIngester extends Configured implements Tool {\n             conf.set(MetricsConfig.MTX + opt.getOpt(), opt.getValue());\n         }\n     }\n-    \n+\n     public static void main(String[] args) throws Exception {\n-        \n         System.exit(ToolRunner.run(new MetricsIngester(), args));\n     }\n-    \n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 87
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "26",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "26",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        }
    ],
    "repaired_by": [],
    "not_repaired_by": [
        "styler",
        "intellij",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ]
}