{
    "project_name": "NationalSecurityAgency-datawave",
    "error_id": "83",
    "information": {
        "errors": [
            {
                "line": "32",
                "severity": "error",
                "message": "Accumulo non-public classes imported",
                "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
            }
        ]
    },
    "source_code": "import org.apache.accumulo.core.data.Value;\nimport org.apache.accumulo.core.security.Authorizations;\nimport org.apache.accumulo.hadoop.mapreduce.AccumuloOutputFormat;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.SequenceFile.CompressionType;",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "32",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "intellij",
            "errors": [
                {
                    "line": "32",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/83/BulkResultsJobConfiguration.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/intellij/83/BulkResultsJobConfiguration.java\nindex aa397fa92c7..02d7a037f10 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/83/BulkResultsJobConfiguration.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/intellij/83/BulkResultsJobConfiguration.java\n@@ -54,356 +54,364 @@ import java.util.Properties;\n import java.util.Set;\n import java.util.concurrent.TimeUnit;\n \n-public class BulkResultsJobConfiguration extends MapReduceJobConfiguration implements NeedCallerDetails, NeedAccumuloConnectionFactory, NeedAccumuloDetails,\n-                NeedQueryLogicFactory, NeedQueryPersister, NeedQueryCache {\n-    \n-    /**\n-     * Container for query settings\n-     *\n-     */\n-    public static class QuerySettings {\n-        private QueryLogic<?> logic = null;\n-        private GenericQueryConfiguration queryConfig = null;\n-        private String base64EncodedQuery = null;\n-        private Set<Authorizations> runtimeQueryAuthorizations = null;\n-        private Class<? extends Query> queryImplClass;\n-        \n-        public QuerySettings(QueryLogic<?> logic, GenericQueryConfiguration queryConfig, String base64EncodedQuery, Class<? extends Query> queryImplClass,\n-                        Set<Authorizations> runtimeQueryAuthorizations) {\n-            super();\n-            this.logic = logic;\n-            this.queryConfig = queryConfig;\n-            this.base64EncodedQuery = base64EncodedQuery;\n-            this.queryImplClass = queryImplClass;\n-            this.runtimeQueryAuthorizations = runtimeQueryAuthorizations;\n-        }\n-        \n-        public QueryLogic<?> getLogic() {\n-            return logic;\n-        }\n-        \n-        public GenericQueryConfiguration getQueryConfig() {\n-            return queryConfig;\n-        }\n-        \n-        public String getBase64EncodedQuery() {\n-            return base64EncodedQuery;\n-        }\n-        \n-        public Class<? extends Query> getQueryImplClass() {\n-            return queryImplClass;\n-        }\n-        \n-        public Set<Authorizations> getRuntimeQueryAuthorizations() {\n-            return runtimeQueryAuthorizations;\n-        }\n+public class BulkResultsJobConfiguration extends MapReduceJobConfiguration\n+    implements NeedCallerDetails, NeedAccumuloConnectionFactory, NeedAccumuloDetails,\n+    NeedQueryLogicFactory, NeedQueryPersister, NeedQueryCache {\n+\n+  /**\n+   * Container for query settings\n+   */\n+  public static class QuerySettings {\n+    private QueryLogic<?> logic = null;\n+    private GenericQueryConfiguration queryConfig = null;\n+    private String base64EncodedQuery = null;\n+    private Set<Authorizations> runtimeQueryAuthorizations = null;\n+    private Class<? extends Query> queryImplClass;\n+\n+    public QuerySettings(QueryLogic<?> logic, GenericQueryConfiguration queryConfig, String base64EncodedQuery,\n+                         Class<? extends Query> queryImplClass,\n+                         Set<Authorizations> runtimeQueryAuthorizations) {\n+      super();\n+      this.logic = logic;\n+      this.queryConfig = queryConfig;\n+      this.base64EncodedQuery = base64EncodedQuery;\n+      this.queryImplClass = queryImplClass;\n+      this.runtimeQueryAuthorizations = runtimeQueryAuthorizations;\n     }\n-    \n-    private Logger log = Logger.getLogger(this.getClass());\n-    \n-    private AccumuloConnectionFactory connectionFactory;\n-    private QueryLogicFactory queryFactory;\n-    private Persister persister;\n-    private QueryCache runningQueryCache = null;\n-    private String user;\n-    private String password;\n-    private String instanceName;\n-    private String zookeepers;\n-    private String sid;\n-    private Principal principal;\n-    \n-    private String tableName = null;\n-    private Class<? extends OutputFormat> outputFormatClass = SequenceFileOutputFormat.class;\n-    \n-    @Override\n-    public void _initializeConfiguration(Job job, Path jobDir, String jobId, Map<String,String> runtimeParameters, DatawavePrincipal serverPrincipal)\n-                    throws IOException, QueryException {\n-        \n-        String queryId = runtimeParameters.get(\"queryId\");\n-        SerializationFormat format = SerializationFormat.valueOf(runtimeParameters.get(\"format\"));\n-        String outputFormatParameter = runtimeParameters.get(\"outputFormat\");\n-        if (outputFormatParameter != null && outputFormatParameter.equalsIgnoreCase(\"TEXT\")) {\n-            this.outputFormatClass = TextOutputFormat.class;\n-        }\n-        if (runtimeParameters.containsKey(\"outputTableName\"))\n-            this.tableName = runtimeParameters.get(\"outputTableName\");\n-        \n-        // Initialize the Query\n-        QueryLogic<?> logic;\n-        GenericQueryConfiguration queryConfig;\n-        String base64EncodedQuery;\n-        Class<? extends Query> queryImplClass;\n-        Set<Authorizations> runtimeQueryAuthorizations;\n-        \n-        try {\n-            QuerySettings settings = setupQuery(sid, queryId, principal);\n-            logic = settings.getLogic();\n-            queryConfig = settings.getQueryConfig();\n-            base64EncodedQuery = settings.getBase64EncodedQuery();\n-            queryImplClass = settings.getQueryImplClass();\n-            runtimeQueryAuthorizations = settings.getRuntimeQueryAuthorizations();\n-        } catch (QueryException qe) {\n-            log.error(\"Error getting Query for id: \" + queryId, qe);\n-            throw qe;\n-        } catch (Exception e) {\n-            log.error(\"Error setting up Query for id: \" + queryId, e);\n-            throw new QueryException(e);\n-        }\n-        \n-        // Setup and run the MapReduce job\n-        try {\n-            \n-            setupJob(job, jobDir, queryConfig, logic, base64EncodedQuery, queryImplClass, runtimeQueryAuthorizations, serverPrincipal);\n-            \n-            if (null == this.tableName) {\n-                // Setup job for output to HDFS\n-                // set the mapper\n-                job.setMapperClass(BulkResultsFileOutputMapper.class);\n-                job.getConfiguration().set(BulkResultsFileOutputMapper.RESULT_SERIALIZATION_FORMAT, format.name());\n-                // Setup the output\n-                job.setOutputFormatClass(outputFormatClass);\n-                job.setOutputKeyClass(Key.class);\n-                job.setOutputValueClass(Value.class);\n-                if (this.outputFormatClass.equals(SequenceFileOutputFormat.class)) {\n-                    SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);\n-                } else if (this.outputFormatClass.equals(TextOutputFormat.class)) {\n-                    // if we are writing Text output to hdfs, we don't want to write key-tab-value, we want just the value\n-                    // this property gets fetched in the Mapper to skip writing the key\n-                    job.setOutputKeyClass(NullWritable.class);\n-                }\n-                job.setNumReduceTasks(0);\n-                SequenceFileOutputFormat.setOutputPath(job, new Path(this.getResultsDir()));\n-            } else {\n-                // Setup job for output to table.\n-                // set the mapper\n-                job.setMapperClass(BulkResultsTableOutputMapper.class);\n-                job.getConfiguration().set(BulkResultsTableOutputMapper.TABLE_NAME, tableName);\n-                job.getConfiguration().set(BulkResultsFileOutputMapper.RESULT_SERIALIZATION_FORMAT, format.name());\n-                // Setup the output\n-                job.setOutputKeyClass(Text.class);\n-                job.setOutputValueClass(Mutation.class);\n-                job.setNumReduceTasks(0);\n-                job.setOutputFormatClass(AccumuloOutputFormat.class);\n-                \n-                // @formatter:off\n-                Properties clientProps = Accumulo.newClientProperties()\n-                        .to(instanceName, zookeepers)\n-                        .as(user, password)\n-                        .batchWriterConfig(new BatchWriterConfig()\n-                                .setMaxLatency(30, TimeUnit.SECONDS)\n-                                .setMaxMemory(10485760)\n-                                .setMaxWriteThreads(2))\n-                        .build();\n-\n-                AccumuloOutputFormat.configure()\n-                        .clientProperties(clientProps)\n-                        .createTables(true)\n-                        .defaultTable(tableName)\n-                        .store(job);\n-                // @formatter:on\n-                \n-                // AccumuloOutputFormat.loglevel\n-                // TODO: this is not supported on the new output format -- just use normal logging configuration\n-                // AccumuloOutputFormat.setLogLevel(job, Level.INFO);\n-            }\n-        } catch (WebApplicationException wex) {\n-            throw wex;\n-        } catch (Exception e) {\n-            log.error(\"Error starting job\", e);\n-            throw new QueryException(DatawaveErrorCode.JOB_STARTING_ERROR, e);\n-        }\n-        \n+\n+    public QueryLogic<?> getLogic() {\n+      return logic;\n     }\n-    \n-    /**\n-     * Common MapReduce setup methods\n-     *\n-     * @param job\n-     *            the job to configure\n-     * @param jobDir\n-     *            the directory in HDFS where aux job files are stored\n-     * @param queryConfig\n-     *            the query configuration for this job's query input format\n-     * @param logic\n-     *            the query logic for this job's query input format\n-     * @param base64EncodedQuery\n-     *            the query, encoded using Base64\n-     * @param queryImplClass\n-     *            the class of query in {@code base64EncodedQuery}\n-     * @param runtimeQueryAuthorizations\n-     *            the authorizations to use for input format query scanners\n-     * @param serverPrincipal\n-     *            the {@link Principal} of the server running DATAWAVE\n-     * @throws IOException\n-     * @throws AccumuloSecurityException\n-     */\n-    private void setupJob(Job job, Path jobDir, GenericQueryConfiguration queryConfig, QueryLogic<?> logic, String base64EncodedQuery,\n-                    Class<? extends Query> queryImplClass, Set<Authorizations> runtimeQueryAuthorizations, DatawavePrincipal serverPrincipal)\n-                    throws IOException, AccumuloSecurityException {\n-        \n-        job.setInputFormatClass(BulkInputFormat.class);\n-        \n-        QueryData queryData = null;\n-        Collection<Range> ranges = new ArrayList<>();\n-        \n-        if (!queryConfig.canRunQuery()) {\n-            throw new UnsupportedOperationException(\"Unable to run query\");\n-        }\n-        \n-        Iterator<QueryData> iter = queryConfig.getQueries();\n-        while (iter.hasNext()) {\n-            queryData = iter.next();\n-            ranges.addAll(queryData.getRanges());\n-        }\n-        \n-        if (ranges.isEmpty()) {\n-            throw new NoResultsException(new QueryException(\"No scan ranges produced for query.\"));\n-        }\n-        \n-        BulkInputFormat.setWorkingDirectory(job.getConfiguration(), jobDir.toString());\n-        \n-        // Copy the information from the GenericQueryConfiguration to the job.\n-        BulkInputFormat.setRanges(job, ranges);\n-        \n-        for (IteratorSetting cfg : queryData.getSettings()) {\n-            BulkInputFormat.addIterator(job.getConfiguration(), cfg);\n-        }\n-        \n-        BulkInputFormat.setZooKeeperInstance(job.getConfiguration(), this.instanceName, this.zookeepers);\n-        Iterator<Authorizations> authsIter = (runtimeQueryAuthorizations == null || runtimeQueryAuthorizations.isEmpty()) ? null : runtimeQueryAuthorizations\n-                        .iterator();\n-        Authorizations auths = (authsIter == null) ? null : authsIter.next();\n-        BulkInputFormat.setInputInfo(job, this.user, this.password.getBytes(), logic.getTableName(), auths);\n-        for (int priority = 10; authsIter != null && authsIter.hasNext(); ++priority) {\n-            IteratorSetting cfg = new IteratorSetting(priority, ConfigurableVisibilityFilter.class);\n-            cfg.setName(\"visibilityFilter\" + priority);\n-            cfg.addOption(ConfigurableVisibilityFilter.AUTHORIZATIONS_OPT, authsIter.next().toString());\n-            BulkInputFormat.addIterator(job.getConfiguration(), cfg);\n-        }\n-        \n-        job.getConfiguration().set(BulkResultsFileOutputMapper.QUERY_LOGIC_SETTINGS, base64EncodedQuery);\n-        job.getConfiguration().set(BulkResultsFileOutputMapper.QUERY_IMPL_CLASS, queryImplClass.getName());\n-        job.getConfiguration().set(BulkResultsFileOutputMapper.QUERY_LOGIC_NAME, logic.getLogicName());\n-        \n-        job.getConfiguration().set(\n-                        BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH,\n-                        \"classpath*:datawave/configuration/spring/CDIBeanPostProcessor.xml,\" + \"classpath*:datawave/query/*QueryLogicFactory.xml,\"\n-                                        + \"classpath*:/MarkingFunctionsContext.xml,\" + \"classpath*:/CacheContext.xml\");\n-        job.getConfiguration().set(BulkResultsFileOutputMapper.SPRING_CONFIG_LOCATIONS,\n-                        job.getConfiguration().get(BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH));\n-        // Tell the Mapper/Reducer to use a specific set of application context files when doing Spring-CDI integration.\n-        String cdiOpts = \"'-Dcdi.spring.configs=\" + job.getConfiguration().get(BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH) + \"'\";\n-        // Pass our server DN along to the child VM so it can be made available for injection.\n-        cdiOpts += \" '-Dserver.principal=\" + encodeServerPrincipal(serverPrincipal) + \"'\";\n-        String javaOpts = job.getConfiguration().get(\"mapreduce.map.java.opts\");\n-        javaOpts = (javaOpts == null) ? cdiOpts : (javaOpts + \" \" + cdiOpts);\n-        job.getConfiguration().set(\"mapreduce.map.java.opts\", javaOpts);\n-        job.setMapOutputKeyClass(Key.class);\n-        job.setMapOutputValueClass(Value.class);\n-        \n-        job.setWorkingDirectory(jobDir);\n+\n+    public GenericQueryConfiguration getQueryConfig() {\n+      return queryConfig;\n     }\n-    \n-    private String encodeServerPrincipal(DatawavePrincipal serverPrincipal) throws IOException {\n-        return Base64.encodeObject(serverPrincipal, Base64.GZIP | Base64.DONT_BREAK_LINES);\n+\n+    public String getBase64EncodedQuery() {\n+      return base64EncodedQuery;\n     }\n-    \n-    private QuerySettings setupQuery(String sid, String queryId, Principal principal) throws Exception {\n-        \n-        AccumuloClient client = null;\n-        QueryLogic<?> logic = null;\n-        try {\n-            // Get the query by the query id\n-            Query q = getQueryById(queryId);\n-            if (!sid.equals(q.getOwner()))\n-                throw new QueryException(\"This query does not belong to you. expected: \" + q.getOwner() + \", value: \" + sid,\n-                                Response.Status.UNAUTHORIZED.getStatusCode());\n-            \n-            // will throw IllegalArgumentException if not defined\n-            logic = queryFactory.getQueryLogic(q.getQueryLogicName(), principal);\n-            \n-            // Get an accumulo connection\n-            Map<String,String> trackingMap = connectionFactory.getTrackingMap(Thread.currentThread().getStackTrace());\n-            client = connectionFactory.getClient(logic.getConnectionPriority(), trackingMap);\n-            \n-            // Merge user auths with the auths that they use in the Query\n-            Set<Authorizations> runtimeQueryAuthorizations = AuthorizationsUtil.getDowngradedAuthorizations(q.getQueryAuthorizations(), principal);\n-            \n-            // Initialize the logic so that the configuration contains all of the iterator options\n-            GenericQueryConfiguration queryConfig = logic.initialize(client, q, runtimeQueryAuthorizations);\n-            \n-            String base64EncodedQuery = BulkResultsFileOutputMapper.serializeQuery(q);\n-            \n-            return new QuerySettings(logic, queryConfig, base64EncodedQuery, q.getClass(), runtimeQueryAuthorizations);\n-        } finally {\n-            if (null != logic && null != client)\n-                connectionFactory.returnClient(client);\n-        }\n-        \n+\n+    public Class<? extends Query> getQueryImplClass() {\n+      return queryImplClass;\n     }\n-    \n-    private Query getQueryById(String id) throws QueryException {\n-        \n-        RunningQuery runningQuery = runningQueryCache.get(id);\n-        if (null != runningQuery) {\n-            return runningQuery.getSettings();\n-        } else {\n-            List<Query> queries = persister.findById(id);\n-            if (null == queries || queries.isEmpty())\n-                throw new QueryException(\"No query object matches this id\", Response.Status.NOT_FOUND.getStatusCode());\n-            if (queries.size() > 1)\n-                throw new QueryException(\"More than one query object matches the id\", Response.Status.NOT_FOUND.getStatusCode());\n-            return queries.get(0);\n-        }\n+\n+    public Set<Authorizations> getRuntimeQueryAuthorizations() {\n+      return runtimeQueryAuthorizations;\n+    }\n+  }\n+\n+  private Logger log = Logger.getLogger(this.getClass());\n+\n+  private AccumuloConnectionFactory connectionFactory;\n+  private QueryLogicFactory queryFactory;\n+  private Persister persister;\n+  private QueryCache runningQueryCache = null;\n+  private String user;\n+  private String password;\n+  private String instanceName;\n+  private String zookeepers;\n+  private String sid;\n+  private Principal principal;\n+\n+  private String tableName = null;\n+  private Class<? extends OutputFormat> outputFormatClass = SequenceFileOutputFormat.class;\n+\n+  @Override\n+  public void _initializeConfiguration(Job job, Path jobDir, String jobId, Map<String, String> runtimeParameters,\n+                                       DatawavePrincipal serverPrincipal)\n+      throws IOException, QueryException {\n+\n+    String queryId = runtimeParameters.get(\"queryId\");\n+    SerializationFormat format = SerializationFormat.valueOf(runtimeParameters.get(\"format\"));\n+    String outputFormatParameter = runtimeParameters.get(\"outputFormat\");\n+    if (outputFormatParameter != null && outputFormatParameter.equalsIgnoreCase(\"TEXT\")) {\n+      this.outputFormatClass = TextOutputFormat.class;\n     }\n-    \n-    @Override\n-    public void setQueryLogicFactory(QueryLogicFactory factory) {\n-        this.queryFactory = factory;\n+    if (runtimeParameters.containsKey(\"outputTableName\")) {\n+      this.tableName = runtimeParameters.get(\"outputTableName\");\n     }\n-    \n-    @Override\n-    public void setUsername(String username) {\n-        this.user = username;\n+\n+    // Initialize the Query\n+    QueryLogic<?> logic;\n+    GenericQueryConfiguration queryConfig;\n+    String base64EncodedQuery;\n+    Class<? extends Query> queryImplClass;\n+    Set<Authorizations> runtimeQueryAuthorizations;\n+\n+    try {\n+      QuerySettings settings = setupQuery(sid, queryId, principal);\n+      logic = settings.getLogic();\n+      queryConfig = settings.getQueryConfig();\n+      base64EncodedQuery = settings.getBase64EncodedQuery();\n+      queryImplClass = settings.getQueryImplClass();\n+      runtimeQueryAuthorizations = settings.getRuntimeQueryAuthorizations();\n+    } catch (QueryException qe) {\n+      log.error(\"Error getting Query for id: \" + queryId, qe);\n+      throw qe;\n+    } catch (Exception e) {\n+      log.error(\"Error setting up Query for id: \" + queryId, e);\n+      throw new QueryException(e);\n     }\n-    \n-    @Override\n-    public void setPassword(String password) {\n-        this.password = password;\n+\n+    // Setup and run the MapReduce job\n+    try {\n+\n+      setupJob(job, jobDir, queryConfig, logic, base64EncodedQuery, queryImplClass, runtimeQueryAuthorizations,\n+          serverPrincipal);\n+\n+      if (null == this.tableName) {\n+        // Setup job for output to HDFS\n+        // set the mapper\n+        job.setMapperClass(BulkResultsFileOutputMapper.class);\n+        job.getConfiguration().set(BulkResultsFileOutputMapper.RESULT_SERIALIZATION_FORMAT, format.name());\n+        // Setup the output\n+        job.setOutputFormatClass(outputFormatClass);\n+        job.setOutputKeyClass(Key.class);\n+        job.setOutputValueClass(Value.class);\n+        if (this.outputFormatClass.equals(SequenceFileOutputFormat.class)) {\n+          SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);\n+        } else if (this.outputFormatClass.equals(TextOutputFormat.class)) {\n+          // if we are writing Text output to hdfs, we don't want to write key-tab-value, we want just the value\n+          // this property gets fetched in the Mapper to skip writing the key\n+          job.setOutputKeyClass(NullWritable.class);\n+        }\n+        job.setNumReduceTasks(0);\n+        SequenceFileOutputFormat.setOutputPath(job, new Path(this.getResultsDir()));\n+      } else {\n+        // Setup job for output to table.\n+        // set the mapper\n+        job.setMapperClass(BulkResultsTableOutputMapper.class);\n+        job.getConfiguration().set(BulkResultsTableOutputMapper.TABLE_NAME, tableName);\n+        job.getConfiguration().set(BulkResultsFileOutputMapper.RESULT_SERIALIZATION_FORMAT, format.name());\n+        // Setup the output\n+        job.setOutputKeyClass(Text.class);\n+        job.setOutputValueClass(Mutation.class);\n+        job.setNumReduceTasks(0);\n+        job.setOutputFormatClass(AccumuloOutputFormat.class);\n+\n+        // @formatter:off\n+        Properties clientProps = Accumulo.newClientProperties()\n+            .to(instanceName, zookeepers)\n+            .as(user, password)\n+            .batchWriterConfig(new BatchWriterConfig()\n+                .setMaxLatency(30, TimeUnit.SECONDS)\n+                .setMaxMemory(10485760)\n+                .setMaxWriteThreads(2))\n+            .build();\n+\n+        AccumuloOutputFormat.configure()\n+            .clientProperties(clientProps)\n+            .createTables(true)\n+            .defaultTable(tableName)\n+            .store(job);\n+        // @formatter:on\n+\n+        // AccumuloOutputFormat.loglevel\n+        // TODO: this is not supported on the new output format -- just use normal logging configuration\n+        // AccumuloOutputFormat.setLogLevel(job, Level.INFO);\n+      }\n+    } catch (WebApplicationException wex) {\n+      throw wex;\n+    } catch (Exception e) {\n+      log.error(\"Error starting job\", e);\n+      throw new QueryException(DatawaveErrorCode.JOB_STARTING_ERROR, e);\n     }\n-    \n-    @Override\n-    public void setInstanceName(String instanceName) {\n-        this.instanceName = instanceName;\n+\n+  }\n+\n+  /**\n+   * Common MapReduce setup methods\n+   *\n+   * @param job                        the job to configure\n+   * @param jobDir                     the directory in HDFS where aux job files are stored\n+   * @param queryConfig                the query configuration for this job's query input format\n+   * @param logic                      the query logic for this job's query input format\n+   * @param base64EncodedQuery         the query, encoded using Base64\n+   * @param queryImplClass             the class of query in {@code base64EncodedQuery}\n+   * @param runtimeQueryAuthorizations the authorizations to use for input format query scanners\n+   * @param serverPrincipal            the {@link Principal} of the server running DATAWAVE\n+   * @throws IOException\n+   * @throws AccumuloSecurityException\n+   */\n+  private void setupJob(Job job, Path jobDir, GenericQueryConfiguration queryConfig, QueryLogic<?> logic,\n+                        String base64EncodedQuery,\n+                        Class<? extends Query> queryImplClass, Set<Authorizations> runtimeQueryAuthorizations,\n+                        DatawavePrincipal serverPrincipal)\n+      throws IOException, AccumuloSecurityException {\n+\n+    job.setInputFormatClass(BulkInputFormat.class);\n+\n+    QueryData queryData = null;\n+    Collection<Range> ranges = new ArrayList<>();\n+\n+    if (!queryConfig.canRunQuery()) {\n+      throw new UnsupportedOperationException(\"Unable to run query\");\n     }\n-    \n-    @Override\n-    public void setZookeepers(String zookeepers) {\n-        this.zookeepers = zookeepers;\n+\n+    Iterator<QueryData> iter = queryConfig.getQueries();\n+    while (iter.hasNext()) {\n+      queryData = iter.next();\n+      ranges.addAll(queryData.getRanges());\n     }\n-    \n-    @Override\n-    public void setAccumuloConnectionFactory(AccumuloConnectionFactory factory) {\n-        this.connectionFactory = factory;\n+\n+    if (ranges.isEmpty()) {\n+      throw new NoResultsException(new QueryException(\"No scan ranges produced for query.\"));\n     }\n-    \n-    @Override\n-    public void setUserSid(String sid) {\n-        this.sid = sid;\n+\n+    BulkInputFormat.setWorkingDirectory(job.getConfiguration(), jobDir.toString());\n+\n+    // Copy the information from the GenericQueryConfiguration to the job.\n+    BulkInputFormat.setRanges(job, ranges);\n+\n+    for (IteratorSetting cfg : queryData.getSettings()) {\n+      BulkInputFormat.addIterator(job.getConfiguration(), cfg);\n     }\n-    \n-    @Override\n-    public void setPrincipal(Principal principal) {\n-        this.principal = principal;\n+\n+    BulkInputFormat.setZooKeeperInstance(job.getConfiguration(), this.instanceName, this.zookeepers);\n+    Iterator<Authorizations> authsIter =\n+        (runtimeQueryAuthorizations == null || runtimeQueryAuthorizations.isEmpty()) ? null : runtimeQueryAuthorizations\n+            .iterator();\n+    Authorizations auths = (authsIter == null) ? null : authsIter.next();\n+    BulkInputFormat.setInputInfo(job, this.user, this.password.getBytes(), logic.getTableName(), auths);\n+    for (int priority = 10; authsIter != null && authsIter.hasNext(); ++priority) {\n+      IteratorSetting cfg = new IteratorSetting(priority, ConfigurableVisibilityFilter.class);\n+      cfg.setName(\"visibilityFilter\" + priority);\n+      cfg.addOption(ConfigurableVisibilityFilter.AUTHORIZATIONS_OPT, authsIter.next().toString());\n+      BulkInputFormat.addIterator(job.getConfiguration(), cfg);\n     }\n-    \n-    @Override\n-    public void setPersister(Persister persister) {\n-        this.persister = persister;\n+\n+    job.getConfiguration().set(BulkResultsFileOutputMapper.QUERY_LOGIC_SETTINGS, base64EncodedQuery);\n+    job.getConfiguration().set(BulkResultsFileOutputMapper.QUERY_IMPL_CLASS, queryImplClass.getName());\n+    job.getConfiguration().set(BulkResultsFileOutputMapper.QUERY_LOGIC_NAME, logic.getLogicName());\n+\n+    job.getConfiguration().set(\n+        BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH,\n+        \"classpath*:datawave/configuration/spring/CDIBeanPostProcessor.xml,\" +\n+            \"classpath*:datawave/query/*QueryLogicFactory.xml,\"\n+            + \"classpath*:/MarkingFunctionsContext.xml,\" + \"classpath*:/CacheContext.xml\");\n+    job.getConfiguration().set(BulkResultsFileOutputMapper.SPRING_CONFIG_LOCATIONS,\n+        job.getConfiguration().get(BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH));\n+    // Tell the Mapper/Reducer to use a specific set of application context files when doing Spring-CDI integration.\n+    String cdiOpts =\n+        \"'-Dcdi.spring.configs=\" + job.getConfiguration().get(BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH) +\n+            \"'\";\n+    // Pass our server DN along to the child VM so it can be made available for injection.\n+    cdiOpts += \" '-Dserver.principal=\" + encodeServerPrincipal(serverPrincipal) + \"'\";\n+    String javaOpts = job.getConfiguration().get(\"mapreduce.map.java.opts\");\n+    javaOpts = (javaOpts == null) ? cdiOpts : (javaOpts + \" \" + cdiOpts);\n+    job.getConfiguration().set(\"mapreduce.map.java.opts\", javaOpts);\n+    job.setMapOutputKeyClass(Key.class);\n+    job.setMapOutputValueClass(Value.class);\n+\n+    job.setWorkingDirectory(jobDir);\n+  }\n+\n+  private String encodeServerPrincipal(DatawavePrincipal serverPrincipal) throws IOException {\n+    return Base64.encodeObject(serverPrincipal, Base64.GZIP | Base64.DONT_BREAK_LINES);\n+  }\n+\n+  private QuerySettings setupQuery(String sid, String queryId, Principal principal) throws Exception {\n+\n+    AccumuloClient client = null;\n+    QueryLogic<?> logic = null;\n+    try {\n+      // Get the query by the query id\n+      Query q = getQueryById(queryId);\n+      if (!sid.equals(q.getOwner())) {\n+        throw new QueryException(\"This query does not belong to you. expected: \" + q.getOwner() + \", value: \" + sid,\n+            Response.Status.UNAUTHORIZED.getStatusCode());\n+      }\n+\n+      // will throw IllegalArgumentException if not defined\n+      logic = queryFactory.getQueryLogic(q.getQueryLogicName(), principal);\n+\n+      // Get an accumulo connection\n+      Map<String, String> trackingMap = connectionFactory.getTrackingMap(Thread.currentThread().getStackTrace());\n+      client = connectionFactory.getClient(logic.getConnectionPriority(), trackingMap);\n+\n+      // Merge user auths with the auths that they use in the Query\n+      Set<Authorizations> runtimeQueryAuthorizations =\n+          AuthorizationsUtil.getDowngradedAuthorizations(q.getQueryAuthorizations(), principal);\n+\n+      // Initialize the logic so that the configuration contains all of the iterator options\n+      GenericQueryConfiguration queryConfig = logic.initialize(client, q, runtimeQueryAuthorizations);\n+\n+      String base64EncodedQuery = BulkResultsFileOutputMapper.serializeQuery(q);\n+\n+      return new QuerySettings(logic, queryConfig, base64EncodedQuery, q.getClass(), runtimeQueryAuthorizations);\n+    } finally {\n+      if (null != logic && null != client) {\n+        connectionFactory.returnClient(client);\n+      }\n     }\n-    \n-    @Override\n-    public void setQueryCache(QueryCache cache) {\n-        this.runningQueryCache = cache;\n+\n+  }\n+\n+  private Query getQueryById(String id) throws QueryException {\n+\n+    RunningQuery runningQuery = runningQueryCache.get(id);\n+    if (null != runningQuery) {\n+      return runningQuery.getSettings();\n+    } else {\n+      List<Query> queries = persister.findById(id);\n+      if (null == queries || queries.isEmpty()) {\n+        throw new QueryException(\"No query object matches this id\", Response.Status.NOT_FOUND.getStatusCode());\n+      }\n+      if (queries.size() > 1) {\n+        throw new QueryException(\"More than one query object matches the id\",\n+            Response.Status.NOT_FOUND.getStatusCode());\n+      }\n+      return queries.get(0);\n     }\n+  }\n+\n+  @Override\n+  public void setQueryLogicFactory(QueryLogicFactory factory) {\n+    this.queryFactory = factory;\n+  }\n+\n+  @Override\n+  public void setUsername(String username) {\n+    this.user = username;\n+  }\n+\n+  @Override\n+  public void setPassword(String password) {\n+    this.password = password;\n+  }\n+\n+  @Override\n+  public void setInstanceName(String instanceName) {\n+    this.instanceName = instanceName;\n+  }\n+\n+  @Override\n+  public void setZookeepers(String zookeepers) {\n+    this.zookeepers = zookeepers;\n+  }\n+\n+  @Override\n+  public void setAccumuloConnectionFactory(AccumuloConnectionFactory factory) {\n+    this.connectionFactory = factory;\n+  }\n+\n+  @Override\n+  public void setUserSid(String sid) {\n+    this.sid = sid;\n+  }\n+\n+  @Override\n+  public void setPrincipal(Principal principal) {\n+    this.principal = principal;\n+  }\n+\n+  @Override\n+  public void setPersister(Persister persister) {\n+    this.persister = persister;\n+  }\n+\n+  @Override\n+  public void setQueryCache(QueryCache cache) {\n+    this.runningQueryCache = cache;\n+  }\n }\n",
            "diff_size": 594
        },
        {
            "tool": "naturalize",
            "errors": [
                {
                    "line": "32",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/83/BulkResultsJobConfiguration.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/naturalize/83/BulkResultsJobConfiguration.java\nindex aa397fa92c7..695dead5dfc 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/83/BulkResultsJobConfiguration.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/naturalize/83/BulkResultsJobConfiguration.java\n@@ -192,8 +192,7 @@ public class BulkResultsJobConfiguration extends MapReduceJobConfiguration imple\n                         .batchWriterConfig(new BatchWriterConfig()\n                                 .setMaxLatency(30, TimeUnit.SECONDS)\n                                 .setMaxMemory(10485760)\n-                                .setMaxWriteThreads(2))\n-                        .build();\n+                                .setMaxWriteThreads(2)).build();\n \n                 AccumuloOutputFormat.configure()\n                         .clientProperties(clientProps)\n@@ -285,11 +284,9 @@ public class BulkResultsJobConfiguration extends MapReduceJobConfiguration imple\n         job.getConfiguration().set(BulkResultsFileOutputMapper.QUERY_IMPL_CLASS, queryImplClass.getName());\n         job.getConfiguration().set(BulkResultsFileOutputMapper.QUERY_LOGIC_NAME, logic.getLogicName());\n         \n-        job.getConfiguration().set(\n-                        BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH,\n-                        \"classpath*:datawave/configuration/spring/CDIBeanPostProcessor.xml,\" + \"classpath*:datawave/query/*QueryLogicFactory.xml,\"\n+        job.getConfiguration().set(BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH, \"classpath*:datawave/configuration/spring/CDIBeanPostProcessor.xml,\" + \"classpath*:datawave/query/*QueryLogicFactory.xml,\"\n                                         + \"classpath*:/MarkingFunctionsContext.xml,\" + \"classpath*:/CacheContext.xml\");\n-        job.getConfiguration().set(BulkResultsFileOutputMapper.SPRING_CONFIG_LOCATIONS,\n+job.getConfiguration().set(BulkResultsFileOutputMapper.SPRING_CONFIG_LOCATIONS,\n                         job.getConfiguration().get(BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH));\n         // Tell the Mapper/Reducer to use a specific set of application context files when doing Spring-CDI integration.\n         String cdiOpts = \"'-Dcdi.spring.configs=\" + job.getConfiguration().get(BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH) + \"'\";\n@@ -406,4 +403,4 @@ public class BulkResultsJobConfiguration extends MapReduceJobConfiguration imple\n     public void setQueryCache(QueryCache cache) {\n         this.runningQueryCache = cache;\n     }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 7
        },
        {
            "tool": "codebuff",
            "errors": [
                {
                    "line": "32",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/83/BulkResultsJobConfiguration.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/codebuff/83/BulkResultsJobConfiguration.java\nindex aa397fa92c7..e7b6297657a 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/83/BulkResultsJobConfiguration.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/codebuff/83/BulkResultsJobConfiguration.java\n@@ -40,7 +40,6 @@ import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;\n import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n import org.apache.log4j.Logger;\n import org.infinispan.commons.util.Base64;\n-\n import javax.ws.rs.WebApplicationException;\n import javax.ws.rs.core.Response;\n import java.io.IOException;\n@@ -54,8 +53,8 @@ import java.util.Properties;\n import java.util.Set;\n import java.util.concurrent.TimeUnit;\n \n-public class BulkResultsJobConfiguration extends MapReduceJobConfiguration implements NeedCallerDetails, NeedAccumuloConnectionFactory, NeedAccumuloDetails,\n-                NeedQueryLogicFactory, NeedQueryPersister, NeedQueryCache {\n+\n+public class BulkResultsJobConfiguration extends MapReduceJobConfiguration implements NeedCallerDetails, NeedAccumuloConnectionFactory, NeedAccumuloDetails, NeedQueryLogicFactory, NeedQueryPersister, NeedQueryCache {\n     \n     /**\n      * Container for query settings\n@@ -67,9 +66,8 @@ public class BulkResultsJobConfiguration extends MapReduceJobConfiguration imple\n         private String base64EncodedQuery = null;\n         private Set<Authorizations> runtimeQueryAuthorizations = null;\n         private Class<? extends Query> queryImplClass;\n-        \n-        public QuerySettings(QueryLogic<?> logic, GenericQueryConfiguration queryConfig, String base64EncodedQuery, Class<? extends Query> queryImplClass,\n-                        Set<Authorizations> runtimeQueryAuthorizations) {\n+\n+        public QuerySettings(QueryLogic<?> logic, GenericQueryConfiguration queryConfig, String base64EncodedQuery, Class<? extends Query> queryImplClass, Set<Authorizations> runtimeQueryAuthorizations) {\n             super();\n             this.logic = logic;\n             this.queryConfig = queryConfig;\n@@ -77,30 +75,29 @@ public class BulkResultsJobConfiguration extends MapReduceJobConfiguration imple\n             this.queryImplClass = queryImplClass;\n             this.runtimeQueryAuthorizations = runtimeQueryAuthorizations;\n         }\n-        \n+\n         public QueryLogic<?> getLogic() {\n             return logic;\n         }\n-        \n+\n         public GenericQueryConfiguration getQueryConfig() {\n             return queryConfig;\n         }\n-        \n+\n         public String getBase64EncodedQuery() {\n             return base64EncodedQuery;\n         }\n-        \n+\n         public Class<? extends Query> getQueryImplClass() {\n             return queryImplClass;\n         }\n-        \n+\n         public Set<Authorizations> getRuntimeQueryAuthorizations() {\n             return runtimeQueryAuthorizations;\n         }\n     }\n-    \n+\n     private Logger log = Logger.getLogger(this.getClass());\n-    \n     private AccumuloConnectionFactory connectionFactory;\n     private QueryLogicFactory queryFactory;\n     private Persister persister;\n@@ -111,30 +108,29 @@ public class BulkResultsJobConfiguration extends MapReduceJobConfiguration imple\n     private String zookeepers;\n     private String sid;\n     private Principal principal;\n-    \n     private String tableName = null;\n     private Class<? extends OutputFormat> outputFormatClass = SequenceFileOutputFormat.class;\n-    \n+\n     @Override\n-    public void _initializeConfiguration(Job job, Path jobDir, String jobId, Map<String,String> runtimeParameters, DatawavePrincipal serverPrincipal)\n-                    throws IOException, QueryException {\n-        \n+    public void _initializeConfiguration(Job job, Path jobDir, String jobId, Map<String,String> runtimeParameters, DatawavePrincipal serverPrincipal) throws IOException, QueryException {\n         String queryId = runtimeParameters.get(\"queryId\");\n         SerializationFormat format = SerializationFormat.valueOf(runtimeParameters.get(\"format\"));\n         String outputFormatParameter = runtimeParameters.get(\"outputFormat\");\n         if (outputFormatParameter != null && outputFormatParameter.equalsIgnoreCase(\"TEXT\")) {\n             this.outputFormatClass = TextOutputFormat.class;\n         }\n+\n+\n         if (runtimeParameters.containsKey(\"outputTableName\"))\n             this.tableName = runtimeParameters.get(\"outputTableName\");\n         \n         // Initialize the Query\n+\n         QueryLogic<?> logic;\n         GenericQueryConfiguration queryConfig;\n         String base64EncodedQuery;\n         Class<? extends Query> queryImplClass;\n         Set<Authorizations> runtimeQueryAuthorizations;\n-        \n         try {\n             QuerySettings settings = setupQuery(sid, queryId, principal);\n             logic = settings.getLogic();\n@@ -151,10 +147,9 @@ public class BulkResultsJobConfiguration extends MapReduceJobConfiguration imple\n         }\n         \n         // Setup and run the MapReduce job\n+\n         try {\n-            \n             setupJob(job, jobDir, queryConfig, logic, base64EncodedQuery, queryImplClass, runtimeQueryAuthorizations, serverPrincipal);\n-            \n             if (null == this.tableName) {\n                 // Setup job for output to HDFS\n                 // set the mapper\n@@ -186,20 +181,9 @@ public class BulkResultsJobConfiguration extends MapReduceJobConfiguration imple\n                 job.setOutputFormatClass(AccumuloOutputFormat.class);\n                 \n                 // @formatter:off\n-                Properties clientProps = Accumulo.newClientProperties()\n-                        .to(instanceName, zookeepers)\n-                        .as(user, password)\n-                        .batchWriterConfig(new BatchWriterConfig()\n-                                .setMaxLatency(30, TimeUnit.SECONDS)\n-                                .setMaxMemory(10485760)\n-                                .setMaxWriteThreads(2))\n-                        .build();\n-\n-                AccumuloOutputFormat.configure()\n-                        .clientProperties(clientProps)\n-                        .createTables(true)\n-                        .defaultTable(tableName)\n-                        .store(job);\n+\n+                Properties clientProps = Accumulo.newClientProperties().to(instanceName, zookeepers).as(user, password).batchWriterConfig(new BatchWriterConfig().setMaxLatency(30, TimeUnit.SECONDS).setMaxMemory(10485760).setMaxWriteThreads(2)).build();\n+                AccumuloOutputFormat.configure().clientProperties(clientProps).createTables(true).defaultTable(tableName).store(job);\n                 // @formatter:on\n                 \n                 // AccumuloOutputFormat.loglevel\n@@ -212,7 +196,6 @@ public class BulkResultsJobConfiguration extends MapReduceJobConfiguration imple\n             log.error(\"Error starting job\", e);\n             throw new QueryException(DatawaveErrorCode.JOB_STARTING_ERROR, e);\n         }\n-        \n     }\n     \n     /**\n@@ -237,79 +220,77 @@ public class BulkResultsJobConfiguration extends MapReduceJobConfiguration imple\n      * @throws IOException\n      * @throws AccumuloSecurityException\n      */\n-    private void setupJob(Job job, Path jobDir, GenericQueryConfiguration queryConfig, QueryLogic<?> logic, String base64EncodedQuery,\n-                    Class<? extends Query> queryImplClass, Set<Authorizations> runtimeQueryAuthorizations, DatawavePrincipal serverPrincipal)\n-                    throws IOException, AccumuloSecurityException {\n-        \n+\n+    private void setupJob(Job job,\n+        Path jobDir,\n+            GenericQueryConfiguration queryConfig,\n+                QueryLogic<?> logic,\n+                    String base64EncodedQuery,\n+                        Class<? extends Query> queryImplClass,\n+                            Set<Authorizations> runtimeQueryAuthorizations, DatawavePrincipal serverPrincipal) throws IOException, AccumuloSecurityException {\n         job.setInputFormatClass(BulkInputFormat.class);\n-        \n+\n         QueryData queryData = null;\n         Collection<Range> ranges = new ArrayList<>();\n-        \n         if (!queryConfig.canRunQuery()) {\n             throw new UnsupportedOperationException(\"Unable to run query\");\n         }\n-        \n+\n         Iterator<QueryData> iter = queryConfig.getQueries();\n         while (iter.hasNext()) {\n             queryData = iter.next();\n             ranges.addAll(queryData.getRanges());\n         }\n-        \n+\n+\n         if (ranges.isEmpty()) {\n             throw new NoResultsException(new QueryException(\"No scan ranges produced for query.\"));\n         }\n-        \n         BulkInputFormat.setWorkingDirectory(job.getConfiguration(), jobDir.toString());\n         \n         // Copy the information from the GenericQueryConfiguration to the job.\n         BulkInputFormat.setRanges(job, ranges);\n-        \n+\n         for (IteratorSetting cfg : queryData.getSettings()) {\n             BulkInputFormat.addIterator(job.getConfiguration(), cfg);\n         }\n-        \n         BulkInputFormat.setZooKeeperInstance(job.getConfiguration(), this.instanceName, this.zookeepers);\n-        Iterator<Authorizations> authsIter = (runtimeQueryAuthorizations == null || runtimeQueryAuthorizations.isEmpty()) ? null : runtimeQueryAuthorizations\n-                        .iterator();\n+\n+        Iterator<Authorizations> authsIter = (runtimeQueryAuthorizations == null || runtimeQueryAuthorizations.isEmpty()) ? null : runtimeQueryAuthorizations.iterator();\n         Authorizations auths = (authsIter == null) ? null : authsIter.next();\n         BulkInputFormat.setInputInfo(job, this.user, this.password.getBytes(), logic.getTableName(), auths);\n+\n         for (int priority = 10; authsIter != null && authsIter.hasNext(); ++priority) {\n             IteratorSetting cfg = new IteratorSetting(priority, ConfigurableVisibilityFilter.class);\n             cfg.setName(\"visibilityFilter\" + priority);\n             cfg.addOption(ConfigurableVisibilityFilter.AUTHORIZATIONS_OPT, authsIter.next().toString());\n             BulkInputFormat.addIterator(job.getConfiguration(), cfg);\n         }\n-        \n         job.getConfiguration().set(BulkResultsFileOutputMapper.QUERY_LOGIC_SETTINGS, base64EncodedQuery);\n         job.getConfiguration().set(BulkResultsFileOutputMapper.QUERY_IMPL_CLASS, queryImplClass.getName());\n         job.getConfiguration().set(BulkResultsFileOutputMapper.QUERY_LOGIC_NAME, logic.getLogicName());\n-        \n-        job.getConfiguration().set(\n-                        BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH,\n-                        \"classpath*:datawave/configuration/spring/CDIBeanPostProcessor.xml,\" + \"classpath*:datawave/query/*QueryLogicFactory.xml,\"\n-                                        + \"classpath*:/MarkingFunctionsContext.xml,\" + \"classpath*:/CacheContext.xml\");\n+        job.getConfiguration().set(BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH, \"classpath*:datawave/configuration/spring/CDIBeanPostProcessor.xml,\" + \"classpath*:datawave/query/*QueryLogicFactory.xml,\" + \"classpath*:/MarkingFunctionsContext.xml,\" + \"classpath*:/CacheContext.xml\");\n         job.getConfiguration().set(BulkResultsFileOutputMapper.SPRING_CONFIG_LOCATIONS,\n-                        job.getConfiguration().get(BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH));\n+            job.getConfiguration().get(BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH));\n         // Tell the Mapper/Reducer to use a specific set of application context files when doing Spring-CDI integration.\n+\n         String cdiOpts = \"'-Dcdi.spring.configs=\" + job.getConfiguration().get(BulkResultsFileOutputMapper.APPLICATION_CONTEXT_PATH) + \"'\";\n         // Pass our server DN along to the child VM so it can be made available for injection.\n         cdiOpts += \" '-Dserver.principal=\" + encodeServerPrincipal(serverPrincipal) + \"'\";\n+\n         String javaOpts = job.getConfiguration().get(\"mapreduce.map.java.opts\");\n         javaOpts = (javaOpts == null) ? cdiOpts : (javaOpts + \" \" + cdiOpts);\n         job.getConfiguration().set(\"mapreduce.map.java.opts\", javaOpts);\n         job.setMapOutputKeyClass(Key.class);\n         job.setMapOutputValueClass(Value.class);\n-        \n         job.setWorkingDirectory(jobDir);\n     }\n-    \n+\n     private String encodeServerPrincipal(DatawavePrincipal serverPrincipal) throws IOException {\n         return Base64.encodeObject(serverPrincipal, Base64.GZIP | Base64.DONT_BREAK_LINES);\n     }\n-    \n+\n     private QuerySettings setupQuery(String sid, String queryId, Principal principal) throws Exception {\n-        \n         AccumuloClient client = null;\n         QueryLogic<?> logic = null;\n         try {\n@@ -317,33 +298,31 @@ public class BulkResultsJobConfiguration extends MapReduceJobConfiguration imple\n             Query q = getQueryById(queryId);\n             if (!sid.equals(q.getOwner()))\n                 throw new QueryException(\"This query does not belong to you. expected: \" + q.getOwner() + \", value: \" + sid,\n-                                Response.Status.UNAUTHORIZED.getStatusCode());\n+                    Response.Status.UNAUTHORIZED.getStatusCode());\n             \n             // will throw IllegalArgumentException if not defined\n             logic = queryFactory.getQueryLogic(q.getQueryLogicName(), principal);\n             \n             // Get an accumulo connection\n+\n             Map<String,String> trackingMap = connectionFactory.getTrackingMap(Thread.currentThread().getStackTrace());\n             client = connectionFactory.getClient(logic.getConnectionPriority(), trackingMap);\n             \n             // Merge user auths with the auths that they use in the Query\n+\n             Set<Authorizations> runtimeQueryAuthorizations = AuthorizationsUtil.getDowngradedAuthorizations(q.getQueryAuthorizations(), principal);\n             \n             // Initialize the logic so that the configuration contains all of the iterator options\n             GenericQueryConfiguration queryConfig = logic.initialize(client, q, runtimeQueryAuthorizations);\n-            \n             String base64EncodedQuery = BulkResultsFileOutputMapper.serializeQuery(q);\n-            \n             return new QuerySettings(logic, queryConfig, base64EncodedQuery, q.getClass(), runtimeQueryAuthorizations);\n         } finally {\n             if (null != logic && null != client)\n                 connectionFactory.returnClient(client);\n         }\n-        \n     }\n-    \n+\n     private Query getQueryById(String id) throws QueryException {\n-        \n         RunningQuery runningQuery = runningQueryCache.get(id);\n         if (null != runningQuery) {\n             return runningQuery.getSettings();\n@@ -356,54 +335,54 @@ public class BulkResultsJobConfiguration extends MapReduceJobConfiguration imple\n             return queries.get(0);\n         }\n     }\n-    \n+\n     @Override\n     public void setQueryLogicFactory(QueryLogicFactory factory) {\n         this.queryFactory = factory;\n     }\n-    \n+\n     @Override\n     public void setUsername(String username) {\n         this.user = username;\n     }\n-    \n+\n     @Override\n     public void setPassword(String password) {\n         this.password = password;\n     }\n-    \n+\n     @Override\n     public void setInstanceName(String instanceName) {\n         this.instanceName = instanceName;\n     }\n-    \n+\n     @Override\n     public void setZookeepers(String zookeepers) {\n         this.zookeepers = zookeepers;\n     }\n-    \n+\n     @Override\n     public void setAccumuloConnectionFactory(AccumuloConnectionFactory factory) {\n         this.connectionFactory = factory;\n     }\n-    \n+\n     @Override\n     public void setUserSid(String sid) {\n         this.sid = sid;\n     }\n-    \n+\n     @Override\n     public void setPrincipal(Principal principal) {\n         this.principal = principal;\n     }\n-    \n+\n     @Override\n     public void setPersister(Persister persister) {\n         this.persister = persister;\n     }\n-    \n+\n     @Override\n     public void setQueryCache(QueryCache cache) {\n         this.runningQueryCache = cache;\n     }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 91
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "32",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "32",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        }
    ],
    "repaired_by": [],
    "not_repaired_by": [
        "styler",
        "intellij",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ]
}