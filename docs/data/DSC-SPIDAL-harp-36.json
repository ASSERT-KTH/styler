{
    "project_name": "DSC-SPIDAL-harp",
    "error_id": "36",
    "information": {
        "errors": [
            {
                "line": "3",
                "severity": "error",
                "message": "Line has trailing spaces.",
                "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
            }
        ]
    },
    "source_code": "/*\n * Copyright 2013-2017 Indiana University\n * \n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "3",
                    "severity": "error",
                    "message": "Line has trailing spaces.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "intellij",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/DSC-SPIDAL-harp/errored/1/36/LDAUtil.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/DSC-SPIDAL-harp/intellij/36/LDAUtil.java\nindex 0400a0246bf..bbd005dbcd3 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/DSC-SPIDAL-harp/errored/1/36/LDAUtil.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/DSC-SPIDAL-harp/intellij/36/LDAUtil.java\n@@ -1,6 +1,6 @@\n /*\n  * Copyright 2013-2017 Indiana University\n- * \n+ *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n  * You may obtain a copy of the License at\n@@ -37,183 +37,183 @@ import java.util.LinkedList;\n import java.util.List;\n \n public class LDAUtil {\n-  protected static final Log LOG =\n-    LogFactory.getLog(LDAUtil.class);\n-\n-  static int load(LinkedList<String> vFilePaths,\n-    int numThreads, Configuration configuration,\n-    Int2ObjectOpenHashMap<DocWord> vDocMap,\n-    Int2ObjectOpenHashMap<String> docIDMap) {\n-    DocStore vStore = new DocStore(vFilePaths,\n-      numThreads, configuration);\n-    vStore.load(vDocMap, docIDMap);\n-    return vStore.getMaxDocID();\n-  }\n+    protected static final Log LOG =\n+        LogFactory.getLog(LDAUtil.class);\n \n-  static void createDWSplitAndModel(\n-    Int2ObjectOpenHashMap<DocWord> vDocMap,\n-    LongArrayList[] dMap,\n-    Table<TopicCountMap> wordTable,\n-    Int2ObjectOpenHashMap<DocWord>[] vDWMap,\n-    int numSplits, int numTopics,\n-    int numThreads) {\n-    // Create a local V Map indexed by W columns\n-    // Create D model and local W model\n-    DWSplit[] dwSplits = new DWSplit[numSplits];\n-    for (int i = 0; i < numSplits; i++) {\n-      dwSplits[i] = new DWSplit(i);\n-      vDWMap[i] = new Int2ObjectOpenHashMap<>();\n+    static int load(LinkedList<String> vFilePaths,\n+                    int numThreads, Configuration configuration,\n+                    Int2ObjectOpenHashMap<DocWord> vDocMap,\n+                    Int2ObjectOpenHashMap<String> docIDMap) {\n+        DocStore vStore = new DocStore(vFilePaths,\n+            numThreads, configuration);\n+        vStore.load(vDocMap, docIDMap);\n+        return vStore.getMaxDocID();\n     }\n-    IntArray idArray =\n-      IntArray.create(vDocMap.size(), false);\n-    int[] ids = idArray.get();\n-    vDocMap.keySet().toArray(ids);\n-    IntArrays.quickSort(ids, 0, idArray.size());\n-    int rowIndex = 0;\n-    // Num of topics and then a topic and a count\n-    for (int i = 0; i < idArray.size(); i++) {\n-      int docID = ids[i];\n-      DocWord docWord = vDocMap.get(docID);\n-      // Create doc model\n-      if (dMap[docID] == null) {\n-        dMap[docID] = new LongArrayList();\n-      }\n-      // Create word model\n-      for (int j = 0; j < docWord.numV; j++) {\n-        Partition<TopicCountMap> partition =\n-          wordTable.getPartition(docWord.id2[j]);\n-        if (partition == null) {\n-          TopicCountMap topicCount =\n-            Writable.create(TopicCountMap.class);\n-          wordTable.addPartition(\n-            new Partition<TopicCountMap>(\n-              docWord.id2[j], topicCount));\n+\n+    static void createDWSplitAndModel(\n+        Int2ObjectOpenHashMap<DocWord> vDocMap,\n+        LongArrayList[] dMap,\n+        Table<TopicCountMap> wordTable,\n+        Int2ObjectOpenHashMap<DocWord>[] vDWMap,\n+        int numSplits, int numTopics,\n+        int numThreads) {\n+        // Create a local V Map indexed by W columns\n+        // Create D model and local W model\n+        DWSplit[] dwSplits = new DWSplit[numSplits];\n+        for (int i = 0; i < numSplits; i++) {\n+            dwSplits[i] = new DWSplit(i);\n+            vDWMap[i] = new Int2ObjectOpenHashMap<>();\n         }\n-      }\n-      int splitID = i % numSplits;\n-      dwSplits[splitID].docWordList.add(docWord);\n-      rowIndex++;\n-      if (rowIndex % 1000000 == 0) {\n-        LOG.info(\"Processed docs \" + rowIndex);\n-      }\n-    }\n-    idArray.release();\n-    vDocMap.clear();\n-    LOG.info(\"D & local W model are created.\");\n-    List<DataInitTask> tasks = new LinkedList<>();\n-    for (int i = 0; i < numThreads; i++) {\n-      tasks.add(new DataInitTask(vDWMap, dMap,\n-        wordTable, numTopics));\n+        IntArray idArray =\n+            IntArray.create(vDocMap.size(), false);\n+        int[] ids = idArray.get();\n+        vDocMap.keySet().toArray(ids);\n+        IntArrays.quickSort(ids, 0, idArray.size());\n+        int rowIndex = 0;\n+        // Num of topics and then a topic and a count\n+        for (int i = 0; i < idArray.size(); i++) {\n+            int docID = ids[i];\n+            DocWord docWord = vDocMap.get(docID);\n+            // Create doc model\n+            if (dMap[docID] == null) {\n+                dMap[docID] = new LongArrayList();\n+            }\n+            // Create word model\n+            for (int j = 0; j < docWord.numV; j++) {\n+                Partition<TopicCountMap> partition =\n+                    wordTable.getPartition(docWord.id2[j]);\n+                if (partition == null) {\n+                    TopicCountMap topicCount =\n+                        Writable.create(TopicCountMap.class);\n+                    wordTable.addPartition(\n+                        new Partition<TopicCountMap>(\n+                            docWord.id2[j], topicCount));\n+                }\n+            }\n+            int splitID = i % numSplits;\n+            dwSplits[splitID].docWordList.add(docWord);\n+            rowIndex++;\n+            if (rowIndex % 1000000 == 0) {\n+                LOG.info(\"Processed docs \" + rowIndex);\n+            }\n+        }\n+        idArray.release();\n+        vDocMap.clear();\n+        LOG.info(\"D & local W model are created.\");\n+        List<DataInitTask> tasks = new LinkedList<>();\n+        for (int i = 0; i < numThreads; i++) {\n+            tasks.add(new DataInitTask(vDWMap, dMap,\n+                wordTable, numTopics));\n+        }\n+        DynamicScheduler<DWSplit, Object, DataInitTask> compute =\n+            new DynamicScheduler<>(tasks);\n+        compute.submitAll(dwSplits);\n+        dwSplits = null;\n+        compute.start();\n+        compute.stop();\n+        LOG.info(\"D & local W model are initialized\");\n     }\n-    DynamicScheduler<DWSplit, Object, DataInitTask> compute =\n-      new DynamicScheduler<>(tasks);\n-    compute.submitAll(dwSplits);\n-    dwSplits = null;\n-    compute.start();\n-    compute.stop();\n-    LOG.info(\"D & local W model are initialized\");\n-  }\n \n-  static void addToData(\n-    Int2ObjectOpenHashMap<DocWord> map, int id1,\n-    int id2, int val) {\n-    DocWord docWord = map.get(id1);\n-    if (docWord == null) {\n-      docWord = new DocWord();\n-      docWord.id1 = id1;\n-      docWord.id2 = new int[Constants.ARR_LEN];\n-      docWord.v = new int[Constants.ARR_LEN];\n-      map.put(id1, docWord);\n-    }\n-    // Search in ids2 for the current id2\n-    int pos = IntArrays.binarySearch(docWord.id2,\n-      0, docWord.numV, id2);\n-    if (pos >= 0) {\n-      docWord.v[pos] += val;\n-    } else {\n-      if (docWord.id2.length == docWord.numV) {\n-        int[] ids2 =\n-          new int[docWord.id2.length << 1];\n-        int[] v =\n-          new int[docWord.id2.length << 1];\n-        System.arraycopy(docWord.id2, 0, ids2, 0,\n-          docWord.numV);\n-        System.arraycopy(docWord.v, 0, v, 0,\n-          docWord.numV);\n-        docWord.id2 = ids2;\n-        docWord.v = v;\n-      }\n-      int insertPos = -pos - 1;\n-      System.arraycopy(docWord.id2, insertPos,\n-        docWord.id2, insertPos + 1,\n-        docWord.numV - insertPos);\n-      System.arraycopy(docWord.v, insertPos,\n-        docWord.v, insertPos + 1,\n-        docWord.numV - insertPos);\n-      docWord.id2[insertPos] = id2;\n-      docWord.v[insertPos] = val;\n-      docWord.numV++;\n+    static void addToData(\n+        Int2ObjectOpenHashMap<DocWord> map, int id1,\n+        int id2, int val) {\n+        DocWord docWord = map.get(id1);\n+        if (docWord == null) {\n+            docWord = new DocWord();\n+            docWord.id1 = id1;\n+            docWord.id2 = new int[Constants.ARR_LEN];\n+            docWord.v = new int[Constants.ARR_LEN];\n+            map.put(id1, docWord);\n+        }\n+        // Search in ids2 for the current id2\n+        int pos = IntArrays.binarySearch(docWord.id2,\n+            0, docWord.numV, id2);\n+        if (pos >= 0) {\n+            docWord.v[pos] += val;\n+        } else {\n+            if (docWord.id2.length == docWord.numV) {\n+                int[] ids2 =\n+                    new int[docWord.id2.length << 1];\n+                int[] v =\n+                    new int[docWord.id2.length << 1];\n+                System.arraycopy(docWord.id2, 0, ids2, 0,\n+                    docWord.numV);\n+                System.arraycopy(docWord.v, 0, v, 0,\n+                    docWord.numV);\n+                docWord.id2 = ids2;\n+                docWord.v = v;\n+            }\n+            int insertPos = -pos - 1;\n+            System.arraycopy(docWord.id2, insertPos,\n+                docWord.id2, insertPos + 1,\n+                docWord.numV - insertPos);\n+            System.arraycopy(docWord.v, insertPos,\n+                docWord.v, insertPos + 1,\n+                docWord.numV - insertPos);\n+            docWord.id2[insertPos] = id2;\n+            docWord.v[insertPos] = val;\n+            docWord.numV++;\n+        }\n     }\n-  }\n \n-  static int createWordModel(\n-    Table<TopicCountList>[] wordTableMap,\n-    int numModelSlices,\n-    Table<TopicCountMap> wordTable,\n-    CollectiveMapper<?, ?, ?, ?> mapper) {\n-    // allreduce column index and the element\n-    // count on each column\n-    long t1 = System.currentTimeMillis();\n-    mapper.regroup(\"lda\", \"regroup-word-model\",\n-      wordTable,\n-      new Partitioner(mapper.getNumWorkers()));\n-    for (int i = 0; i < numModelSlices; i++) {\n-      wordTableMap[i] = new Table<>(i,\n-        new TopicCountListCombiner());\n-    }\n-    IntArray idArray = IntArray.create(\n-      wordTable.getNumPartitions(), false);\n-    int[] ids = idArray.get();\n-    wordTable.getPartitionIDs().toArray(ids);\n-    int size = idArray.size();\n-    IntArrays.quickSort(ids, 0, size);\n-    for (int i = 0; i < size; i++) {\n-      int sliceID = i % numModelSlices;\n-      TopicCountMap map =\n-        wordTable.getPartition(ids[i]).get();\n-      ObjectIterator<Int2IntMap.Entry> iterator =\n-        map.getTopicCount().int2IntEntrySet()\n-          .fastIterator();\n-      TopicCountList list =\n-        Writable.create(TopicCountList.class);\n-      LongArrayList array = list.getTopicCount();\n-      while (iterator.hasNext()) {\n-        Int2IntMap.Entry entry = iterator.next();\n-        long topicID = entry.getIntKey();\n-        long topicCount = entry.getIntValue();\n-        array.add((topicCount << 32) + topicID);\n-      }\n-      wordTableMap[sliceID].addPartition(\n-        new Partition<TopicCountList>(ids[i],\n-          list));\n+    static int createWordModel(\n+        Table<TopicCountList>[] wordTableMap,\n+        int numModelSlices,\n+        Table<TopicCountMap> wordTable,\n+        CollectiveMapper<?, ?, ?, ?> mapper) {\n+        // allreduce column index and the element\n+        // count on each column\n+        long t1 = System.currentTimeMillis();\n+        mapper.regroup(\"lda\", \"regroup-word-model\",\n+            wordTable,\n+            new Partitioner(mapper.getNumWorkers()));\n+        for (int i = 0; i < numModelSlices; i++) {\n+            wordTableMap[i] = new Table<>(i,\n+                new TopicCountListCombiner());\n+        }\n+        IntArray idArray = IntArray.create(\n+            wordTable.getNumPartitions(), false);\n+        int[] ids = idArray.get();\n+        wordTable.getPartitionIDs().toArray(ids);\n+        int size = idArray.size();\n+        IntArrays.quickSort(ids, 0, size);\n+        for (int i = 0; i < size; i++) {\n+            int sliceID = i % numModelSlices;\n+            TopicCountMap map =\n+                wordTable.getPartition(ids[i]).get();\n+            ObjectIterator<Int2IntMap.Entry> iterator =\n+                map.getTopicCount().int2IntEntrySet()\n+                    .fastIterator();\n+            TopicCountList list =\n+                Writable.create(TopicCountList.class);\n+            LongArrayList array = list.getTopicCount();\n+            while (iterator.hasNext()) {\n+                Int2IntMap.Entry entry = iterator.next();\n+                long topicID = entry.getIntKey();\n+                long topicCount = entry.getIntValue();\n+                array.add((topicCount << 32) + topicID);\n+            }\n+            wordTableMap[sliceID].addPartition(\n+                new Partition<TopicCountList>(ids[i],\n+                    list));\n+        }\n+        idArray.release();\n+        wordTable.release();\n+        wordTable = null;\n+        Table<IntArray> wordSumTable =\n+            new Table<>(0, new IntArrPlus());\n+        IntArray array = IntArray.create(1, false);\n+        array.get()[0] = size;\n+        wordSumTable\n+            .addPartition(new Partition<>(0, array));\n+        mapper.allreduce(\"lda\", \"allreduce-wordsum\",\n+            wordSumTable);\n+        int vocabularySize =\n+            wordSumTable.getPartition(0).get().get()[0];\n+        wordSumTable.release();\n+        long t2 = System.currentTimeMillis();\n+        LOG.info(\"W model is created, vocabulary: \"\n+            + vocabularySize + \", took: \" + (t2 - t1));\n+        return vocabularySize;\n     }\n-    idArray.release();\n-    wordTable.release();\n-    wordTable = null;\n-    Table<IntArray> wordSumTable =\n-      new Table<>(0, new IntArrPlus());\n-    IntArray array = IntArray.create(1, false);\n-    array.get()[0] = size;\n-    wordSumTable\n-      .addPartition(new Partition<>(0, array));\n-    mapper.allreduce(\"lda\", \"allreduce-wordsum\",\n-      wordSumTable);\n-    int vocabularySize =\n-      wordSumTable.getPartition(0).get().get()[0];\n-    wordSumTable.release();\n-    long t2 = System.currentTimeMillis();\n-    LOG.info(\"W model is created, vocabulary: \"\n-      + vocabularySize + \", took: \" + (t2 - t1));\n-    return vocabularySize;\n-  }\n }\n",
            "diff_size": 225
        },
        {
            "tool": "naturalize",
            "errors": [
                {
                    "line": "3",
                    "severity": "error",
                    "message": "Line has trailing spaces.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/DSC-SPIDAL-harp/errored/1/36/LDAUtil.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/DSC-SPIDAL-harp/naturalize/36/LDAUtil.java\nindex 0400a0246bf..5d723802356 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/DSC-SPIDAL-harp/errored/1/36/LDAUtil.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/DSC-SPIDAL-harp/naturalize/36/LDAUtil.java\n@@ -40,7 +40,7 @@ public class LDAUtil {\n   protected static final Log LOG =\n     LogFactory.getLog(LDAUtil.class);\n \n-  static int load(LinkedList<String> vFilePaths,\n+    static int load(LinkedList<String> vFilePaths,\n     int numThreads, Configuration configuration,\n     Int2ObjectOpenHashMap<DocWord> vDocMap,\n     Int2ObjectOpenHashMap<String> docIDMap) {\n@@ -216,4 +216,4 @@ public class LDAUtil {\n       + vocabularySize + \", took: \" + (t2 - t1));\n     return vocabularySize;\n   }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 2
        },
        {
            "tool": "codebuff",
            "errors": [
                {
                    "line": "3",
                    "severity": "error",
                    "message": "Line has trailing spaces.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/DSC-SPIDAL-harp/errored/1/36/LDAUtil.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/DSC-SPIDAL-harp/codebuff/36/LDAUtil.java\nindex 0400a0246bf..2d495555c3a 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/DSC-SPIDAL-harp/errored/1/36/LDAUtil.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/DSC-SPIDAL-harp/codebuff/36/LDAUtil.java\n@@ -32,31 +32,26 @@ import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.mapred.CollectiveMapper;\n-\n import java.util.LinkedList;\n import java.util.List;\n \n+\n public class LDAUtil {\n-  protected static final Log LOG =\n-    LogFactory.getLog(LDAUtil.class);\n \n-  static int load(LinkedList<String> vFilePaths,\n-    int numThreads, Configuration configuration,\n+  protected static final Log LOG = LogFactory.getLog(LDAUtil.class);\n+  static int load(\n+    LinkedList<String> vFilePaths,\n+    int numThreads,\n+    Configuration configuration,\n     Int2ObjectOpenHashMap<DocWord> vDocMap,\n     Int2ObjectOpenHashMap<String> docIDMap) {\n-    DocStore vStore = new DocStore(vFilePaths,\n-      numThreads, configuration);\n+    DocStore vStore = new DocStore(vFilePaths, numThreads, configuration);\n     vStore.load(vDocMap, docIDMap);\n     return vStore.getMaxDocID();\n   }\n \n-  static void createDWSplitAndModel(\n-    Int2ObjectOpenHashMap<DocWord> vDocMap,\n-    LongArrayList[] dMap,\n-    Table<TopicCountMap> wordTable,\n-    Int2ObjectOpenHashMap<DocWord>[] vDWMap,\n-    int numSplits, int numTopics,\n-    int numThreads) {\n+\n+  static void createDWSplitAndModel(Int2ObjectOpenHashMap<DocWord> vDocMap, LongArrayList[] dMap, Table<TopicCountMap> wordTable, Int2ObjectOpenHashMap<DocWord>[] vDWMap, int numSplits, int numTopics, int numThreads) {\n     // Create a local V Map indexed by W columns\n     // Create D model and local W model\n     DWSplit[] dwSplits = new DWSplit[numSplits];\n@@ -64,8 +59,7 @@ public class LDAUtil {\n       dwSplits[i] = new DWSplit(i);\n       vDWMap[i] = new Int2ObjectOpenHashMap<>();\n     }\n-    IntArray idArray =\n-      IntArray.create(vDocMap.size(), false);\n+    IntArray idArray = IntArray.create(vDocMap.size(), false);\n     int[] ids = idArray.get();\n     vDocMap.keySet().toArray(ids);\n     IntArrays.quickSort(ids, 0, idArray.size());\n@@ -79,15 +73,12 @@ public class LDAUtil {\n         dMap[docID] = new LongArrayList();\n       }\n       // Create word model\n+\n       for (int j = 0; j < docWord.numV; j++) {\n-        Partition<TopicCountMap> partition =\n-          wordTable.getPartition(docWord.id2[j]);\n+        Partition<TopicCountMap> partition = wordTable.getPartition(docWord.id2[j]);\n         if (partition == null) {\n-          TopicCountMap topicCount =\n-            Writable.create(TopicCountMap.class);\n-          wordTable.addPartition(\n-            new Partition<TopicCountMap>(\n-              docWord.id2[j], topicCount));\n+          TopicCountMap topicCount = Writable.create(TopicCountMap.class);\n+          wordTable.addPartition(new Partition<TopicCountMap>(docWord.id2[j], topicCount));\n         }\n       }\n       int splitID = i % numSplits;\n@@ -102,11 +93,9 @@ public class LDAUtil {\n     LOG.info(\"D & local W model are created.\");\n     List<DataInitTask> tasks = new LinkedList<>();\n     for (int i = 0; i < numThreads; i++) {\n-      tasks.add(new DataInitTask(vDWMap, dMap,\n-        wordTable, numTopics));\n+      tasks.add(new DataInitTask(vDWMap, dMap, wordTable, numTopics));\n     }\n-    DynamicScheduler<DWSplit, Object, DataInitTask> compute =\n-      new DynamicScheduler<>(tasks);\n+    DynamicScheduler<DWSplit, Object, DataInitTask> compute = new DynamicScheduler<>(tasks);\n     compute.submitAll(dwSplits);\n     dwSplits = null;\n     compute.start();\n@@ -114,9 +103,8 @@ public class LDAUtil {\n     LOG.info(\"D & local W model are initialized\");\n   }\n \n-  static void addToData(\n-    Int2ObjectOpenHashMap<DocWord> map, int id1,\n-    int id2, int val) {\n+\n+  static void addToData(Int2ObjectOpenHashMap<DocWord> map, int id1, int id2, int val) {\n     DocWord docWord = map.get(id1);\n     if (docWord == null) {\n       docWord = new DocWord();\n@@ -126,36 +114,28 @@ public class LDAUtil {\n       map.put(id1, docWord);\n     }\n     // Search in ids2 for the current id2\n-    int pos = IntArrays.binarySearch(docWord.id2,\n-      0, docWord.numV, id2);\n+    int pos = IntArrays.binarySearch(docWord.id2, 0, docWord.numV, id2);\n     if (pos >= 0) {\n       docWord.v[pos] += val;\n     } else {\n       if (docWord.id2.length == docWord.numV) {\n-        int[] ids2 =\n-          new int[docWord.id2.length << 1];\n-        int[] v =\n-          new int[docWord.id2.length << 1];\n-        System.arraycopy(docWord.id2, 0, ids2, 0,\n-          docWord.numV);\n-        System.arraycopy(docWord.v, 0, v, 0,\n-          docWord.numV);\n+        int[] ids2 = new int[docWord.id2.length << 1];\n+        int[] v = new int[docWord.id2.length << 1];\n+        System.arraycopy(docWord.id2, 0, ids2, 0, docWord.numV);\n+        System.arraycopy(docWord.v, 0, v, 0, docWord.numV);\n         docWord.id2 = ids2;\n         docWord.v = v;\n       }\n       int insertPos = -pos - 1;\n-      System.arraycopy(docWord.id2, insertPos,\n-        docWord.id2, insertPos + 1,\n-        docWord.numV - insertPos);\n-      System.arraycopy(docWord.v, insertPos,\n-        docWord.v, insertPos + 1,\n-        docWord.numV - insertPos);\n+      System.arraycopy(docWord.id2, insertPos, docWord.id2, insertPos + 1, docWord.numV - insertPos);\n+      System.arraycopy(docWord.v, insertPos, docWord.v, insertPos + 1, docWord.numV - insertPos);\n       docWord.id2[insertPos] = id2;\n       docWord.v[insertPos] = val;\n       docWord.numV++;\n     }\n   }\n \n+\n   static int createWordModel(\n     Table<TopicCountList>[] wordTableMap,\n     int numModelSlices,\n@@ -164,28 +144,20 @@ public class LDAUtil {\n     // allreduce column index and the element\n     // count on each column\n     long t1 = System.currentTimeMillis();\n-    mapper.regroup(\"lda\", \"regroup-word-model\",\n-      wordTable,\n-      new Partitioner(mapper.getNumWorkers()));\n+    mapper.regroup(\"lda\", \"regroup-word-model\", wordTable, new Partitioner(mapper.getNumWorkers()));\n     for (int i = 0; i < numModelSlices; i++) {\n-      wordTableMap[i] = new Table<>(i,\n-        new TopicCountListCombiner());\n+      wordTableMap[i] = new Table<>(i, new TopicCountListCombiner());\n     }\n-    IntArray idArray = IntArray.create(\n-      wordTable.getNumPartitions(), false);\n+    IntArray idArray = IntArray.create(wordTable.getNumPartitions(), false);\n     int[] ids = idArray.get();\n     wordTable.getPartitionIDs().toArray(ids);\n     int size = idArray.size();\n     IntArrays.quickSort(ids, 0, size);\n     for (int i = 0; i < size; i++) {\n       int sliceID = i % numModelSlices;\n-      TopicCountMap map =\n-        wordTable.getPartition(ids[i]).get();\n-      ObjectIterator<Int2IntMap.Entry> iterator =\n-        map.getTopicCount().int2IntEntrySet()\n-          .fastIterator();\n-      TopicCountList list =\n-        Writable.create(TopicCountList.class);\n+      TopicCountMap map = wordTable.getPartition(ids[i]).get();\n+      ObjectIterator<Int2IntMap.Entry> iterator = map.getTopicCount().int2IntEntrySet().fastIterator();\n+      TopicCountList list = Writable.create(TopicCountList.class);\n       LongArrayList array = list.getTopicCount();\n       while (iterator.hasNext()) {\n         Int2IntMap.Entry entry = iterator.next();\n@@ -193,27 +165,22 @@ public class LDAUtil {\n         long topicCount = entry.getIntValue();\n         array.add((topicCount << 32) + topicID);\n       }\n-      wordTableMap[sliceID].addPartition(\n-        new Partition<TopicCountList>(ids[i],\n-          list));\n+      wordTableMap[sliceID].addPartition(new Partition<TopicCountList>(ids[i], list));\n     }\n     idArray.release();\n     wordTable.release();\n     wordTable = null;\n-    Table<IntArray> wordSumTable =\n-      new Table<>(0, new IntArrPlus());\n+    Table<IntArray> wordSumTable = new Table<>(0, new IntArrPlus());\n     IntArray array = IntArray.create(1, false);\n     array.get()[0] = size;\n-    wordSumTable\n-      .addPartition(new Partition<>(0, array));\n-    mapper.allreduce(\"lda\", \"allreduce-wordsum\",\n-      wordSumTable);\n-    int vocabularySize =\n-      wordSumTable.getPartition(0).get().get()[0];\n+    wordSumTable.addPartition(new Partition<>(0, array));\n+    mapper.allreduce(\"lda\",\n+      \"allreduce-wordsum\", wordSumTable);\n+    int vocabularySize = wordSumTable.getPartition(0).get().get()[0];\n     wordSumTable.release();\n+\n     long t2 = System.currentTimeMillis();\n-    LOG.info(\"W model is created, vocabulary: \"\n-      + vocabularySize + \", took: \" + (t2 - t1));\n+    LOG.info(\"W model is created, vocabulary: \" + vocabularySize + \", took: \" + (t2 - t1));\n     return vocabularySize;\n   }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 81
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "3",
                    "severity": "error",
                    "message": "Line has trailing spaces.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "3",
                    "severity": "error",
                    "message": "Line has trailing spaces.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        }
    ],
    "repaired_by": [
        "intellij"
    ],
    "not_repaired_by": [
        "styler",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ]
}