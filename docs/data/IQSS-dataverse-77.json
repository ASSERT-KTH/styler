{
    "project_name": "IQSS-dataverse",
    "error_id": "77",
    "information": {
        "errors": [
            {
                "line": "65",
                "column": "1",
                "severity": "error",
                "message": "File contains tab characters (this is the first instance).",
                "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.FileTabCharacterCheck"
            }
        ]
    },
    "source_code": "    public FinalizeDatasetPublicationCommand(Dataset aDataset, DataverseRequest aRequest, boolean isPidPrePublished) {\n        super(aDataset, aRequest);\n\tdatasetExternallyReleased = isPidPrePublished;\n    }\n\n    @Override",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "87",
                    "column": "1",
                    "severity": "error",
                    "message": "File contains tab characters (this is the first instance).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.FileTabCharacterCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/77/FinalizeDatasetPublicationCommand.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler/77/FinalizeDatasetPublicationCommand.java\nindex 4fa07dedede..6408fcbbeb4 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/77/FinalizeDatasetPublicationCommand.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler/77/FinalizeDatasetPublicationCommand.java\n@@ -62,7 +62,7 @@ public class FinalizeDatasetPublicationCommand extends AbstractPublishDatasetCom\n     }\n     public FinalizeDatasetPublicationCommand(Dataset aDataset, DataverseRequest aRequest, boolean isPidPrePublished) {\n         super(aDataset, aRequest);\n-\tdatasetExternallyReleased = isPidPrePublished;\n+        datasetExternallyReleased = isPidPrePublished;\n     }\n \n     @Override\n",
            "diff_size": 1
        },
        {
            "tool": "intellij",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/77/FinalizeDatasetPublicationCommand.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/intellij/77/FinalizeDatasetPublicationCommand.java\nindex 4fa07dedede..530bbd3f8b7 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/77/FinalizeDatasetPublicationCommand.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/intellij/77/FinalizeDatasetPublicationCommand.java\n@@ -38,7 +38,6 @@ import org.apache.solr.client.solrj.SolrServerException;\n \n \n /**\n- *\n  * Takes the last internal steps in publishing a dataset.\n  *\n  * @author michael\n@@ -46,404 +45,423 @@ import org.apache.solr.client.solrj.SolrServerException;\n @RequiredPermissions(Permission.PublishDataset)\n public class FinalizeDatasetPublicationCommand extends AbstractPublishDatasetCommand<Dataset> {\n \n-    private static final Logger logger = Logger.getLogger(FinalizeDatasetPublicationCommand.class.getName());\n-    \n-    /**\n-     * mirror field from {@link PublishDatasetCommand} of same name\n+  private static final Logger logger = Logger.getLogger(FinalizeDatasetPublicationCommand.class.getName());\n+\n+  /**\n+   * mirror field from {@link PublishDatasetCommand} of same name\n+   */\n+  final boolean datasetExternallyReleased;\n+\n+  List<Dataverse> dataversesToIndex = new ArrayList<>();\n+\n+  public static final String FILE_VALIDATION_ERROR = \"FILE VALIDATION ERROR\";\n+\n+  public FinalizeDatasetPublicationCommand(Dataset aDataset, DataverseRequest aRequest) {\n+    this(aDataset, aRequest, false);\n+  }\n+\n+  public FinalizeDatasetPublicationCommand(Dataset aDataset, DataverseRequest aRequest, boolean isPidPrePublished) {\n+    super(aDataset, aRequest);\n+    datasetExternallyReleased = isPidPrePublished;\n+  }\n+\n+  @Override\n+  public Dataset execute(CommandContext ctxt) throws CommandException {\n+    Dataset theDataset = getDataset();\n+\n+    logger.info(\"Finalizing publication of the dataset \" + theDataset.getGlobalId().asString());\n+\n+    // validate the physical files before we do anything else:\n+    // (unless specifically disabled; or a minor version)\n+    if (theDataset.getLatestVersion().getVersionState() != RELEASED\n+      && theDataset.getLatestVersion().getMinorVersionNumber() != null\n+      && theDataset.getLatestVersion().getMinorVersionNumber().equals((long) 0)\n+      && ctxt.systemConfig().isDatafileValidationOnPublishEnabled()) {\n+      // some imported datasets may already be released.\n+\n+      // validate the physical files (verify checksums):\n+      validateDataFiles(theDataset, ctxt);\n+      // (this will throw a CommandException if it fails)\n+    }\n+\n+    /*\n+     * Try to register the dataset identifier. For PID providers that have registerWhenPublished == false (all except the FAKE provider at present)\n+     * the registerExternalIdentifier command will make one try to create the identifier if needed (e.g. if reserving at dataset creation wasn't done/failed).\n+     * For registerWhenPublished == true providers, if a PID conflict is found, the call will retry with new PIDs.\n      */\n-    final boolean datasetExternallyReleased;\n-    \n-    List<Dataverse> dataversesToIndex = new ArrayList<>();\n-    \n-    public static final String FILE_VALIDATION_ERROR = \"FILE VALIDATION ERROR\";\n-    \n-    public FinalizeDatasetPublicationCommand(Dataset aDataset, DataverseRequest aRequest) {\n-        this( aDataset, aRequest, false );\n+    if (theDataset.getGlobalIdCreateTime() == null) {\n+      try {\n+        // This can potentially throw a CommandException, so let's make\n+        // sure we exit cleanly:\n+\n+        registerExternalIdentifier(theDataset, ctxt, false);\n+      } catch (CommandException comEx) {\n+        logger.warning(\"Failed to reserve the identifier \" + theDataset.getGlobalId().asString() +\n+          \"; notifying the user(s), unlocking the dataset\");\n+        // Send failure notification to the user:\n+        notifyUsersDatasetPublishStatus(ctxt, theDataset, UserNotification.Type.PUBLISHFAILED_PIDREG);\n+        // Remove the dataset lock:\n+        ctxt.datasets().removeDatasetLocks(theDataset, DatasetLock.Reason.finalizePublication);\n+        // re-throw the exception:\n+        throw comEx;\n+      }\n+    }\n+\n+    // is this the first publication of the dataset?\n+    if (theDataset.getPublicationDate() == null) {\n+      theDataset.setReleaseUser((AuthenticatedUser) getUser());\n     }\n-    public FinalizeDatasetPublicationCommand(Dataset aDataset, DataverseRequest aRequest, boolean isPidPrePublished) {\n-        super(aDataset, aRequest);\n-\tdatasetExternallyReleased = isPidPrePublished;\n+    if (theDataset.getPublicationDate() == null) {\n+      theDataset.setPublicationDate(new Timestamp(new Date().getTime()));\n     }\n \n-    @Override\n-    public Dataset execute(CommandContext ctxt) throws CommandException {\n-        Dataset theDataset = getDataset();\n-        \n-        logger.info(\"Finalizing publication of the dataset \"+theDataset.getGlobalId().asString());\n-        \n-        // validate the physical files before we do anything else: \n-        // (unless specifically disabled; or a minor version)\n-        if (theDataset.getLatestVersion().getVersionState() != RELEASED\n-                && theDataset.getLatestVersion().getMinorVersionNumber() != null\n-                && theDataset.getLatestVersion().getMinorVersionNumber().equals((long) 0)\n-                && ctxt.systemConfig().isDatafileValidationOnPublishEnabled()) {\n-            // some imported datasets may already be released.\n-\n-            // validate the physical files (verify checksums):\n-            validateDataFiles(theDataset, ctxt);\n-            // (this will throw a CommandException if it fails)\n-        }\n+    // update metadata\n+    theDataset.getLatestVersion().setReleaseTime(getTimestamp());\n+    theDataset.getLatestVersion().setLastUpdateTime(getTimestamp());\n+    theDataset.setModificationTime(getTimestamp());\n+    theDataset.setFileAccessRequest(theDataset.getLatestVersion().getTermsOfUseAndAccess().isFileAccessRequest());\n \n-\t\t/*\n-\t\t * Try to register the dataset identifier. For PID providers that have registerWhenPublished == false (all except the FAKE provider at present)\n-\t\t * the registerExternalIdentifier command will make one try to create the identifier if needed (e.g. if reserving at dataset creation wasn't done/failed).\n-\t\t * For registerWhenPublished == true providers, if a PID conflict is found, the call will retry with new PIDs. \n-\t\t */\n-        if ( theDataset.getGlobalIdCreateTime() == null ) {\n-            try {\n-                // This can potentially throw a CommandException, so let's make \n-                // sure we exit cleanly:\n-\n-            \tregisterExternalIdentifier(theDataset, ctxt, false);\n-            } catch (CommandException comEx) {\n-                logger.warning(\"Failed to reserve the identifier \"+theDataset.getGlobalId().asString()+\"; notifying the user(s), unlocking the dataset\");\n-                // Send failure notification to the user: \n-                notifyUsersDatasetPublishStatus(ctxt, theDataset, UserNotification.Type.PUBLISHFAILED_PIDREG);\n-                // Remove the dataset lock: \n-                ctxt.datasets().removeDatasetLocks(theDataset, DatasetLock.Reason.finalizePublication);\n-                // re-throw the exception:\n-                throw comEx;\n-            }\n-        }\n-                \n-        // is this the first publication of the dataset?\n-        if (theDataset.getPublicationDate() == null) {\n-            theDataset.setReleaseUser((AuthenticatedUser) getUser());\n-        }\n-        if ( theDataset.getPublicationDate() == null ) {\n-            theDataset.setPublicationDate(new Timestamp(new Date().getTime()));\n-        } \n-\n-        // update metadata\n-        theDataset.getLatestVersion().setReleaseTime(getTimestamp());\n-        theDataset.getLatestVersion().setLastUpdateTime(getTimestamp());\n-        theDataset.setModificationTime(getTimestamp());\n-        theDataset.setFileAccessRequest(theDataset.getLatestVersion().getTermsOfUseAndAccess().isFileAccessRequest());\n-        \n-        updateFiles(getTimestamp(), ctxt);\n-        \n-        // \n-        // TODO: Not sure if this .merge() is necessary here - ? \n-        // I'm moving a bunch of code from PublishDatasetCommand here; and this .merge()\n-        // comes from there. There's a chance that the final merge, at the end of this\n-        // command, would be sufficient. -- L.A. Sep. 6 2017\n-        theDataset = ctxt.em().merge(theDataset);\n-        setDataset(theDataset);\n-        updateDatasetUser(ctxt);\n-        \n-        //if the publisher hasn't contributed to this version\n-        DatasetVersionUser ddu = ctxt.datasets().getDatasetVersionUser(theDataset.getLatestVersion(), getUser());\n-        \n-        if (ddu == null) {\n-            ddu = new DatasetVersionUser();\n-            ddu.setDatasetVersion(theDataset.getLatestVersion());\n-            String id = getUser().getIdentifier();\n-            id = id.startsWith(\"@\") ? id.substring(1) : id;\n-            AuthenticatedUser au = ctxt.authentication().getAuthenticatedUser(id);\n-            ddu.setAuthenticatedUser(au);\n-        }\n-        ddu.setLastUpdateDate(getTimestamp());\n-        ctxt.em().merge(ddu);\n-        \n-        try {\n-            updateParentDataversesSubjectsField(theDataset, ctxt);\n-        } catch (IOException | SolrServerException e) {\n-            String failureLogText = \"Post-publication indexing failed for Dataverse subject update. \";\n-            failureLogText += \"\\r\\n\" + e.getLocalizedMessage();\n-            LoggingUtil.writeOnSuccessFailureLog(this, failureLogText, theDataset);\n+    updateFiles(getTimestamp(), ctxt);\n \n-        }\n+    //\n+    // TODO: Not sure if this .merge() is necessary here - ?\n+    // I'm moving a bunch of code from PublishDatasetCommand here; and this .merge()\n+    // comes from there. There's a chance that the final merge, at the end of this\n+    // command, would be sufficient. -- L.A. Sep. 6 2017\n+    theDataset = ctxt.em().merge(theDataset);\n+    setDataset(theDataset);\n+    updateDatasetUser(ctxt);\n \n-        List<Command> previouslyCalled = ctxt.getCommandsCalled();\n-        \n-        PrivateUrl privateUrl = ctxt.engine().submit(new GetPrivateUrlCommand(getRequest(), theDataset));\n-        List<Command> afterSub = ctxt.getCommandsCalled();\n-        previouslyCalled.forEach((c) -> {\n-            ctxt.getCommandsCalled().add(c);\n-        });\n-        if (privateUrl != null) {\n-            ctxt.engine().submit(new DeletePrivateUrlCommand(getRequest(), theDataset));\n-        }\n-        \n-\tif (theDataset.getLatestVersion().getVersionState() != RELEASED) {\n-            // some imported datasets may already be released.\n-\n-            if (!datasetExternallyReleased) {\n-                publicizeExternalIdentifier(theDataset, ctxt);\n-                // Will throw a CommandException, unless successful.\n-                // This will end the execution of the command, but the method \n-                // above takes proper care to \"clean up after itself\" in case of\n-                // a failure - it will remove any locks, and it will send a\n-                // proper notification to the user(s). \n-            }\n-            theDataset.getLatestVersion().setVersionState(RELEASED);\n-        }\n-        \n-        final Dataset ds = ctxt.em().merge(theDataset);\n-        //Remove any pre-pub workflow lock (not needed as WorkflowServiceBean.workflowComplete() should already have removed it after setting the finalizePublication lock?)\n+    //if the publisher hasn't contributed to this version\n+    DatasetVersionUser ddu = ctxt.datasets().getDatasetVersionUser(theDataset.getLatestVersion(), getUser());\n+\n+    if (ddu == null) {\n+      ddu = new DatasetVersionUser();\n+      ddu.setDatasetVersion(theDataset.getLatestVersion());\n+      String id = getUser().getIdentifier();\n+      id = id.startsWith(\"@\") ? id.substring(1) : id;\n+      AuthenticatedUser au = ctxt.authentication().getAuthenticatedUser(id);\n+      ddu.setAuthenticatedUser(au);\n+    }\n+    ddu.setLastUpdateDate(getTimestamp());\n+    ctxt.em().merge(ddu);\n+\n+    try {\n+      updateParentDataversesSubjectsField(theDataset, ctxt);\n+    } catch (IOException | SolrServerException e) {\n+      String failureLogText = \"Post-publication indexing failed for Dataverse subject update. \";\n+      failureLogText += \"\\r\\n\" + e.getLocalizedMessage();\n+      LoggingUtil.writeOnSuccessFailureLog(this, failureLogText, theDataset);\n+\n+    }\n+\n+    List<Command> previouslyCalled = ctxt.getCommandsCalled();\n+\n+    PrivateUrl privateUrl = ctxt.engine().submit(new GetPrivateUrlCommand(getRequest(), theDataset));\n+    List<Command> afterSub = ctxt.getCommandsCalled();\n+    previouslyCalled.forEach((c) -> {\n+      ctxt.getCommandsCalled().add(c);\n+    });\n+    if (privateUrl != null) {\n+      ctxt.engine().submit(new DeletePrivateUrlCommand(getRequest(), theDataset));\n+    }\n+\n+    if (theDataset.getLatestVersion().getVersionState() != RELEASED) {\n+      // some imported datasets may already be released.\n+\n+      if (!datasetExternallyReleased) {\n+        publicizeExternalIdentifier(theDataset, ctxt);\n+        // Will throw a CommandException, unless successful.\n+        // This will end the execution of the command, but the method\n+        // above takes proper care to \"clean up after itself\" in case of\n+        // a failure - it will remove any locks, and it will send a\n+        // proper notification to the user(s).\n+      }\n+      theDataset.getLatestVersion().setVersionState(RELEASED);\n+    }\n+\n+    final Dataset ds = ctxt.em().merge(theDataset);\n+    //Remove any pre-pub workflow lock (not needed as WorkflowServiceBean.workflowComplete() should already have removed it after setting the finalizePublication lock?)\n+    ctxt.datasets().removeDatasetLocks(ds, DatasetLock.Reason.Workflow);\n+\n+    //Should this be in onSuccess()?\n+    ctxt.workflows().getDefaultWorkflow(TriggerType.PostPublishDataset).ifPresent(wf -> {\n+      try {\n+        ctxt.workflows().start(wf, buildContext(ds, TriggerType.PostPublishDataset, datasetExternallyReleased), false);\n+      } catch (CommandException ex) {\n         ctxt.datasets().removeDatasetLocks(ds, DatasetLock.Reason.Workflow);\n-        \n-        //Should this be in onSuccess()?\n-        ctxt.workflows().getDefaultWorkflow(TriggerType.PostPublishDataset).ifPresent(wf -> {\n-            try {\n-                ctxt.workflows().start(wf, buildContext(ds, TriggerType.PostPublishDataset, datasetExternallyReleased), false);\n-            } catch (CommandException ex) {\n-                ctxt.datasets().removeDatasetLocks(ds, DatasetLock.Reason.Workflow);\n-                logger.log(Level.SEVERE, \"Error invoking post-publish workflow: \" + ex.getMessage(), ex);\n-            }\n-        });\n-\n-        Dataset readyDataset = ctxt.em().merge(ds);\n-        \n-        // Finally, unlock the dataset (leaving any post-publish workflow lock in place)\n-        ctxt.datasets().removeDatasetLocks(readyDataset, DatasetLock.Reason.finalizePublication);\n-        if (readyDataset.isLockedFor(DatasetLock.Reason.InReview) ) {\n-            ctxt.datasets().removeDatasetLocks(readyDataset, DatasetLock.Reason.InReview);\n-        }\n-        \n-        logger.info(\"Successfully published the dataset \"+readyDataset.getGlobalId().asString());\n-        readyDataset = ctxt.em().merge(readyDataset);\n-        \n-        return readyDataset;\n+        logger.log(Level.SEVERE, \"Error invoking post-publish workflow: \" + ex.getMessage(), ex);\n+      }\n+    });\n+\n+    Dataset readyDataset = ctxt.em().merge(ds);\n+\n+    // Finally, unlock the dataset (leaving any post-publish workflow lock in place)\n+    ctxt.datasets().removeDatasetLocks(readyDataset, DatasetLock.Reason.finalizePublication);\n+    if (readyDataset.isLockedFor(DatasetLock.Reason.InReview)) {\n+      ctxt.datasets().removeDatasetLocks(readyDataset, DatasetLock.Reason.InReview);\n     }\n-    \n-    @Override\n-    public boolean onSuccess(CommandContext ctxt, Object r) {\n-        boolean retVal = true;\n-        Dataset dataset = null;\n-        try{\n-            dataset = (Dataset) r;\n-        } catch (ClassCastException e){\n-            dataset  = ((PublishDatasetResult) r).getDataset();\n-        }\n-        \n-        try {\n-            // Success! - send notification:\n-            notifyUsersDatasetPublishStatus(ctxt, dataset, UserNotification.Type.PUBLISHEDDS);\n-        } catch (Exception e) {\n-            logger.warning(\"Failure to send dataset published messages for : \" + dataset.getId() + \" : \" + e.getMessage());\n-        }\n-        try {\n-            Future<String> indexString = ctxt.index().indexDataset(dataset, true);                   \n-        } catch (IOException | SolrServerException e) {    \n-            String failureLogText = \"Post-publication indexing failed. You can kick off a re-index of this dataset with: \\r\\n curl http://localhost:8080/api/admin/index/datasets/\" + dataset.getId().toString();\n-            failureLogText += \"\\r\\n\" + e.getLocalizedMessage();\n-            LoggingUtil.writeOnSuccessFailureLog(this, failureLogText,  dataset);\n-            retVal = false;\n-        }\n-        \n-        //re-indexing dataverses that have additional subjects\n-        if (!dataversesToIndex.isEmpty()){\n-            for (Dataverse dv : dataversesToIndex) {\n-                try {\n-                    Future<String> indexString = ctxt.index().indexDataverse(dv);\n-                } catch (IOException | SolrServerException e) {\n-                    String failureLogText = \"Post-publication indexing failed. You can kick off a re-index of this dataverse with: \\r\\n curl http://localhost:8080/api/admin/index/dataverses/\" + dv.getId().toString();\n-                    failureLogText += \"\\r\\n\" + e.getLocalizedMessage();\n-                    LoggingUtil.writeOnSuccessFailureLog(this, failureLogText, dataset);\n-                    retVal = false;\n-                } \n-            }\n-        }\n \n-        exportMetadata(dataset);\n-                \n-        ctxt.datasets().updateLastExportTimeStamp(dataset.getId());\n+    logger.info(\"Successfully published the dataset \" + readyDataset.getGlobalId().asString());\n+    readyDataset = ctxt.em().merge(readyDataset);\n+\n+    return readyDataset;\n+  }\n \n-        return retVal;\n+  @Override\n+  public boolean onSuccess(CommandContext ctxt, Object r) {\n+    boolean retVal = true;\n+    Dataset dataset = null;\n+    try {\n+      dataset = (Dataset) r;\n+    } catch (ClassCastException e) {\n+      dataset = ((PublishDatasetResult) r).getDataset();\n     }\n \n-    /**\n-     * Attempting to run metadata export, for all the formats for which we have\n-     * metadata Exporters.\n-     */\n-    private void exportMetadata(Dataset dataset) {\n+    try {\n+      // Success! - send notification:\n+      notifyUsersDatasetPublishStatus(ctxt, dataset, UserNotification.Type.PUBLISHEDDS);\n+    } catch (Exception e) {\n+      logger.warning(\"Failure to send dataset published messages for : \" + dataset.getId() + \" : \" + e.getMessage());\n+    }\n+    try {\n+      Future<String> indexString = ctxt.index().indexDataset(dataset, true);\n+    } catch (IOException | SolrServerException e) {\n+      String failureLogText =\n+        \"Post-publication indexing failed. You can kick off a re-index of this dataset with: \\r\\n curl http://localhost:8080/api/admin/index/datasets/\" +\n+          dataset.getId().toString();\n+      failureLogText += \"\\r\\n\" + e.getLocalizedMessage();\n+      LoggingUtil.writeOnSuccessFailureLog(this, failureLogText, dataset);\n+      retVal = false;\n+    }\n \n+    //re-indexing dataverses that have additional subjects\n+    if (!dataversesToIndex.isEmpty()) {\n+      for (Dataverse dv : dataversesToIndex) {\n         try {\n-            ExportService instance = ExportService.getInstance();\n-            instance.exportAllFormats(dataset);\n-\n-        } catch (Exception ex) {\n-            // Something went wrong!\n-            // Just like with indexing, a failure to export is not a fatal\n-            // condition. We'll just log the error as a warning and keep\n-            // going:\n-            logger.log(Level.WARNING, \"Dataset publication finalization: exception while exporting:{0}\", ex.getMessage());\n+          Future<String> indexString = ctxt.index().indexDataverse(dv);\n+        } catch (IOException | SolrServerException e) {\n+          String failureLogText =\n+            \"Post-publication indexing failed. You can kick off a re-index of this dataverse with: \\r\\n curl http://localhost:8080/api/admin/index/dataverses/\" +\n+              dv.getId().toString();\n+          failureLogText += \"\\r\\n\" + e.getLocalizedMessage();\n+          LoggingUtil.writeOnSuccessFailureLog(this, failureLogText, dataset);\n+          retVal = false;\n         }\n+      }\n     }\n \n-    /**\n-     * add the dataset subjects to all parent dataverses.\n-     */\n-    private void updateParentDataversesSubjectsField(Dataset savedDataset, CommandContext ctxt) throws  SolrServerException, IOException {\n-        \n-        for (DatasetField dsf : savedDataset.getLatestVersion().getDatasetFields()) {\n-            if (dsf.getDatasetFieldType().getName().equals(DatasetFieldConstant.subject)) {\n-                Dataverse dv = savedDataset.getOwner();\n-                while (dv != null) {\n-                    boolean newSubjectsAdded = false;\n-                    for (ControlledVocabularyValue cvv : dsf.getControlledVocabularyValues()) {                   \n-                        if (!dv.getDataverseSubjects().contains(cvv)) {\n-                            logger.fine(\"dv \"+dv.getAlias()+\" does not have subject \"+cvv.getStrValue());\n-                            newSubjectsAdded = true;\n-                            dv.getDataverseSubjects().add(cvv);\n-                        } else {\n-                            logger.fine(\"dv \"+dv.getAlias()+\" already has subject \"+cvv.getStrValue());\n-                        }\n-                    }\n-                    if (newSubjectsAdded) {\n-                        logger.fine(\"new dataverse subjects added - saving and reindexing in OnSuccess\");\n-                        Dataverse dvWithSubjectJustAdded = ctxt.em().merge(dv);\n-                        ctxt.em().flush();\n-                        //adding dv to list of those we need to re-index for new subjects\n-                        dataversesToIndex.add(dvWithSubjectJustAdded);                       \n-                    } else {\n-                        logger.fine(\"no new subjects added to the dataverse; skipping reindexing\");\n-                    }\n-                    dv = dv.getOwner();\n-                }\n-                break; // we just update the field whose name is DatasetFieldConstant.subject\n-            }\n-        }\n+    exportMetadata(dataset);\n+\n+    ctxt.datasets().updateLastExportTimeStamp(dataset.getId());\n+\n+    return retVal;\n+  }\n+\n+  /**\n+   * Attempting to run metadata export, for all the formats for which we have\n+   * metadata Exporters.\n+   */\n+  private void exportMetadata(Dataset dataset) {\n+\n+    try {\n+      ExportService instance = ExportService.getInstance();\n+      instance.exportAllFormats(dataset);\n+\n+    } catch (Exception ex) {\n+      // Something went wrong!\n+      // Just like with indexing, a failure to export is not a fatal\n+      // condition. We'll just log the error as a warning and keep\n+      // going:\n+      logger.log(Level.WARNING, \"Dataset publication finalization: exception while exporting:{0}\", ex.getMessage());\n     }\n+  }\n \n-    private void validateDataFiles(Dataset dataset, CommandContext ctxt) throws CommandException {\n-        try {\n-            for (DataFile dataFile : dataset.getFiles()) {\n-                // TODO: Should we validate all the files in the dataset, or only \n-                // the files that haven't been published previously?\n-                // (the decision was made to validate all the files on every  \n-                // major release; we can revisit the decision if there's any \n-                // indication that this makes publishing take significantly longer.\n-                logger.log(Level.FINE, \"validating DataFile {0}\", dataFile.getId());\n-                FileUtil.validateDataFileChecksum(dataFile);\n-            }\n-        } catch (Throwable e) {\n-            \n-            if (dataset.isLockedFor(DatasetLock.Reason.finalizePublication)) {\n-                DatasetLock lock = dataset.getLockFor(DatasetLock.Reason.finalizePublication);\n-                lock.setReason(DatasetLock.Reason.FileValidationFailed);\n-                lock.setInfo(FILE_VALIDATION_ERROR);\n-                ctxt.datasets().updateDatasetLock(lock);\n-            } else {            \n-                // Lock the dataset with a new FileValidationFailed lock: \n-                DatasetLock lock = new DatasetLock(DatasetLock.Reason.FileValidationFailed, getRequest().getAuthenticatedUser()); //(AuthenticatedUser)getUser());\n-                lock.setDataset(dataset);\n-                lock.setInfo(FILE_VALIDATION_ERROR);\n-                ctxt.datasets().addDatasetLock(dataset, lock);\n+  /**\n+   * add the dataset subjects to all parent dataverses.\n+   */\n+  private void updateParentDataversesSubjectsField(Dataset savedDataset, CommandContext ctxt)\n+    throws SolrServerException, IOException {\n+\n+    for (DatasetField dsf : savedDataset.getLatestVersion().getDatasetFields()) {\n+      if (dsf.getDatasetFieldType().getName().equals(DatasetFieldConstant.subject)) {\n+        Dataverse dv = savedDataset.getOwner();\n+        while (dv != null) {\n+          boolean newSubjectsAdded = false;\n+          for (ControlledVocabularyValue cvv : dsf.getControlledVocabularyValues()) {\n+            if (!dv.getDataverseSubjects().contains(cvv)) {\n+              logger.fine(\"dv \" + dv.getAlias() + \" does not have subject \" + cvv.getStrValue());\n+              newSubjectsAdded = true;\n+              dv.getDataverseSubjects().add(cvv);\n+            } else {\n+              logger.fine(\"dv \" + dv.getAlias() + \" already has subject \" + cvv.getStrValue());\n             }\n-            \n-            // Throw a new CommandException; if the command is being called \n-            // synchronously, it will be intercepted and the page will display \n-            // the error message for the user.\n-            throw new CommandException(BundleUtil.getStringFromBundle(\"dataset.publish.file.validation.error.details\"), this);\n+          }\n+          if (newSubjectsAdded) {\n+            logger.fine(\"new dataverse subjects added - saving and reindexing in OnSuccess\");\n+            Dataverse dvWithSubjectJustAdded = ctxt.em().merge(dv);\n+            ctxt.em().flush();\n+            //adding dv to list of those we need to re-index for new subjects\n+            dataversesToIndex.add(dvWithSubjectJustAdded);\n+          } else {\n+            logger.fine(\"no new subjects added to the dataverse; skipping reindexing\");\n+          }\n+          dv = dv.getOwner();\n         }\n+        break; // we just update the field whose name is DatasetFieldConstant.subject\n+      }\n     }\n-    \n-    private void publicizeExternalIdentifier(Dataset dataset, CommandContext ctxt) throws CommandException {\n-        String protocol = getDataset().getProtocol();\n-        String authority = getDataset().getAuthority();\n-        GlobalIdServiceBean idServiceBean = GlobalIdServiceBean.getBean(protocol, ctxt);\n- \n-        if (idServiceBean != null) {\n-            List<String> args = idServiceBean.getProviderInformation();\n-            try {\n-                String currentGlobalIdProtocol = ctxt.settings().getValueForKey(SettingsServiceBean.Key.Protocol, \"\");\n-                String currentGlobalAuthority = ctxt.settings().getValueForKey(SettingsServiceBean.Key.Authority, \"\");\n-                String dataFilePIDFormat = ctxt.settings().getValueForKey(SettingsServiceBean.Key.DataFilePIDFormat, \"DEPENDENT\");\n-                boolean isFilePIDsEnabled = ctxt.systemConfig().isFilePIDsEnabled();\n-                // We will skip trying to register the global identifiers for datafiles \n-                // if \"dependent\" file-level identifiers are requested, AND the naming \n-                // protocol, or the authority of the dataset global id is different from \n-                // what's currently configured for the Dataverse. In other words\n-                // we can't get \"dependent\" DOIs assigned to files in a dataset  \n-                // with the registered id that is a handle; or even a DOI, but in \n-                // an authority that's different from what's currently configured.\n-                // Additionaly in 4.9.3 we have added a system variable to disable \n-                // registering file PIDs on the installation level.\n-                if (((currentGlobalIdProtocol.equals(protocol) && currentGlobalAuthority.equals(authority))\n-                        || dataFilePIDFormat.equals(\"INDEPENDENT\"))\n-                        && isFilePIDsEnabled\n-                        && dataset.getLatestVersion().getMinorVersionNumber() != null\n-                        && dataset.getLatestVersion().getMinorVersionNumber().equals((long) 0)) {\n-                    //A false return value indicates a failure in calling the service\n-                    for (DataFile df : dataset.getFiles()) {\n-                        logger.log(Level.FINE, \"registering global id for file {0}\", df.getId());\n-                        //A false return value indicates a failure in calling the service\n-                        if (!idServiceBean.publicizeIdentifier(df)) {\n-                            throw new Exception();\n-                        }\n-                        df.setGlobalIdCreateTime(getTimestamp());\n-                        df.setIdentifierRegistered(true);\n-                    }\n-                }\n-                if (!idServiceBean.publicizeIdentifier(dataset)) {\n-                    throw new Exception();\n-                }\n-                dataset.setGlobalIdCreateTime(new Date()); // TODO these two methods should be in the responsibility of the idServiceBean.\n-                dataset.setIdentifierRegistered(true);\n-            } catch (Throwable e) {\n-                logger.warning(\"Failed to register the identifier \"+dataset.getGlobalId().asString()+\", or to register a file in the dataset; notifying the user(s), unlocking the dataset\");\n-\n-                // Send failure notification to the user: \n-                notifyUsersDatasetPublishStatus(ctxt, dataset, UserNotification.Type.PUBLISHFAILED_PIDREG);\n-                \n-                ctxt.datasets().removeDatasetLocks(dataset, DatasetLock.Reason.finalizePublication);\n-                throw new CommandException(BundleUtil.getStringFromBundle(\"dataset.publish.error\", args), this);\n-            }\n-        }\n+  }\n+\n+  private void validateDataFiles(Dataset dataset, CommandContext ctxt) throws CommandException {\n+    try {\n+      for (DataFile dataFile : dataset.getFiles()) {\n+        // TODO: Should we validate all the files in the dataset, or only\n+        // the files that haven't been published previously?\n+        // (the decision was made to validate all the files on every\n+        // major release; we can revisit the decision if there's any\n+        // indication that this makes publishing take significantly longer.\n+        logger.log(Level.FINE, \"validating DataFile {0}\", dataFile.getId());\n+        FileUtil.validateDataFileChecksum(dataFile);\n+      }\n+    } catch (Throwable e) {\n+\n+      if (dataset.isLockedFor(DatasetLock.Reason.finalizePublication)) {\n+        DatasetLock lock = dataset.getLockFor(DatasetLock.Reason.finalizePublication);\n+        lock.setReason(DatasetLock.Reason.FileValidationFailed);\n+        lock.setInfo(FILE_VALIDATION_ERROR);\n+        ctxt.datasets().updateDatasetLock(lock);\n+      } else {\n+        // Lock the dataset with a new FileValidationFailed lock:\n+        DatasetLock lock = new DatasetLock(DatasetLock.Reason.FileValidationFailed,\n+          getRequest().getAuthenticatedUser()); //(AuthenticatedUser)getUser());\n+        lock.setDataset(dataset);\n+        lock.setInfo(FILE_VALIDATION_ERROR);\n+        ctxt.datasets().addDatasetLock(dataset, lock);\n+      }\n+\n+      // Throw a new CommandException; if the command is being called\n+      // synchronously, it will be intercepted and the page will display\n+      // the error message for the user.\n+      throw new CommandException(BundleUtil.getStringFromBundle(\"dataset.publish.file.validation.error.details\"), this);\n     }\n-    \n-    private void updateFiles(Timestamp updateTime, CommandContext ctxt) throws CommandException {\n-        for (DataFile dataFile : getDataset().getFiles()) {\n-            if (dataFile.getPublicationDate() == null) {\n-                // this is a new, previously unpublished file, so publish by setting date\n-                dataFile.setPublicationDate(updateTime);\n-                \n-                // check if any prexisting roleassignments have file download and send notifications\n-                notifyUsersFileDownload(ctxt, dataFile);\n-            }\n-            \n-            // set the files restriction flag to the same as the latest version's\n-            if (dataFile.getFileMetadata() != null && dataFile.getFileMetadata().getDatasetVersion().equals(getDataset().getLatestVersion())) {\n-                dataFile.setRestricted(dataFile.getFileMetadata().isRestricted());\n-            }\n-            \n-            \n-            if (dataFile.isRestricted()) {\n-                // If the file has been restricted: \n-                //    If this (image) file has been assigned as the dedicated \n-                //    thumbnail for the dataset, we need to remove that assignment, \n-                //    now that the file is restricted. \n-               \n-                // Dataset thumbnail assignment: \n-                \n-                if (dataFile.equals(getDataset().getThumbnailFile())) {\n-                    getDataset().setThumbnailFile(null);\n-                }\n+  }\n+\n+  private void publicizeExternalIdentifier(Dataset dataset, CommandContext ctxt) throws CommandException {\n+    String protocol = getDataset().getProtocol();\n+    String authority = getDataset().getAuthority();\n+    GlobalIdServiceBean idServiceBean = GlobalIdServiceBean.getBean(protocol, ctxt);\n+\n+    if (idServiceBean != null) {\n+      List<String> args = idServiceBean.getProviderInformation();\n+      try {\n+        String currentGlobalIdProtocol = ctxt.settings().getValueForKey(SettingsServiceBean.Key.Protocol, \"\");\n+        String currentGlobalAuthority = ctxt.settings().getValueForKey(SettingsServiceBean.Key.Authority, \"\");\n+        String dataFilePIDFormat =\n+          ctxt.settings().getValueForKey(SettingsServiceBean.Key.DataFilePIDFormat, \"DEPENDENT\");\n+        boolean isFilePIDsEnabled = ctxt.systemConfig().isFilePIDsEnabled();\n+        // We will skip trying to register the global identifiers for datafiles\n+        // if \"dependent\" file-level identifiers are requested, AND the naming\n+        // protocol, or the authority of the dataset global id is different from\n+        // what's currently configured for the Dataverse. In other words\n+        // we can't get \"dependent\" DOIs assigned to files in a dataset\n+        // with the registered id that is a handle; or even a DOI, but in\n+        // an authority that's different from what's currently configured.\n+        // Additionaly in 4.9.3 we have added a system variable to disable\n+        // registering file PIDs on the installation level.\n+        if (((currentGlobalIdProtocol.equals(protocol) && currentGlobalAuthority.equals(authority))\n+          || dataFilePIDFormat.equals(\"INDEPENDENT\"))\n+          && isFilePIDsEnabled\n+          && dataset.getLatestVersion().getMinorVersionNumber() != null\n+          && dataset.getLatestVersion().getMinorVersionNumber().equals((long) 0)) {\n+          //A false return value indicates a failure in calling the service\n+          for (DataFile df : dataset.getFiles()) {\n+            logger.log(Level.FINE, \"registering global id for file {0}\", df.getId());\n+            //A false return value indicates a failure in calling the service\n+            if (!idServiceBean.publicizeIdentifier(df)) {\n+              throw new Exception();\n             }\n+            df.setGlobalIdCreateTime(getTimestamp());\n+            df.setIdentifierRegistered(true);\n+          }\n         }\n+        if (!idServiceBean.publicizeIdentifier(dataset)) {\n+          throw new Exception();\n+        }\n+        dataset.setGlobalIdCreateTime(\n+          new Date()); // TODO these two methods should be in the responsibility of the idServiceBean.\n+        dataset.setIdentifierRegistered(true);\n+      } catch (Throwable e) {\n+        logger.warning(\"Failed to register the identifier \" + dataset.getGlobalId().asString() +\n+          \", or to register a file in the dataset; notifying the user(s), unlocking the dataset\");\n+\n+        // Send failure notification to the user:\n+        notifyUsersDatasetPublishStatus(ctxt, dataset, UserNotification.Type.PUBLISHFAILED_PIDREG);\n+\n+        ctxt.datasets().removeDatasetLocks(dataset, DatasetLock.Reason.finalizePublication);\n+        throw new CommandException(BundleUtil.getStringFromBundle(\"dataset.publish.error\", args), this);\n+      }\n     }\n-    \n-   \n-    //These notification methods are fairly similar, but it was cleaner to create a few copies.\n-    //If more notifications are needed in this command, they should probably be collapsed.\n-    private void notifyUsersFileDownload(CommandContext ctxt, DvObject subject) {\n-        ctxt.roles().directRoleAssignments(subject).stream()\n-            .filter(  ra -> ra.getRole().permissions().contains(Permission.DownloadFile) )\n-            .flatMap( ra -> ctxt.roleAssignees().getExplicitUsers(ctxt.roleAssignees().getRoleAssignee(ra.getAssigneeIdentifier())).stream() )\n-            .distinct() // prevent double-send\n-            .forEach( au -> ctxt.notifications().sendNotification(au, getTimestamp(), UserNotification.Type.GRANTFILEACCESS, getDataset().getId()) );\n-    }\n-    \n-    private void notifyUsersDatasetPublishStatus(CommandContext ctxt, DvObject subject, UserNotification.Type type) {\n-        \n-        ctxt.roles().rolesAssignments(subject).stream()\n-            .filter(  ra -> ra.getRole().permissions().contains(Permission.ViewUnpublishedDataset) || ra.getRole().permissions().contains(Permission.DownloadFile))\n-            .flatMap( ra -> ctxt.roleAssignees().getExplicitUsers(ctxt.roleAssignees().getRoleAssignee(ra.getAssigneeIdentifier())).stream() )\n-            .distinct() // prevent double-send\n-            //.forEach( au -> ctxt.notifications().sendNotification(au, timestamp, messageType, theDataset.getId()) ); //not sure why this line doesn't work instead\n-            .forEach( au -> ctxt.notifications().sendNotificationInNewTransaction(au, getTimestamp(), type, getDataset().getLatestVersion().getId()) ); \n+  }\n+\n+  private void updateFiles(Timestamp updateTime, CommandContext ctxt) throws CommandException {\n+    for (DataFile dataFile : getDataset().getFiles()) {\n+      if (dataFile.getPublicationDate() == null) {\n+        // this is a new, previously unpublished file, so publish by setting date\n+        dataFile.setPublicationDate(updateTime);\n+\n+        // check if any prexisting roleassignments have file download and send notifications\n+        notifyUsersFileDownload(ctxt, dataFile);\n+      }\n+\n+      // set the files restriction flag to the same as the latest version's\n+      if (dataFile.getFileMetadata() != null &&\n+        dataFile.getFileMetadata().getDatasetVersion().equals(getDataset().getLatestVersion())) {\n+        dataFile.setRestricted(dataFile.getFileMetadata().isRestricted());\n+      }\n+\n+\n+      if (dataFile.isRestricted()) {\n+        // If the file has been restricted:\n+        //    If this (image) file has been assigned as the dedicated\n+        //    thumbnail for the dataset, we need to remove that assignment,\n+        //    now that the file is restricted.\n+\n+        // Dataset thumbnail assignment:\n+\n+        if (dataFile.equals(getDataset().getThumbnailFile())) {\n+          getDataset().setThumbnailFile(null);\n+        }\n+      }\n     }\n+  }\n+\n+\n+  //These notification methods are fairly similar, but it was cleaner to create a few copies.\n+  //If more notifications are needed in this command, they should probably be collapsed.\n+  private void notifyUsersFileDownload(CommandContext ctxt, DvObject subject) {\n+    ctxt.roles().directRoleAssignments(subject).stream()\n+      .filter(ra -> ra.getRole().permissions().contains(Permission.DownloadFile))\n+      .flatMap(\n+        ra -> ctxt.roleAssignees().getExplicitUsers(ctxt.roleAssignees().getRoleAssignee(ra.getAssigneeIdentifier()))\n+          .stream())\n+      .distinct() // prevent double-send\n+      .forEach(au -> ctxt.notifications()\n+        .sendNotification(au, getTimestamp(), UserNotification.Type.GRANTFILEACCESS, getDataset().getId()));\n+  }\n+\n+  private void notifyUsersDatasetPublishStatus(CommandContext ctxt, DvObject subject, UserNotification.Type type) {\n+\n+    ctxt.roles().rolesAssignments(subject).stream()\n+      .filter(ra -> ra.getRole().permissions().contains(Permission.ViewUnpublishedDataset) ||\n+        ra.getRole().permissions().contains(Permission.DownloadFile))\n+      .flatMap(\n+        ra -> ctxt.roleAssignees().getExplicitUsers(ctxt.roleAssignees().getRoleAssignee(ra.getAssigneeIdentifier()))\n+          .stream())\n+      .distinct() // prevent double-send\n+      //.forEach( au -> ctxt.notifications().sendNotification(au, timestamp, messageType, theDataset.getId()) ); //not sure why this line doesn't work instead\n+      .forEach(au -> ctxt.notifications()\n+        .sendNotificationInNewTransaction(au, getTimestamp(), type, getDataset().getLatestVersion().getId()));\n+  }\n \n }\n",
            "diff_size": 560
        },
        {
            "tool": "naturalize",
            "errors": null,
            "diff": null
        },
        {
            "tool": "codebuff",
            "errors": null,
            "diff": null
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "87",
                    "column": "1",
                    "severity": "error",
                    "message": "File contains tab characters (this is the first instance).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.FileTabCharacterCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/77/FinalizeDatasetPublicationCommand.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_random/77/FinalizeDatasetPublicationCommand.java\nindex 4fa07dedede..6408fcbbeb4 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/77/FinalizeDatasetPublicationCommand.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_random/77/FinalizeDatasetPublicationCommand.java\n@@ -62,7 +62,7 @@ public class FinalizeDatasetPublicationCommand extends AbstractPublishDatasetCom\n     }\n     public FinalizeDatasetPublicationCommand(Dataset aDataset, DataverseRequest aRequest, boolean isPidPrePublished) {\n         super(aDataset, aRequest);\n-\tdatasetExternallyReleased = isPidPrePublished;\n+        datasetExternallyReleased = isPidPrePublished;\n     }\n \n     @Override\n",
            "diff_size": 1
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "87",
                    "column": "1",
                    "severity": "error",
                    "message": "File contains tab characters (this is the first instance).",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.FileTabCharacterCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/77/FinalizeDatasetPublicationCommand.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_three_grams/77/FinalizeDatasetPublicationCommand.java\nindex 4fa07dedede..6408fcbbeb4 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/77/FinalizeDatasetPublicationCommand.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_three_grams/77/FinalizeDatasetPublicationCommand.java\n@@ -62,7 +62,7 @@ public class FinalizeDatasetPublicationCommand extends AbstractPublishDatasetCom\n     }\n     public FinalizeDatasetPublicationCommand(Dataset aDataset, DataverseRequest aRequest, boolean isPidPrePublished) {\n         super(aDataset, aRequest);\n-\tdatasetExternallyReleased = isPidPrePublished;\n+        datasetExternallyReleased = isPidPrePublished;\n     }\n \n     @Override\n",
            "diff_size": 1
        }
    ],
    "repaired_by": [
        "intellij"
    ],
    "not_repaired_by": [
        "styler",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ]
}