{
    "project_name": "codefollower-H2-Research",
    "error_id": "223",
    "information": {
        "errors": [
            {
                "line": "297",
                "column": "9",
                "severity": "warning",
                "message": "'if' is not followed by whitespace.",
                "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.WhitespaceAfterCheck"
            }
        ]
    },
    "source_code": "        fileStore = (FileStore) config.get(\"fileStore\");\n        fileStoreIsProvided = fileStore != null;\n        if(fileStore == null && fileName != null) {\n            fileStore = new FileStore();\n        }\n        o = config.get(\"pageSplitSize\");",
    "results": [
        {
            "tool": "styler",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/errored/1/223/MVStore.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/styler/223/MVStore.java\nindex 9906203edd7..b646f44c169 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/errored/1/223/MVStore.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/styler/223/MVStore.java\n@@ -294,7 +294,7 @@ public final class MVStore {\n         String fileName = (String) config.get(\"fileName\");\n         fileStore = (FileStore) config.get(\"fileStore\");\n         fileStoreIsProvided = fileStore != null;\n-        if(fileStore == null && fileName != null) {\n+        if (fileStore == null && fileName != null) {\n             fileStore = new FileStore();\n         }\n         o = config.get(\"pageSplitSize\");\n",
            "diff_size": 1
        },
        {
            "tool": "intellij",
            "errors": [
                {
                    "line": "487",
                    "column": "73",
                    "severity": "warning",
                    "message": "';' is followed by whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.EmptyForIteratorPadCheck"
                },
                {
                    "line": "692",
                    "column": "74",
                    "severity": "warning",
                    "message": "';' is followed by whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.EmptyForIteratorPadCheck"
                },
                {
                    "line": "1304",
                    "column": "72",
                    "severity": "warning",
                    "message": "';' is followed by whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.EmptyForIteratorPadCheck"
                },
                {
                    "line": "1896",
                    "column": "61",
                    "severity": "warning",
                    "message": "';' is followed by whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.EmptyForIteratorPadCheck"
                },
                {
                    "line": "2162",
                    "column": "26",
                    "severity": "warning",
                    "message": "';' is followed by whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.EmptyForIteratorPadCheck"
                },
                {
                    "line": "2362",
                    "column": "24",
                    "severity": "warning",
                    "message": "';' is followed by whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.EmptyForIteratorPadCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/errored/1/223/MVStore.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/intellij/223/MVStore.java\nindex 9906203edd7..5f62c32ef15 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/errored/1/223/MVStore.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/intellij/223/MVStore.java\n@@ -3,6 +3,7 @@\n  * and the EPL 1.0 (http://h2database.com/html/license.html).\n  * Initial Developer: H2 Group\n  */\n+\n package org.h2.mvstore;\n \n import java.lang.Thread.UncaughtExceptionHandler;\n@@ -18,6 +19,7 @@ import java.util.Map;\n import java.util.Map.Entry;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n+\n import org.h2.compress.CompressDeflate;\n import org.h2.compress.CompressLZF;\n import org.h2.compress.Compressor;\n@@ -122,2827 +124,2827 @@ MVStore:\n  */\n public final class MVStore {\n \n-    /**\n-     * Whether assertions are enabled.\n-     */\n-    public static final boolean ASSERT = false;\n-\n-    /**\n-     * The block size (physical sector size) of the disk. The store header is\n-     * written twice, one copy in each block, to ensure it survives a crash.\n-     */\n-    static final int BLOCK_SIZE = 4 * 1024;\n-\n-    private static final int FORMAT_WRITE = 1;\n-    private static final int FORMAT_READ = 1;\n-\n-    /**\n-     * Used to mark a chunk as free, when it was detected that live bookkeeping\n-     * is incorrect.\n-     */\n-    private static final int MARKED_FREE = 10000000;\n-\n-    /**\n-     * The background thread, if any.\n-     */\n-    volatile BackgroundWriterThread backgroundWriterThread;\n-\n-    private volatile boolean reuseSpace = true;\n-\n-    private boolean closed;\n-\n-    private FileStore fileStore;\n-    private boolean fileStoreIsProvided;\n-\n-    private final int pageSplitSize;\n-\n-    /**\n-     * The page cache. The default size is 16 MB, and the average size is 2 KB.\n-     * It is split in 16 segments. The stack move distance is 2% of the expected\n-     * number of entries.\n-     */\n-    private CacheLongKeyLIRS<Page> cache;\n-\n-    /**\n-     * The page chunk references cache. The default size is 4 MB, and the\n-     * average size is 2 KB. It is split in 16 segments. The stack move distance\n-     * is 2% of the expected number of entries.\n-     */\n-    private CacheLongKeyLIRS<PageChildren> cacheChunkRef;\n-\n-    /**\n-     * The newest chunk. If nothing was stored yet, this field is not set.\n-     */\n-    private Chunk lastChunk;\n-\n-    /**\n-     * The map of chunks.\n-     */\n-    private final ConcurrentHashMap<Integer, Chunk> chunks =\n-            new ConcurrentHashMap<Integer, Chunk>();\n-\n-    /**\n-     * The map of temporarily freed storage space caused by freed pages. The key\n-     * is the unsaved version, the value is the map of chunks. The maps contains\n-     * the number of freed entries per chunk. Access is synchronized.\n-     */\n-    private final ConcurrentHashMap<Long,\n-            HashMap<Integer, Chunk>> freedPageSpace =\n-            new ConcurrentHashMap<Long, HashMap<Integer, Chunk>>();\n-\n-    /**\n-     * The metadata map. Write access to this map needs to be synchronized on\n-     * the store.\n-     */\n-    private MVMap<String, String> meta;\n-\n-    private final ConcurrentHashMap<Integer, MVMap<?, ?>> maps =\n-            new ConcurrentHashMap<Integer, MVMap<?, ?>>();\n-\n-    private HashMap<String, Object> storeHeader = New.hashMap();\n-\n-    private WriteBuffer writeBuffer;\n-\n-    private int lastMapId;\n-\n-    private int versionsToKeep = 5;\n-\n-    /**\n-     * The compression level for new pages (0 for disabled, 1 for fast, 2 for\n-     * high). Even if disabled, the store may contain (old) compressed pages.\n-     */\n-    private final int compressionLevel;\n-\n-    private Compressor compressorFast;\n-\n-    private Compressor compressorHigh;\n-\n-    private final UncaughtExceptionHandler backgroundExceptionHandler;\n-\n-    private long currentVersion;\n-\n-    /**\n-     * The version of the last stored chunk, or -1 if nothing was stored so far.\n-     */\n-    private long lastStoredVersion;\n-\n-    /**\n-     * The estimated memory used by unsaved pages. This number is not accurate,\n-     * also because it may be changed concurrently, and because temporary pages\n-     * are counted.\n-     */\n-    private int unsavedMemory;\n-    private int autoCommitMemory;\n-    private boolean saveNeeded;\n-\n-    /**\n-     * The time the store was created, in milliseconds since 1970.\n-     */\n-    private long creationTime;\n-\n-    /**\n-     * How long to retain old, persisted chunks, in milliseconds. For larger or\n-     * equal to zero, a chunk is never directly overwritten if unused, but\n-     * instead, the unused field is set. If smaller zero, chunks are directly\n-     * overwritten if unused.\n-     */\n-    private int retentionTime;\n-\n-    private long lastCommitTime;\n-\n-    /**\n-     * The earliest chunk to retain, if any.\n-     */\n-    private Chunk retainChunk;\n-\n-    /**\n-     * The version of the current store operation (if any).\n-     */\n-    private volatile long currentStoreVersion = -1;\n+  /**\n+   * Whether assertions are enabled.\n+   */\n+  public static final boolean ASSERT = false;\n \n-    private Thread currentStoreThread;\n+  /**\n+   * The block size (physical sector size) of the disk. The store header is\n+   * written twice, one copy in each block, to ensure it survives a crash.\n+   */\n+  static final int BLOCK_SIZE = 4 * 1024;\n+\n+  private static final int FORMAT_WRITE = 1;\n+  private static final int FORMAT_READ = 1;\n \n-    private volatile boolean metaChanged;\n+  /**\n+   * Used to mark a chunk as free, when it was detected that live bookkeeping\n+   * is incorrect.\n+   */\n+  private static final int MARKED_FREE = 10000000;\n+\n+  /**\n+   * The background thread, if any.\n+   */\n+  volatile BackgroundWriterThread backgroundWriterThread;\n+\n+  private volatile boolean reuseSpace = true;\n+\n+  private boolean closed;\n+\n+  private FileStore fileStore;\n+  private boolean fileStoreIsProvided;\n+\n+  private final int pageSplitSize;\n+\n+  /**\n+   * The page cache. The default size is 16 MB, and the average size is 2 KB.\n+   * It is split in 16 segments. The stack move distance is 2% of the expected\n+   * number of entries.\n+   */\n+  private CacheLongKeyLIRS<Page> cache;\n+\n+  /**\n+   * The page chunk references cache. The default size is 4 MB, and the\n+   * average size is 2 KB. It is split in 16 segments. The stack move distance\n+   * is 2% of the expected number of entries.\n+   */\n+  private CacheLongKeyLIRS<PageChildren> cacheChunkRef;\n+\n+  /**\n+   * The newest chunk. If nothing was stored yet, this field is not set.\n+   */\n+  private Chunk lastChunk;\n \n-    /**\n-     * The delay in milliseconds to automatically commit and write changes.\n-     */\n-    private int autoCommitDelay;\n+  /**\n+   * The map of chunks.\n+   */\n+  private final ConcurrentHashMap<Integer, Chunk> chunks =\n+      new ConcurrentHashMap<Integer, Chunk>();\n \n-    private int autoCompactFillRate;\n-    private long autoCompactLastFileOpCount;\n+  /**\n+   * The map of temporarily freed storage space caused by freed pages. The key\n+   * is the unsaved version, the value is the map of chunks. The maps contains\n+   * the number of freed entries per chunk. Access is synchronized.\n+   */\n+  private final ConcurrentHashMap<Long,\n+      HashMap<Integer, Chunk>> freedPageSpace =\n+      new ConcurrentHashMap<Long, HashMap<Integer, Chunk>>();\n+\n+  /**\n+   * The metadata map. Write access to this map needs to be synchronized on\n+   * the store.\n+   */\n+  private MVMap<String, String> meta;\n+\n+  private final ConcurrentHashMap<Integer, MVMap<?, ?>> maps =\n+      new ConcurrentHashMap<Integer, MVMap<?, ?>>();\n+\n+  private HashMap<String, Object> storeHeader = New.hashMap();\n+\n+  private WriteBuffer writeBuffer;\n+\n+  private int lastMapId;\n+\n+  private int versionsToKeep = 5;\n+\n+  /**\n+   * The compression level for new pages (0 for disabled, 1 for fast, 2 for\n+   * high). Even if disabled, the store may contain (old) compressed pages.\n+   */\n+  private final int compressionLevel;\n+\n+  private Compressor compressorFast;\n+\n+  private Compressor compressorHigh;\n+\n+  private final UncaughtExceptionHandler backgroundExceptionHandler;\n+\n+  private long currentVersion;\n+\n+  /**\n+   * The version of the last stored chunk, or -1 if nothing was stored so far.\n+   */\n+  private long lastStoredVersion;\n+\n+  /**\n+   * The estimated memory used by unsaved pages. This number is not accurate,\n+   * also because it may be changed concurrently, and because temporary pages\n+   * are counted.\n+   */\n+  private int unsavedMemory;\n+  private int autoCommitMemory;\n+  private boolean saveNeeded;\n+\n+  /**\n+   * The time the store was created, in milliseconds since 1970.\n+   */\n+  private long creationTime;\n+\n+  /**\n+   * How long to retain old, persisted chunks, in milliseconds. For larger or\n+   * equal to zero, a chunk is never directly overwritten if unused, but\n+   * instead, the unused field is set. If smaller zero, chunks are directly\n+   * overwritten if unused.\n+   */\n+  private int retentionTime;\n+\n+  private long lastCommitTime;\n+\n+  /**\n+   * The earliest chunk to retain, if any.\n+   */\n+  private Chunk retainChunk;\n+\n+  /**\n+   * The version of the current store operation (if any).\n+   */\n+  private volatile long currentStoreVersion = -1;\n+\n+  private Thread currentStoreThread;\n+\n+  private volatile boolean metaChanged;\n+\n+  /**\n+   * The delay in milliseconds to automatically commit and write changes.\n+   */\n+  private int autoCommitDelay;\n+\n+  private int autoCompactFillRate;\n+  private long autoCompactLastFileOpCount;\n+\n+  private Object compactSync = new Object();\n+\n+  private IllegalStateException panicException;\n+\n+  private long lastTimeAbsolute;\n+\n+  private long lastFreeUnusedChunks;\n+\n+  /**\n+   * Create and open the store.\n+   *\n+   * @param config the configuration to use\n+   * @throws IllegalStateException    if the file is corrupt, or an exception\n+   *                                  occurred while opening\n+   * @throws IllegalArgumentException if the directory does not exist\n+   */\n+  MVStore(HashMap<String, Object> config) {\n+    Object o = config.get(\"compress\");\n+    this.compressionLevel = o == null ? 0 : (Integer) o;\n+    String fileName = (String) config.get(\"fileName\");\n+    fileStore = (FileStore) config.get(\"fileStore\");\n+    fileStoreIsProvided = fileStore != null;\n+    if (fileStore == null && fileName != null) {\n+      fileStore = new FileStore();\n+    }\n+    o = config.get(\"pageSplitSize\");\n+    pageSplitSize = o != null ? (Integer) o :\n+        fileStore == null ? 48 :\n+            16 * 1024;\n+    o = config.get(\"backgroundExceptionHandler\");\n+    this.backgroundExceptionHandler = (UncaughtExceptionHandler) o;\n+    meta = new MVMap<String, String>(StringDataType.INSTANCE,\n+        StringDataType.INSTANCE);\n+    HashMap<String, Object> c = New.hashMap();\n+    c.put(\"id\", 0);\n+    c.put(\"createVersion\", currentVersion);\n+    meta.init(this, c);\n+    if (fileStore == null) {\n+      cache = null;\n+      cacheChunkRef = null;\n+      return;\n+    }\n+    retentionTime = fileStore.getDefaultRetentionTime();\n+    boolean readOnly = config.containsKey(\"readOnly\");\n+    o = config.get(\"cacheSize\");\n+    int mb = o == null ? 16 : (Integer) o;\n+    if (mb > 0) {\n+      CacheLongKeyLIRS.Config cc = new CacheLongKeyLIRS.Config();\n+      cc.maxMemory = mb * 1024L * 1024L;\n+      o = config.get(\"cacheConcurrency\");\n+      if (o != null) {\n+        cc.segmentCount = (Integer) o;\n+      }\n+      cache = new CacheLongKeyLIRS<Page>(cc);\n+      cc.maxMemory /= 4;\n+      cacheChunkRef = new CacheLongKeyLIRS<PageChildren>(cc);\n+    }\n+    o = config.get(\"autoCommitBufferSize\");\n+    int kb = o == null ? 1024 : (Integer) o;\n+    // 19 KB memory is about 1 KB storage\n+    autoCommitMemory = kb * 1024 * 19;\n+\n+    o = config.get(\"autoCompactFillRate\");\n+    autoCompactFillRate = o == null ? 50 : (Integer) o;\n+\n+    char[] encryptionKey = (char[]) config.get(\"encryptionKey\");\n+    try {\n+      if (!fileStoreIsProvided) {\n+        fileStore.open(fileName, readOnly, encryptionKey);\n+      }\n+      if (fileStore.size() == 0) {\n+        creationTime = getTimeAbsolute();\n+        lastCommitTime = creationTime;\n+        storeHeader.put(\"H\", 2);\n+        storeHeader.put(\"blockSize\", BLOCK_SIZE);\n+        storeHeader.put(\"format\", FORMAT_WRITE);\n+        storeHeader.put(\"created\", creationTime);\n+        writeStoreHeader();\n+      } else {\n+        readStoreHeader();\n+      }\n+    } catch (IllegalStateException e) {\n+      panic(e);\n+    } finally {\n+      if (encryptionKey != null) {\n+        Arrays.fill(encryptionKey, (char) 0);\n+      }\n+    }\n+    lastCommitTime = getTimeSinceCreation();\n+\n+    // setAutoCommitDelay starts the thread, but only if\n+    // the parameter is different from the old value\n+    o = config.get(\"autoCommitDelay\");\n+    int delay = o == null ? 1000 : (Integer) o;\n+    setAutoCommitDelay(delay);\n+  }\n+\n+  private void panic(IllegalStateException e) {\n+    if (backgroundExceptionHandler != null) {\n+      backgroundExceptionHandler.uncaughtException(null, e);\n+    }\n+    panicException = e;\n+    closeImmediately();\n+    throw e;\n+  }\n+\n+  /**\n+   * Open a store in exclusive mode. For a file-based store, the parent\n+   * directory must already exist.\n+   *\n+   * @param fileName the file name (null for in-memory)\n+   * @return the store\n+   */\n+  public static MVStore open(String fileName) {\n+    HashMap<String, Object> config = New.hashMap();\n+    config.put(\"fileName\", fileName);\n+    return new MVStore(config);\n+  }\n+\n+  /**\n+   * Open an old, stored version of a map.\n+   *\n+   * @param version  the version\n+   * @param mapId    the map id\n+   * @param template the template map\n+   * @return the read-only map\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  <T extends MVMap<?, ?>> T openMapVersion(long version, int mapId,\n+                                           MVMap<?, ?> template) {\n+    MVMap<String, String> oldMeta = getMetaMap(version);\n+    long rootPos = getRootPos(oldMeta, mapId);\n+    MVMap<?, ?> m = template.openReadOnly();\n+    m.setRootPos(rootPos, version);\n+    return (T) m;\n+  }\n+\n+  /**\n+   * Open a map with the default settings. The map is automatically create if\n+   * it does not yet exist. If a map with this name is already open, this map\n+   * is returned.\n+   *\n+   * @param <K>  the key type\n+   * @param <V>  the value type\n+   * @param name the name of the map\n+   * @return the map\n+   */\n+  public <K, V> MVMap<K, V> openMap(String name) {\n+    return openMap(name, new MVMap.Builder<K, V>());\n+  }\n+\n+  /**\n+   * Open a map with the given builder. The map is automatically create if it\n+   * does not yet exist. If a map with this name is already open, this map is\n+   * returned.\n+   *\n+   * @param <K>     the key type\n+   * @param <V>     the value type\n+   * @param name    the name of the map\n+   * @param builder the map builder\n+   * @return the map\n+   */\n+  public synchronized <M extends MVMap<K, V>, K, V> M openMap(\n+      String name, MVMap.MapBuilder<M, K, V> builder) {\n+    checkOpen();\n+    String x = meta.get(\"name.\" + name);\n+    int id;\n+    long root;\n+    HashMap<String, Object> c;\n+    M map;\n+    if (x != null) {\n+      id = DataUtils.parseHexInt(x);\n+      @SuppressWarnings(\"unchecked\")\n+      M old = (M) maps.get(id);\n+      if (old != null) {\n+        return old;\n+      }\n+      map = builder.create();\n+      String config = meta.get(MVMap.getMapKey(id));\n+      c = New.hashMap();\n+      c.putAll(DataUtils.parseMap(config));\n+      c.put(\"id\", id);\n+      map.init(this, c);\n+      root = getRootPos(meta, id);\n+    } else {\n+      c = New.hashMap();\n+      id = ++lastMapId;\n+      c.put(\"id\", id);\n+      c.put(\"createVersion\", currentVersion);\n+      map = builder.create();\n+      map.init(this, c);\n+      markMetaChanged();\n+      x = Integer.toHexString(id);\n+      meta.put(MVMap.getMapKey(id), map.asString(name));\n+      meta.put(\"name.\" + name, x);\n+      root = 0;\n+    }\n+    map.setRootPos(root, -1);\n+    maps.put(id, map);\n+    return map;\n+  }\n+\n+  /**\n+   * Get the set of all map names.\n+   *\n+   * @return the set of names\n+   */\n+  public synchronized Set<String> getMapNames() {\n+    HashSet<String> set = New.hashSet();\n+    checkOpen();\n+    for (Iterator<String> it = meta.keyIterator(\"name.\"); it.hasNext(); ) {\n+      String x = it.next();\n+      if (!x.startsWith(\"name.\")) {\n+        break;\n+      }\n+      set.add(x.substring(\"name.\".length()));\n+    }\n+    return set;\n+  }\n+\n+  /**\n+   * Get the metadata map. This data is for informational purposes only. The\n+   * data is subject to change in future versions.\n+   * <p>\n+   * The data in this map should not be modified (changing system data may\n+   * corrupt the store). If modifications are needed, they need be\n+   * synchronized on the store.\n+   * <p>\n+   * The metadata map contains the following entries:\n+   * <pre>\n+   * chunk.{chunkId} = {chunk metadata}\n+   * name.{name} = {mapId}\n+   * map.{mapId} = {map metadata}\n+   * root.{mapId} = {root position}\n+   * setting.storeVersion = {version}\n+   * </pre>\n+   *\n+   * @return the metadata map\n+   */\n+  public MVMap<String, String> getMetaMap() {\n+    checkOpen();\n+    return meta;\n+  }\n+\n+  private MVMap<String, String> getMetaMap(long version) {\n+    Chunk c = getChunkForVersion(version);\n+    DataUtils.checkArgument(c != null, \"Unknown version {0}\", version);\n+    c = readChunkHeader(c.block);\n+    MVMap<String, String> oldMeta = meta.openReadOnly();\n+    oldMeta.setRootPos(c.metaRootPos, version);\n+    return oldMeta;\n+  }\n+\n+  private Chunk getChunkForVersion(long version) {\n+    Chunk newest = null;\n+    for (Chunk c : chunks.values()) {\n+      if (c.version <= version) {\n+        if (newest == null || c.id > newest.id) {\n+          newest = c;\n+        }\n+      }\n+    }\n+    return newest;\n+  }\n+\n+  /**\n+   * Check whether a given map exists.\n+   *\n+   * @param name the map name\n+   * @return true if it exists\n+   */\n+  public boolean hasMap(String name) {\n+    return meta.containsKey(\"name.\" + name);\n+  }\n+\n+  private void markMetaChanged() {\n+    // changes in the metadata alone are usually not detected, as the meta\n+    // map is changed after storing\n+    metaChanged = true;\n+  }\n+\n+  private synchronized void readStoreHeader() {\n+    Chunk newest = null;\n+    boolean validStoreHeader = false;\n+    // find out which chunk and version are the newest\n+    // read the first two blocks\n+    ByteBuffer fileHeaderBlocks = fileStore.readFully(0, 2 * BLOCK_SIZE);\n+    byte[] buff = new byte[BLOCK_SIZE];\n+    for (int i = 0; i <= BLOCK_SIZE; i += BLOCK_SIZE) {\n+      fileHeaderBlocks.get(buff);\n+      // the following can fail for various reasons\n+      try {\n+        String s = new String(buff, 0, BLOCK_SIZE,\n+            DataUtils.LATIN).trim();\n+        HashMap<String, String> m = DataUtils.parseMap(s);\n+        int blockSize = DataUtils.readHexInt(\n+            m, \"blockSize\", BLOCK_SIZE);\n+        if (blockSize != BLOCK_SIZE) {\n+          throw DataUtils.newIllegalStateException(\n+              DataUtils.ERROR_UNSUPPORTED_FORMAT,\n+              \"Block size {0} is currently not supported\",\n+              blockSize);\n+        }\n+        int check = DataUtils.readHexInt(m, \"fletcher\", 0);\n+        m.remove(\"fletcher\");\n+        s = s.substring(0, s.lastIndexOf(\"fletcher\") - 1);\n+        byte[] bytes = s.getBytes(DataUtils.LATIN);\n+        int checksum = DataUtils.getFletcher32(bytes,\n+            bytes.length);\n+        if (check != checksum) {\n+          continue;\n+        }\n+        long version = DataUtils.readHexLong(m, \"version\", 0);\n+        if (newest == null || version > newest.version) {\n+          validStoreHeader = true;\n+          storeHeader.putAll(m);\n+          creationTime = DataUtils.readHexLong(m, \"created\", 0);\n+          int chunkId = DataUtils.readHexInt(m, \"chunk\", 0);\n+          long block = DataUtils.readHexLong(m, \"block\", 0);\n+          Chunk test = readChunkHeaderAndFooter(block);\n+          if (test != null && test.id == chunkId) {\n+            newest = test;\n+          }\n+        }\n+      } catch (Exception e) {\n+        continue;\n+      }\n+    }\n+    if (!validStoreHeader) {\n+      throw DataUtils.newIllegalStateException(\n+          DataUtils.ERROR_FILE_CORRUPT,\n+          \"Store header is corrupt: {0}\", fileStore);\n+    }\n+    long format = DataUtils.readHexLong(storeHeader, \"format\", 1);\n+    if (format > FORMAT_WRITE && !fileStore.isReadOnly()) {\n+      throw DataUtils.newIllegalStateException(\n+          DataUtils.ERROR_UNSUPPORTED_FORMAT,\n+          \"The write format {0} is larger \" +\n+              \"than the supported format {1}, \" +\n+              \"and the file was not opened in read-only mode\",\n+          format, FORMAT_WRITE);\n+    }\n+    format = DataUtils.readHexLong(storeHeader, \"formatRead\", format);\n+    if (format > FORMAT_READ) {\n+      throw DataUtils.newIllegalStateException(\n+          DataUtils.ERROR_UNSUPPORTED_FORMAT,\n+          \"The read format {0} is larger \" +\n+              \"than the supported format {1}\",\n+          format, FORMAT_READ);\n+    }\n+    lastStoredVersion = -1;\n+    chunks.clear();\n+    long now = System.currentTimeMillis();\n+    // calculate the year (doesn't have to be exact;\n+    // we assume 365.25 days per year, * 4 = 1461)\n+    int year = 1970 + (int) (now / (1000L * 60 * 60 * 6 * 1461));\n+    if (year < 2014) {\n+      // if the year is before 2014,\n+      // we assume the system doesn't have a real-time clock,\n+      // and we set the creationTime to the past, so that\n+      // existing chunks are overwritten\n+      creationTime = now - fileStore.getDefaultRetentionTime();\n+    } else if (now < creationTime) {\n+      // the system time was set to the past:\n+      // we change the creation time\n+      creationTime = now;\n+      storeHeader.put(\"created\", creationTime);\n+    }\n+    Chunk test = readChunkFooter(fileStore.size());\n+    if (test != null) {\n+      test = readChunkHeaderAndFooter(test.block);\n+      if (test != null) {\n+        if (newest == null || test.version > newest.version) {\n+          newest = test;\n+        }\n+      }\n+    }\n+    if (newest == null) {\n+      // no chunk\n+      return;\n+    }\n+    // read the chunk header and footer,\n+    // and follow the chain of next chunks\n+    while (true) {\n+      if (newest.next == 0 ||\n+          newest.next >= fileStore.size() / BLOCK_SIZE) {\n+        // no (valid) next\n+        break;\n+      }\n+      test = readChunkHeaderAndFooter(newest.next);\n+      if (test == null || test.id <= newest.id) {\n+        break;\n+      }\n+      newest = test;\n+    }\n+    setLastChunk(newest);\n+    loadChunkMeta();\n+    // read all chunk headers and footers within the retention time,\n+    // to detect unwritten data after a power failure\n+    verifyLastChunks();\n+    // build the free space list\n+    for (Chunk c : chunks.values()) {\n+      if (c.pageCountLive == 0) {\n+        // remove this chunk in the next save operation\n+        registerFreePage(currentVersion, c.id, 0, 0);\n+      }\n+      long start = c.block * BLOCK_SIZE;\n+      int length = c.len * BLOCK_SIZE;\n+      fileStore.markUsed(start, length);\n+    }\n+  }\n+\n+  private void loadChunkMeta() {\n+    // load the chunk metadata: we can load in any order,\n+    // because loading chunk metadata might recursively load another chunk\n+    for (Iterator<String> it = meta.keyIterator(\"chunk.\"); it.hasNext(); ) {\n+      String s = it.next();\n+      if (!s.startsWith(\"chunk.\")) {\n+        break;\n+      }\n+      s = meta.get(s);\n+      Chunk c = Chunk.fromString(s);\n+      if (!chunks.containsKey(c.id)) {\n+        if (c.block == Long.MAX_VALUE) {\n+          throw DataUtils.newIllegalStateException(\n+              DataUtils.ERROR_FILE_CORRUPT,\n+              \"Chunk {0} is invalid\", c.id);\n+        }\n+        chunks.put(c.id, c);\n+      }\n+    }\n+  }\n+\n+  private void setLastChunk(Chunk last) {\n+    lastChunk = last;\n+    if (last == null) {\n+      // no valid chunk\n+      lastMapId = 0;\n+      currentVersion = 0;\n+      meta.setRootPos(0, -1);\n+    } else {\n+      lastMapId = last.mapId;\n+      currentVersion = last.version;\n+      chunks.put(last.id, last);\n+      meta.setRootPos(last.metaRootPos, -1);\n+    }\n+    setWriteVersion(currentVersion);\n+  }\n+\n+  private void verifyLastChunks() {\n+    long time = getTimeSinceCreation();\n+    ArrayList<Integer> ids = new ArrayList<Integer>(chunks.keySet());\n+    Collections.sort(ids);\n+    int newestValidChunk = -1;\n+    Chunk old = null;\n+    for (Integer chunkId : ids) {\n+      Chunk c = chunks.get(chunkId);\n+      if (old != null && c.time < old.time) {\n+        // old chunk (maybe leftover from a previous crash)\n+        break;\n+      }\n+      old = c;\n+      if (c.time + retentionTime < time) {\n+        // old chunk, no need to verify\n+        newestValidChunk = c.id;\n+        continue;\n+      }\n+      Chunk test = readChunkHeaderAndFooter(c.block);\n+      if (test == null || test.id != c.id) {\n+        break;\n+      }\n+      newestValidChunk = chunkId;\n+    }\n+    Chunk newest = chunks.get(newestValidChunk);\n+    if (newest != lastChunk) {\n+      // to avoid re-using newer chunks later on, we could clear\n+      // the headers and footers of those, but we might not know about all\n+      // of them, so that could be incomplete - but we check that newer\n+      // chunks are written after older chunks, so we are safe\n+      rollbackTo(newest == null ? 0 : newest.version);\n+    }\n+  }\n+\n+  /**\n+   * Read a chunk header and footer, and verify the stored data is consistent.\n+   *\n+   * @param block the block\n+   * @return the chunk, or null if the header or footer don't match or are not\n+   * consistent\n+   */\n+  private Chunk readChunkHeaderAndFooter(long block) {\n+    Chunk header;\n+    try {\n+      header = readChunkHeader(block);\n+    } catch (Exception e) {\n+      // invalid chunk header: ignore, but stop\n+      return null;\n+    }\n+    if (header == null) {\n+      return null;\n+    }\n+    Chunk footer = readChunkFooter((block + header.len) * BLOCK_SIZE);\n+    if (footer == null || footer.id != header.id) {\n+      return null;\n+    }\n+    return header;\n+  }\n+\n+  /**\n+   * Try to read a chunk footer.\n+   *\n+   * @param end the end of the chunk\n+   * @return the chunk, or null if not successful\n+   */\n+  private Chunk readChunkFooter(long end) {\n+    // the following can fail for various reasons\n+    try {\n+      // read the chunk footer of the last block of the file\n+      ByteBuffer lastBlock = fileStore.readFully(\n+          end - Chunk.FOOTER_LENGTH, Chunk.FOOTER_LENGTH);\n+      byte[] buff = new byte[Chunk.FOOTER_LENGTH];\n+      lastBlock.get(buff);\n+      String s = new String(buff, DataUtils.LATIN).trim();\n+      HashMap<String, String> m = DataUtils.parseMap(s);\n+      int check = DataUtils.readHexInt(m, \"fletcher\", 0);\n+      m.remove(\"fletcher\");\n+      s = s.substring(0, s.lastIndexOf(\"fletcher\") - 1);\n+      byte[] bytes = s.getBytes(DataUtils.LATIN);\n+      int checksum = DataUtils.getFletcher32(bytes, bytes.length);\n+      if (check == checksum) {\n+        int chunk = DataUtils.readHexInt(m, \"chunk\", 0);\n+        Chunk c = new Chunk(chunk);\n+        c.version = DataUtils.readHexLong(m, \"version\", 0);\n+        c.block = DataUtils.readHexLong(m, \"block\", 0);\n+        return c;\n+      }\n+    } catch (Exception e) {\n+      // ignore\n+    }\n+    return null;\n+  }\n+\n+  private void writeStoreHeader() {\n+    StringBuilder buff = new StringBuilder();\n+    if (lastChunk != null) {\n+      storeHeader.put(\"block\", lastChunk.block);\n+      storeHeader.put(\"chunk\", lastChunk.id);\n+      storeHeader.put(\"version\", lastChunk.version);\n+    }\n+    DataUtils.appendMap(buff, storeHeader);\n+    byte[] bytes = buff.toString().getBytes(DataUtils.LATIN);\n+    int checksum = DataUtils.getFletcher32(bytes, bytes.length);\n+    DataUtils.appendMap(buff, \"fletcher\", checksum);\n+    buff.append(\"\\n\");\n+    bytes = buff.toString().getBytes(DataUtils.LATIN);\n+    ByteBuffer header = ByteBuffer.allocate(2 * BLOCK_SIZE);\n+    header.put(bytes);\n+    header.position(BLOCK_SIZE);\n+    header.put(bytes);\n+    header.rewind();\n+    write(0, header);\n+  }\n+\n+  private void write(long pos, ByteBuffer buffer) {\n+    try {\n+      fileStore.writeFully(pos, buffer);\n+    } catch (IllegalStateException e) {\n+      panic(e);\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Close the file and the store. Unsaved changes are written to disk first.\n+   */\n+  public void close() {\n+    if (closed) {\n+      return;\n+    }\n+    FileStore f = fileStore;\n+    if (f != null && !f.isReadOnly()) {\n+      stopBackgroundThread();\n+      if (hasUnsavedChanges()) {\n+        commitAndSave();\n+      }\n+    }\n+    closeStore(true);\n+  }\n+\n+  /**\n+   * Close the file and the store, without writing anything. This will stop\n+   * the background thread. This method ignores all errors.\n+   */\n+  public void closeImmediately() {\n+    try {\n+      closeStore(false);\n+    } catch (Exception e) {\n+      if (backgroundExceptionHandler != null) {\n+        backgroundExceptionHandler.uncaughtException(null, e);\n+      }\n+    }\n+  }\n+\n+  private void closeStore(boolean shrinkIfPossible) {\n+    if (closed) {\n+      return;\n+    }\n+    // can not synchronize on this yet, because\n+    // the thread also synchronized on this, which\n+    // could result in a deadlock\n+    stopBackgroundThread();\n+    closed = true;\n+    synchronized (this) {\n+      if (fileStore != null && shrinkIfPossible) {\n+        shrinkFileIfPossible(0);\n+      }\n+      // release memory early - this is important when called\n+      // because of out of memory\n+      cache = null;\n+      cacheChunkRef = null;\n+      for (MVMap<?, ?> m : New.arrayList(maps.values())) {\n+        m.close();\n+      }\n+      meta = null;\n+      chunks.clear();\n+      maps.clear();\n+      if (fileStore != null) {\n+        try {\n+          if (!fileStoreIsProvided) {\n+            fileStore.close();\n+          }\n+        } finally {\n+          fileStore = null;\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Whether the chunk at the given position is live.\n+   *\n+   * @param chunkId the chunk id\n+   * @return true if it is live\n+   */\n+  boolean isChunkLive(int chunkId) {\n+    String s = meta.get(Chunk.getMetaKey(chunkId));\n+    return s != null;\n+  }\n+\n+  /**\n+   * Get the chunk for the given position.\n+   *\n+   * @param pos the position\n+   * @return the chunk\n+   */\n+  private Chunk getChunk(long pos) {\n+    Chunk c = getChunkIfFound(pos);\n+    if (c == null) {\n+      int chunkId = DataUtils.getPageChunkId(pos);\n+      throw DataUtils.newIllegalStateException(\n+          DataUtils.ERROR_FILE_CORRUPT,\n+          \"Chunk {0} not found\", chunkId);\n+    }\n+    return c;\n+  }\n+\n+  private Chunk getChunkIfFound(long pos) {\n+    int chunkId = DataUtils.getPageChunkId(pos);\n+    Chunk c = chunks.get(chunkId);\n+    if (c == null) {\n+      checkOpen();\n+      if (!Thread.holdsLock(this)) {\n+        // it could also be unsynchronized metadata\n+        // access (if synchronization on this was forgotten)\n+        throw DataUtils.newIllegalStateException(\n+            DataUtils.ERROR_CHUNK_NOT_FOUND,\n+            \"Chunk {0} no longer exists\",\n+            chunkId);\n+      }\n+      String s = meta.get(Chunk.getMetaKey(chunkId));\n+      if (s == null) {\n+        return null;\n+      }\n+      c = Chunk.fromString(s);\n+      if (c.block == Long.MAX_VALUE) {\n+        throw DataUtils.newIllegalStateException(\n+            DataUtils.ERROR_FILE_CORRUPT,\n+            \"Chunk {0} is invalid\", chunkId);\n+      }\n+      chunks.put(c.id, c);\n+    }\n+    return c;\n+  }\n+\n+  private void setWriteVersion(long version) {\n+    for (MVMap<?, ?> map : maps.values()) {\n+      map.setWriteVersion(version);\n+    }\n+    MVMap<String, String> m = meta;\n+    if (m == null) {\n+      checkOpen();\n+    }\n+    m.setWriteVersion(version);\n+  }\n+\n+  /**\n+   * Commit the changes.\n+   * <p>\n+   * For in-memory stores, this method increments the version.\n+   * <p>\n+   * For persistent stores, it also writes changes to disk. It does nothing if\n+   * there are no unsaved changes, and returns the old version. It is not\n+   * necessary to call this method when auto-commit is enabled (the default\n+   * setting), as in this case it is automatically called from time to time or\n+   * when enough changes have accumulated. However, it may still be called to\n+   * flush all changes to disk.\n+   *\n+   * @return the new version\n+   */\n+  public synchronized long commit() {\n+    if (fileStore != null) {\n+      return commitAndSave();\n+    }\n+    long v = ++currentVersion;\n+    setWriteVersion(v);\n+    return v;\n+  }\n+\n+  /**\n+   * Commit all changes and persist them to disk. This method does nothing if\n+   * there are no unsaved changes, otherwise it increments the current version\n+   * and stores the data (for file based stores).\n+   * <p>\n+   * At most one store operation may run at any time.\n+   *\n+   * @return the new version (incremented if there were changes)\n+   */\n+  private synchronized long commitAndSave() {\n+    if (closed) {\n+      return currentVersion;\n+    }\n+    if (fileStore == null) {\n+      throw DataUtils.newIllegalStateException(\n+          DataUtils.ERROR_WRITING_FAILED,\n+          \"This is an in-memory store\");\n+    }\n+    if (currentStoreVersion >= 0) {\n+      // store is possibly called within store, if the meta map changed\n+      return currentVersion;\n+    }\n+    if (!hasUnsavedChanges()) {\n+      return currentVersion;\n+    }\n+    if (fileStore.isReadOnly()) {\n+      throw DataUtils.newIllegalStateException(\n+          DataUtils.ERROR_WRITING_FAILED, \"This store is read-only\");\n+    }\n+    try {\n+      currentStoreVersion = currentVersion;\n+      currentStoreThread = Thread.currentThread();\n+      return storeNow();\n+    } finally {\n+      // in any case reset the current store version,\n+      // to allow closing the store\n+      currentStoreVersion = -1;\n+      currentStoreThread = null;\n+    }\n+  }\n+\n+  private long storeNow() {\n+    try {\n+      return storeNowTry();\n+    } catch (IllegalStateException e) {\n+      panic(e);\n+      return -1;\n+    }\n+  }\n+\n+  private long storeNowTry() {\n+    long time = getTimeSinceCreation();\n+    int freeDelay = retentionTime / 10;\n+    if (time >= lastFreeUnusedChunks + freeDelay) {\n+      // set early in case it fails (out of memory or so)\n+      lastFreeUnusedChunks = time;\n+      freeUnusedChunks();\n+      // set it here as well, to avoid calling it often if it was slow\n+      lastFreeUnusedChunks = getTimeSinceCreation();\n+    }\n+    int currentUnsavedPageCount = unsavedMemory;\n+    long storeVersion = currentStoreVersion;\n+    long version = ++currentVersion;\n+    lastCommitTime = time;\n+    retainChunk = null;\n+\n+    // the metadata of the last chunk was not stored so far, and needs to be\n+    // set now (it's better not to update right after storing, because that\n+    // would modify the meta map again)\n+    int lastChunkId;\n+    if (lastChunk == null) {\n+      lastChunkId = 0;\n+    } else {\n+      lastChunkId = lastChunk.id;\n+      meta.put(Chunk.getMetaKey(lastChunkId), lastChunk.asString());\n+      // never go backward in time\n+      time = Math.max(lastChunk.time, time);\n+    }\n+    int newChunkId = lastChunkId;\n+    while (true) {\n+      newChunkId = (newChunkId + 1) % Chunk.MAX_ID;\n+      Chunk old = chunks.get(newChunkId);\n+      if (old == null) {\n+        break;\n+      }\n+      if (old.block == Long.MAX_VALUE) {\n+        IllegalStateException e = DataUtils.newIllegalStateException(\n+            DataUtils.ERROR_INTERNAL,\n+            \"Last block not stored, possibly due to out-of-memory\");\n+        panic(e);\n+      }\n+    }\n+    Chunk c = new Chunk(newChunkId);\n+\n+    c.pageCount = Integer.MAX_VALUE;\n+    c.pageCountLive = Integer.MAX_VALUE;\n+    c.maxLen = Long.MAX_VALUE;\n+    c.maxLenLive = Long.MAX_VALUE;\n+    c.metaRootPos = Long.MAX_VALUE;\n+    c.block = Long.MAX_VALUE;\n+    c.len = Integer.MAX_VALUE;\n+    c.time = time;\n+    c.version = version;\n+    c.mapId = lastMapId;\n+    c.next = Long.MAX_VALUE;\n+    chunks.put(c.id, c);\n+    // force a metadata update\n+    meta.put(Chunk.getMetaKey(c.id), c.asString());\n+    meta.remove(Chunk.getMetaKey(c.id));\n+    ArrayList<MVMap<?, ?>> list = New.arrayList(maps.values());\n+    ArrayList<MVMap<?, ?>> changed = New.arrayList();\n+    for (MVMap<?, ?> m : list) {\n+      m.setWriteVersion(version);\n+      long v = m.getVersion();\n+      if (m.getCreateVersion() > storeVersion) {\n+        // the map was created after storing started\n+        continue;\n+      }\n+      if (m.isVolatile()) {\n+        continue;\n+      }\n+      if (v >= 0 && v >= lastStoredVersion) {\n+        MVMap<?, ?> r = m.openVersion(storeVersion);\n+        if (r.getRoot().getPos() == 0) {\n+          changed.add(r);\n+        }\n+      }\n+    }\n+    applyFreedSpace(storeVersion);\n+    WriteBuffer buff = getWriteBuffer();\n+    // need to patch the header later\n+    c.writeChunkHeader(buff, 0);\n+    int headerLength = buff.position();\n+    c.pageCount = 0;\n+    c.pageCountLive = 0;\n+    c.maxLen = 0;\n+    c.maxLenLive = 0;\n+    for (MVMap<?, ?> m : changed) {\n+      Page p = m.getRoot();\n+      String key = MVMap.getMapRootKey(m.getId());\n+      if (p.getTotalCount() == 0) {\n+        meta.put(key, \"0\");\n+      } else {\n+        p.writeUnsavedRecursive(c, buff);\n+        long root = p.getPos();\n+        meta.put(key, Long.toHexString(root));\n+      }\n+    }\n+    meta.setWriteVersion(version);\n+\n+    Page metaRoot = meta.getRoot();\n+    metaRoot.writeUnsavedRecursive(c, buff);\n+\n+    int chunkLength = buff.position();\n+\n+    // add the store header and round to the next block\n+    int length = MathUtils.roundUpInt(chunkLength +\n+        Chunk.FOOTER_LENGTH, BLOCK_SIZE);\n+    buff.limit(length);\n+\n+    // the length of the file that is still in use\n+    // (not necessarily the end of the file)\n+    long end = getFileLengthInUse();\n+    long filePos;\n+    if (reuseSpace) {\n+      filePos = fileStore.allocate(length);\n+    } else {\n+      filePos = end;\n+    }\n+    // end is not necessarily the end of the file\n+    boolean storeAtEndOfFile = filePos + length >= fileStore.size();\n+\n+    if (!reuseSpace) {\n+      // we can not mark it earlier, because it\n+      // might have been allocated by one of the\n+      // removed chunks\n+      fileStore.markUsed(end, length);\n+    }\n+\n+    c.block = filePos / BLOCK_SIZE;\n+    c.len = length / BLOCK_SIZE;\n+    c.metaRootPos = metaRoot.getPos();\n+    // calculate and set the likely next position\n+    if (reuseSpace) {\n+      int predictBlocks = c.len;\n+      long predictedNextStart = fileStore.allocate(\n+          predictBlocks * BLOCK_SIZE);\n+      fileStore.free(predictedNextStart, predictBlocks * BLOCK_SIZE);\n+      c.next = predictedNextStart / BLOCK_SIZE;\n+    } else {\n+      // just after this chunk\n+      c.next = 0;\n+    }\n+    buff.position(0);\n+    c.writeChunkHeader(buff, headerLength);\n+    revertTemp(storeVersion);\n+\n+    buff.position(buff.limit() - Chunk.FOOTER_LENGTH);\n+    buff.put(c.getFooterBytes());\n+\n+    buff.position(0);\n+    write(filePos, buff.getBuffer());\n+    releaseWriteBuffer(buff);\n+\n+    // whether we need to write the store header\n+    boolean writeStoreHeader = false;\n+    if (!storeAtEndOfFile) {\n+      if (lastChunk == null) {\n+        writeStoreHeader = true;\n+      } else if (lastChunk.next != c.block) {\n+        // the last prediction did not matched\n+        writeStoreHeader = true;\n+      } else {\n+        long headerVersion = DataUtils.readHexLong(\n+            storeHeader, \"version\", 0);\n+        if (lastChunk.version - headerVersion > 20) {\n+          // we write after at least 20 entries\n+          writeStoreHeader = true;\n+        } else {\n+          int chunkId = DataUtils.readHexInt(storeHeader, \"chunk\", 0);\n+          while (true) {\n+            Chunk old = chunks.get(chunkId);\n+            if (old == null) {\n+              // one of the chunks in between\n+              // was removed\n+              writeStoreHeader = true;\n+              break;\n+            }\n+            if (chunkId == lastChunk.id) {\n+              break;\n+            }\n+            chunkId++;\n+          }\n+        }\n+      }\n+    }\n \n-    private Object compactSync = new Object();\n+    lastChunk = c;\n+    if (writeStoreHeader) {\n+      writeStoreHeader();\n+    }\n+    if (!storeAtEndOfFile) {\n+      // may only shrink after the store header was written\n+      shrinkFileIfPossible(1);\n+    }\n+    for (MVMap<?, ?> m : changed) {\n+      Page p = m.getRoot();\n+      if (p.getTotalCount() > 0) {\n+        p.writeEnd();\n+      }\n+    }\n+    metaRoot.writeEnd();\n \n-    private IllegalStateException panicException;\n+    // some pages might have been changed in the meantime (in the newest\n+    // version)\n+    unsavedMemory = Math.max(0, unsavedMemory\n+        - currentUnsavedPageCount);\n \n-    private long lastTimeAbsolute;\n+    metaChanged = false;\n+    lastStoredVersion = storeVersion;\n \n-    private long lastFreeUnusedChunks;\n+    return version;\n+  }\n \n-    /**\n-     * Create and open the store.\n-     *\n-     * @param config the configuration to use\n-     * @throws IllegalStateException if the file is corrupt, or an exception\n-     *             occurred while opening\n-     * @throws IllegalArgumentException if the directory does not exist\n-     */\n-    MVStore(HashMap<String, Object> config) {\n-        Object o = config.get(\"compress\");\n-        this.compressionLevel = o == null ? 0 : (Integer) o;\n-        String fileName = (String) config.get(\"fileName\");\n-        fileStore = (FileStore) config.get(\"fileStore\");\n-        fileStoreIsProvided = fileStore != null;\n-        if(fileStore == null && fileName != null) {\n-            fileStore = new FileStore();\n+  private synchronized void freeUnusedChunks() {\n+    if (lastChunk == null || !reuseSpace) {\n+      return;\n+    }\n+    Set<Integer> referenced = collectReferencedChunks();\n+    ArrayList<Chunk> free = New.arrayList();\n+    long time = getTimeSinceCreation();\n+    for (Chunk c : chunks.values()) {\n+      if (!referenced.contains(c.id)) {\n+        free.add(c);\n+      }\n+    }\n+    for (Chunk c : free) {\n+      if (canOverwriteChunk(c, time)) {\n+        chunks.remove(c.id);\n+        markMetaChanged();\n+        meta.remove(Chunk.getMetaKey(c.id));\n+        long start = c.block * BLOCK_SIZE;\n+        int length = c.len * BLOCK_SIZE;\n+        fileStore.free(start, length);\n+      } else {\n+        if (c.unused == 0) {\n+          c.unused = time;\n+          meta.put(Chunk.getMetaKey(c.id), c.asString());\n+          markMetaChanged();\n+        }\n+      }\n+    }\n+  }\n+\n+  private Set<Integer> collectReferencedChunks() {\n+    long testVersion = lastChunk.version;\n+    DataUtils.checkArgument(testVersion > 0, \"Collect references on version 0\");\n+    long readCount = getFileStore().readCount;\n+    Set<Integer> referenced = New.hashSet();\n+    for (Cursor<String, String> c = meta.cursor(\"root.\"); c.hasNext(); ) {\n+      String key = c.next();\n+      if (!key.startsWith(\"root.\")) {\n+        break;\n+      }\n+      long pos = DataUtils.parseHexLong(c.getValue());\n+      if (pos == 0) {\n+        continue;\n+      }\n+      int mapId = DataUtils.parseHexInt(key.substring(\"root.\".length()));\n+      collectReferencedChunks(referenced, mapId, pos, 0);\n+    }\n+    long pos = lastChunk.metaRootPos;\n+    collectReferencedChunks(referenced, 0, pos, 0);\n+    readCount = fileStore.readCount - readCount;\n+    return referenced;\n+  }\n+\n+  private void collectReferencedChunks(Set<Integer> targetChunkSet,\n+                                       int mapId, long pos, int level) {\n+    int c = DataUtils.getPageChunkId(pos);\n+    targetChunkSet.add(c);\n+    if (DataUtils.getPageType(pos) == DataUtils.PAGE_TYPE_LEAF) {\n+      return;\n+    }\n+    PageChildren refs = readPageChunkReferences(mapId, pos, -1);\n+    if (!refs.chunkList) {\n+      Set<Integer> target = New.hashSet();\n+      for (int i = 0; i < refs.children.length; i++) {\n+        long p = refs.children[i];\n+        collectReferencedChunks(target, mapId, p, level + 1);\n+      }\n+      // we don't need a reference to this chunk\n+      target.remove(c);\n+      long[] children = new long[target.size()];\n+      int i = 0;\n+      for (Integer p : target) {\n+        children[i++] = DataUtils.getPagePos(p, 0, 0,\n+            DataUtils.PAGE_TYPE_LEAF);\n+      }\n+      refs.children = children;\n+      refs.chunkList = true;\n+      if (cacheChunkRef != null) {\n+        cacheChunkRef.put(refs.pos, refs, refs.getMemory());\n+      }\n+    }\n+    for (long p : refs.children) {\n+      targetChunkSet.add(DataUtils.getPageChunkId(p));\n+    }\n+  }\n+\n+  private PageChildren readPageChunkReferences(int mapId, long pos, int parentChunk) {\n+    if (DataUtils.getPageType(pos) == DataUtils.PAGE_TYPE_LEAF) {\n+      return null;\n+    }\n+    PageChildren r;\n+    if (cacheChunkRef != null) {\n+      r = cacheChunkRef.get(pos);\n+    } else {\n+      r = null;\n+    }\n+    if (r == null) {\n+      // if possible, create it from the cached page\n+      if (cache != null) {\n+        Page p = cache.get(pos);\n+        if (p != null) {\n+          r = new PageChildren(p);\n+        }\n+      }\n+      if (r == null) {\n+        // page was not cached: read the data\n+        Chunk c = getChunk(pos);\n+        long filePos = c.block * BLOCK_SIZE;\n+        filePos += DataUtils.getPageOffset(pos);\n+        if (filePos < 0) {\n+          throw DataUtils.newIllegalStateException(\n+              DataUtils.ERROR_FILE_CORRUPT,\n+              \"Negative position {0}; p={1}, c={2}\", filePos, pos, c.toString());\n+        }\n+        long maxPos = (c.block + c.len) * BLOCK_SIZE;\n+        r = PageChildren.read(fileStore, pos, mapId, filePos, maxPos);\n+      }\n+      r.removeDuplicateChunkReferences();\n+      if (cacheChunkRef != null) {\n+        cacheChunkRef.put(pos, r, r.getMemory());\n+      }\n+    }\n+    if (r.children.length == 0) {\n+      int chunk = DataUtils.getPageChunkId(pos);\n+      if (chunk == parentChunk) {\n+        return null;\n+      }\n+    }\n+    return r;\n+  }\n+\n+  /**\n+   * Get a buffer for writing. This caller must synchronize on the store\n+   * before calling the method and until after using the buffer.\n+   *\n+   * @return the buffer\n+   */\n+  private WriteBuffer getWriteBuffer() {\n+    WriteBuffer buff;\n+    if (writeBuffer != null) {\n+      buff = writeBuffer;\n+      buff.clear();\n+    } else {\n+      buff = new WriteBuffer();\n+    }\n+    return buff;\n+  }\n+\n+  /**\n+   * Release a buffer for writing. This caller must synchronize on the store\n+   * before calling the method and until after using the buffer.\n+   *\n+   * @param buff the buffer than can be re-used\n+   */\n+  private void releaseWriteBuffer(WriteBuffer buff) {\n+    if (buff.capacity() <= 4 * 1024 * 1024) {\n+      writeBuffer = buff;\n+    }\n+  }\n+\n+  private boolean canOverwriteChunk(Chunk c, long time) {\n+    if (retentionTime >= 0) {\n+      if (c.time + retentionTime > time) {\n+        return false;\n+      }\n+      if (c.unused == 0 || c.unused + retentionTime / 2 > time) {\n+        return false;\n+      }\n+    }\n+    Chunk r = retainChunk;\n+    if (r != null && c.version > r.version) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  private long getTimeSinceCreation() {\n+    return Math.max(0, getTimeAbsolute() - creationTime);\n+  }\n+\n+  private long getTimeAbsolute() {\n+    long now = System.currentTimeMillis();\n+    if (lastTimeAbsolute != 0 && now < lastTimeAbsolute) {\n+      // time seems to have run backwards - this can happen\n+      // when the system time is adjusted, for example\n+      // on a leap second\n+      now = lastTimeAbsolute;\n+    } else {\n+      lastTimeAbsolute = now;\n+    }\n+    return now;\n+  }\n+\n+  /**\n+   * Apply the freed space to the chunk metadata. The metadata is updated, but\n+   * completely free chunks are not removed from the set of chunks, and the\n+   * disk space is not yet marked as free.\n+   *\n+   * @param storeVersion apply up to the given version\n+   */\n+  private void applyFreedSpace(long storeVersion) {\n+    while (true) {\n+      ArrayList<Chunk> modified = New.arrayList();\n+      Iterator<Entry<Long, HashMap<Integer, Chunk>>> it;\n+      it = freedPageSpace.entrySet().iterator();\n+      while (it.hasNext()) {\n+        Entry<Long, HashMap<Integer, Chunk>> e = it.next();\n+        long v = e.getKey();\n+        if (v > storeVersion) {\n+          continue;\n+        }\n+        HashMap<Integer, Chunk> freed = e.getValue();\n+        for (Chunk f : freed.values()) {\n+          Chunk c = chunks.get(f.id);\n+          if (c == null) {\n+            // already removed\n+            continue;\n+          }\n+          // no need to synchronize, as old entries\n+          // are not concurrently modified\n+          c.maxLenLive += f.maxLenLive;\n+          c.pageCountLive += f.pageCountLive;\n+          if (c.pageCountLive < 0 && c.pageCountLive > -MARKED_FREE) {\n+            // can happen after a rollback\n+            c.pageCountLive = 0;\n+          }\n+          if (c.maxLenLive < 0 && c.maxLenLive > -MARKED_FREE) {\n+            // can happen after a rollback\n+            c.maxLenLive = 0;\n+          }\n+          modified.add(c);\n+        }\n+        it.remove();\n+      }\n+      for (Chunk c : modified) {\n+        meta.put(Chunk.getMetaKey(c.id), c.asString());\n+      }\n+      if (modified.size() == 0) {\n+        break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Shrink the file if possible, and if at least a given percentage can be\n+   * saved.\n+   *\n+   * @param minPercent the minimum percentage to save\n+   */\n+  private void shrinkFileIfPossible(int minPercent) {\n+    if (fileStore.isReadOnly()) {\n+      return;\n+    }\n+    long end = getFileLengthInUse();\n+    long fileSize = fileStore.size();\n+    if (end >= fileSize) {\n+      return;\n+    }\n+    if (minPercent > 0 && fileSize - end < BLOCK_SIZE) {\n+      return;\n+    }\n+    int savedPercent = (int) (100 - (end * 100 / fileSize));\n+    if (savedPercent < minPercent) {\n+      return;\n+    }\n+    if (!closed) {\n+      sync();\n+    }\n+    fileStore.truncate(end);\n+  }\n+\n+  /**\n+   * Get the position of the last used byte.\n+   *\n+   * @return the position\n+   */\n+  private long getFileLengthInUse() {\n+    long size = 2 * BLOCK_SIZE;\n+    for (Chunk c : chunks.values()) {\n+      if (c.len != Integer.MAX_VALUE) {\n+        long x = (c.block + c.len) * BLOCK_SIZE;\n+        size = Math.max(size, x);\n+      }\n+    }\n+    return size;\n+  }\n+\n+  /**\n+   * Check whether there are any unsaved changes.\n+   *\n+   * @return if there are any changes\n+   */\n+  public boolean hasUnsavedChanges() {\n+    checkOpen();\n+    if (metaChanged) {\n+      return true;\n+    }\n+    for (MVMap<?, ?> m : maps.values()) {\n+      if (!m.isClosed()) {\n+        long v = m.getVersion();\n+        if (v >= 0 && v > lastStoredVersion) {\n+          return true;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n+\n+  private Chunk readChunkHeader(long block) {\n+    long p = block * BLOCK_SIZE;\n+    ByteBuffer buff = fileStore.readFully(p, Chunk.MAX_HEADER_LENGTH);\n+    return Chunk.readChunkHeader(buff, p);\n+  }\n+\n+  /**\n+   * Compact the store by moving all live pages to new chunks.\n+   *\n+   * @return if anything was written\n+   */\n+  public synchronized boolean compactRewriteFully() {\n+    checkOpen();\n+    if (lastChunk == null) {\n+      // nothing to do\n+      return false;\n+    }\n+    for (MVMap<?, ?> m : maps.values()) {\n+      @SuppressWarnings(\"unchecked\")\n+      MVMap<Object, Object> map = (MVMap<Object, Object>) m;\n+      Cursor<Object, Object> cursor = map.cursor(null);\n+      Page lastPage = null;\n+      while (cursor.hasNext()) {\n+        cursor.next();\n+        Page p = cursor.getPage();\n+        if (p == lastPage) {\n+          continue;\n+        }\n+        Object k = p.getKey(0);\n+        Object v = p.getValue(0);\n+        map.put(k, v);\n+        lastPage = p;\n+      }\n+    }\n+    commitAndSave();\n+    return true;\n+  }\n+\n+  /**\n+   * Compact by moving all chunks next to each other.\n+   *\n+   * @return if anything was written\n+   */\n+  public synchronized boolean compactMoveChunks() {\n+    return compactMoveChunks(100, Long.MAX_VALUE);\n+  }\n+\n+  /**\n+   * Compact the store by moving all chunks next to each other, if there is\n+   * free space between chunks. This might temporarily increase the file size.\n+   * Chunks are overwritten irrespective of the current retention time. Before\n+   * overwriting chunks and before resizing the file, syncFile() is called.\n+   *\n+   * @param targetFillRate do nothing if the file store fill rate is higher\n+   *                       than this\n+   * @param moveSize       the number of bytes to move\n+   * @return if anything was written\n+   */\n+  public synchronized boolean compactMoveChunks(int targetFillRate, long moveSize) {\n+    checkOpen();\n+    if (lastChunk == null || !reuseSpace) {\n+      // nothing to do\n+      return false;\n+    }\n+    int oldRetentionTime = retentionTime;\n+    boolean oldReuse = reuseSpace;\n+    try {\n+      retentionTime = -1;\n+      freeUnusedChunks();\n+      if (fileStore.getFillRate() > targetFillRate) {\n+        return false;\n+      }\n+      long start = fileStore.getFirstFree() / BLOCK_SIZE;\n+      ArrayList<Chunk> move = compactGetMoveBlocks(start, moveSize);\n+      compactMoveChunks(move);\n+      freeUnusedChunks();\n+      storeNow();\n+    } finally {\n+      reuseSpace = oldReuse;\n+      retentionTime = oldRetentionTime;\n+    }\n+    return true;\n+  }\n+\n+  private ArrayList<Chunk> compactGetMoveBlocks(long startBlock, long moveSize) {\n+    ArrayList<Chunk> move = New.arrayList();\n+    for (Chunk c : chunks.values()) {\n+      if (c.block > startBlock) {\n+        move.add(c);\n+      }\n+    }\n+    // sort by block\n+    Collections.sort(move, new Comparator<Chunk>() {\n+      @Override\n+      public int compare(Chunk o1, Chunk o2) {\n+        return Long.signum(o1.block - o2.block);\n+      }\n+    });\n+    // find which is the last block to keep\n+    int count = 0;\n+    long size = 0;\n+    for (Chunk c : move) {\n+      long chunkSize = c.len * (long) BLOCK_SIZE;\n+      if (size + chunkSize > moveSize) {\n+        break;\n+      }\n+      size += chunkSize;\n+      count++;\n+    }\n+    // move the first block (so the first gap is moved),\n+    // and the one at the end (so the file shrinks)\n+    while (move.size() > count && move.size() > 1) {\n+      move.remove(1);\n+    }\n+\n+    return move;\n+  }\n+\n+  private void compactMoveChunks(ArrayList<Chunk> move) {\n+    for (Chunk c : move) {\n+      WriteBuffer buff = getWriteBuffer();\n+      long start = c.block * BLOCK_SIZE;\n+      int length = c.len * BLOCK_SIZE;\n+      buff.limit(length);\n+      ByteBuffer readBuff = fileStore.readFully(start, length);\n+      Chunk.readChunkHeader(readBuff, start);\n+      int chunkHeaderLen = readBuff.position();\n+      buff.position(chunkHeaderLen);\n+      buff.put(readBuff);\n+      long end = getFileLengthInUse();\n+      fileStore.markUsed(end, length);\n+      fileStore.free(start, length);\n+      c.block = end / BLOCK_SIZE;\n+      c.next = 0;\n+      buff.position(0);\n+      c.writeChunkHeader(buff, chunkHeaderLen);\n+      buff.position(length - Chunk.FOOTER_LENGTH);\n+      buff.put(c.getFooterBytes());\n+      buff.position(0);\n+      write(end, buff.getBuffer());\n+      releaseWriteBuffer(buff);\n+      markMetaChanged();\n+      meta.put(Chunk.getMetaKey(c.id), c.asString());\n+    }\n+\n+    // update the metadata (store at the end of the file)\n+    reuseSpace = false;\n+    commitAndSave();\n+    sync();\n+\n+    // now re-use the empty space\n+    reuseSpace = true;\n+    for (Chunk c : move) {\n+      if (!chunks.containsKey(c.id)) {\n+        // already removed during the\n+        // previous store operation\n+        continue;\n+      }\n+      WriteBuffer buff = getWriteBuffer();\n+      long start = c.block * BLOCK_SIZE;\n+      int length = c.len * BLOCK_SIZE;\n+      buff.limit(length);\n+      ByteBuffer readBuff = fileStore.readFully(start, length);\n+      Chunk.readChunkHeader(readBuff, 0);\n+      int chunkHeaderLen = readBuff.position();\n+      buff.position(chunkHeaderLen);\n+      buff.put(readBuff);\n+      long pos = fileStore.allocate(length);\n+      fileStore.free(start, length);\n+      buff.position(0);\n+      c.block = pos / BLOCK_SIZE;\n+      c.writeChunkHeader(buff, chunkHeaderLen);\n+      buff.position(length - Chunk.FOOTER_LENGTH);\n+      buff.put(c.getFooterBytes());\n+      buff.position(0);\n+      write(pos, buff.getBuffer());\n+      releaseWriteBuffer(buff);\n+      markMetaChanged();\n+      meta.put(Chunk.getMetaKey(c.id), c.asString());\n+    }\n+\n+    // update the metadata (within the file)\n+    commitAndSave();\n+    sync();\n+    shrinkFileIfPossible(0);\n+  }\n+\n+  /**\n+   * Force all stored changes to be written to the storage. The default\n+   * implementation calls FileChannel.force(true).\n+   */\n+  public void sync() {\n+    checkOpen();\n+    FileStore f = fileStore;\n+    if (f != null) {\n+      f.sync();\n+    }\n+  }\n+\n+  /**\n+   * Try to increase the fill rate by re-writing partially full chunks. Chunks\n+   * with a low number of live items are re-written.\n+   * <p>\n+   * If the current fill rate is higher than the target fill rate, nothing is\n+   * done.\n+   * <p>\n+   * Please note this method will not necessarily reduce the file size, as\n+   * empty chunks are not overwritten.\n+   * <p>\n+   * Only data of open maps can be moved. For maps that are not open, the old\n+   * chunk is still referenced. Therefore, it is recommended to open all maps\n+   * before calling this method.\n+   *\n+   * @param targetFillRate the minimum percentage of live entries\n+   * @param write          the minimum number of bytes to write\n+   * @return if a chunk was re-written\n+   */\n+  public boolean compact(int targetFillRate, int write) {\n+    if (!reuseSpace) {\n+      return false;\n+    }\n+    synchronized (compactSync) {\n+      checkOpen();\n+      ArrayList<Chunk> old;\n+      synchronized (this) {\n+        old = compactGetOldChunks(targetFillRate, write);\n+      }\n+      if (old == null || old.size() == 0) {\n+        return false;\n+      }\n+      compactRewrite(old);\n+      return true;\n+    }\n+  }\n+\n+  private ArrayList<Chunk> compactGetOldChunks(int targetFillRate, int write) {\n+    if (lastChunk == null) {\n+      // nothing to do\n+      return null;\n+    }\n+\n+    // calculate the fill rate\n+    long maxLengthSum = 0;\n+    long maxLengthLiveSum = 0;\n+\n+    long time = getTimeSinceCreation();\n+\n+    for (Chunk c : chunks.values()) {\n+      // ignore young chunks, because we don't optimize those\n+      if (c.time + retentionTime > time) {\n+        continue;\n+      }\n+      maxLengthSum += c.maxLen;\n+      maxLengthLiveSum += c.maxLenLive;\n+    }\n+    if (maxLengthLiveSum < 0) {\n+      // no old data\n+      return null;\n+    }\n+    // the fill rate of all chunks combined\n+    if (maxLengthSum <= 0) {\n+      // avoid division by 0\n+      maxLengthSum = 1;\n+    }\n+    int fillRate = (int) (100 * maxLengthLiveSum / maxLengthSum);\n+    if (fillRate >= targetFillRate) {\n+      return null;\n+    }\n+\n+    // the 'old' list contains the chunks we want to free up\n+    ArrayList<Chunk> old = New.arrayList();\n+    Chunk last = chunks.get(lastChunk.id);\n+    for (Chunk c : chunks.values()) {\n+      // only look at chunk older than the retention time\n+      // (it's possible to compact chunks earlier, but right\n+      // now we don't do that)\n+      if (c.time + retentionTime > time) {\n+        continue;\n+      }\n+      long age = last.version - c.version + 1;\n+      c.collectPriority = (int) (c.getFillRate() * 1000 / age);\n+      old.add(c);\n+    }\n+    if (old.size() == 0) {\n+      return null;\n+    }\n+\n+    // sort the list, so the first entry should be collected first\n+    Collections.sort(old, new Comparator<Chunk>() {\n+      @Override\n+      public int compare(Chunk o1, Chunk o2) {\n+        int comp = new Integer(o1.collectPriority).\n+            compareTo(o2.collectPriority);\n+        if (comp == 0) {\n+          comp = new Long(o1.maxLenLive).\n+              compareTo(o2.maxLenLive);\n+        }\n+        return comp;\n+      }\n+    });\n+    // find out up to were in the old list we need to move\n+    long written = 0;\n+    int chunkCount = 0;\n+    Chunk move = null;\n+    for (Chunk c : old) {\n+      if (move != null) {\n+        if (c.collectPriority > 0 && written > write) {\n+          break;\n+        }\n+      }\n+      written += c.maxLenLive;\n+      chunkCount++;\n+      move = c;\n+    }\n+    if (chunkCount < 1) {\n+      return null;\n+    }\n+    // remove the chunks we want to keep from this list\n+    boolean remove = false;\n+    for (Iterator<Chunk> it = old.iterator(); it.hasNext(); ) {\n+      Chunk c = it.next();\n+      if (move == c) {\n+        remove = true;\n+      } else if (remove) {\n+        it.remove();\n+      }\n+    }\n+    return old;\n+  }\n+\n+  private void compactRewrite(ArrayList<Chunk> old) {\n+    HashSet<Integer> set = New.hashSet();\n+    for (Chunk c : old) {\n+      set.add(c.id);\n+    }\n+    for (MVMap<?, ?> m : maps.values()) {\n+      @SuppressWarnings(\"unchecked\")\n+      MVMap<Object, Object> map = (MVMap<Object, Object>) m;\n+      if (!map.rewrite(set)) {\n+        return;\n+      }\n+    }\n+    if (!meta.rewrite(set)) {\n+      return;\n+    }\n+    freeUnusedChunks();\n+    commitAndSave();\n+  }\n+\n+  /**\n+   * Read a page.\n+   *\n+   * @param map the map\n+   * @param pos the page position\n+   * @return the page\n+   */\n+  Page readPage(MVMap<?, ?> map, long pos) {\n+    if (pos == 0) {\n+      throw DataUtils.newIllegalStateException(\n+          DataUtils.ERROR_FILE_CORRUPT, \"Position 0\");\n+    }\n+    Page p = cache == null ? null : cache.get(pos);\n+    if (p == null) {\n+      Chunk c = getChunk(pos);\n+      long filePos = c.block * BLOCK_SIZE;\n+      filePos += DataUtils.getPageOffset(pos);\n+      if (filePos < 0) {\n+        throw DataUtils.newIllegalStateException(\n+            DataUtils.ERROR_FILE_CORRUPT,\n+            \"Negative position {0}\", filePos);\n+      }\n+      long maxPos = (c.block + c.len) * BLOCK_SIZE;\n+      p = Page.read(fileStore, pos, map, filePos, maxPos);\n+      cachePage(pos, p, p.getMemory());\n+    }\n+    return p;\n+  }\n+\n+  /**\n+   * Remove a page.\n+   *\n+   * @param map    the map the page belongs to\n+   * @param pos    the position of the page\n+   * @param memory the memory usage\n+   */\n+  void removePage(MVMap<?, ?> map, long pos, int memory) {\n+    // we need to keep temporary pages,\n+    // to support reading old versions and rollback\n+    if (pos == 0) {\n+      // the page was not yet stored:\n+      // just using \"unsavedMemory -= memory\" could result in negative\n+      // values, because in some cases a page is allocated, but never\n+      // stored, so we need to use max\n+      unsavedMemory = Math.max(0, unsavedMemory - memory);\n+      return;\n+    }\n+\n+    // This could result in a cache miss if the operation is rolled back,\n+    // but we don't optimize for rollback.\n+    // We could also keep the page in the cache, as somebody\n+    // could still read it (reading the old version).\n+    if (cache != null) {\n+      if (DataUtils.getPageType(pos) == DataUtils.PAGE_TYPE_LEAF) {\n+        // keep nodes in the cache, because they are still used for\n+        // garbage collection\n+        cache.remove(pos);\n+      }\n+    }\n+\n+    Chunk c = getChunk(pos);\n+    long version = currentVersion;\n+    if (map == meta && currentStoreVersion >= 0) {\n+      if (Thread.currentThread() == currentStoreThread) {\n+        // if the meta map is modified while storing,\n+        // then this freed page needs to be registered\n+        // with the stored chunk, so that the old chunk\n+        // can be re-used\n+        version = currentStoreVersion;\n+      }\n+    }\n+    registerFreePage(version, c.id,\n+        DataUtils.getPageMaxLength(pos), 1);\n+  }\n+\n+  private void registerFreePage(long version, int chunkId,\n+                                long maxLengthLive, int pageCount) {\n+    HashMap<Integer, Chunk> freed = freedPageSpace.get(version);\n+    if (freed == null) {\n+      freed = New.hashMap();\n+      HashMap<Integer, Chunk> f2 = freedPageSpace.putIfAbsent(version,\n+          freed);\n+      if (f2 != null) {\n+        freed = f2;\n+      }\n+    }\n+    // synchronize, because pages could be freed concurrently\n+    synchronized (freed) {\n+      Chunk f = freed.get(chunkId);\n+      if (f == null) {\n+        f = new Chunk(chunkId);\n+        freed.put(chunkId, f);\n+      }\n+      f.maxLenLive -= maxLengthLive;\n+      f.pageCountLive -= pageCount;\n+    }\n+  }\n+\n+  Compressor getCompressorFast() {\n+    if (compressorFast == null) {\n+      compressorFast = new CompressLZF();\n+    }\n+    return compressorFast;\n+  }\n+\n+  Compressor getCompressorHigh() {\n+    if (compressorHigh == null) {\n+      compressorHigh = new CompressDeflate();\n+    }\n+    return compressorHigh;\n+  }\n+\n+  int getCompressionLevel() {\n+    return compressionLevel;\n+  }\n+\n+  public int getPageSplitSize() {\n+    return pageSplitSize;\n+  }\n+\n+  public boolean getReuseSpace() {\n+    return reuseSpace;\n+  }\n+\n+  /**\n+   * Whether empty space in the file should be re-used. If enabled, old data\n+   * is overwritten (default). If disabled, writes are appended at the end of\n+   * the file.\n+   * <p>\n+   * This setting is specially useful for online backup. To create an online\n+   * backup, disable this setting, then copy the file (starting at the\n+   * beginning of the file). In this case, concurrent backup and write\n+   * operations are possible (obviously the backup process needs to be faster\n+   * than the write operations).\n+   *\n+   * @param reuseSpace the new value\n+   */\n+  public void setReuseSpace(boolean reuseSpace) {\n+    this.reuseSpace = reuseSpace;\n+  }\n+\n+  public int getRetentionTime() {\n+    return retentionTime;\n+  }\n+\n+  /**\n+   * How long to retain old, persisted chunks, in milliseconds. Chunks that\n+   * are older may be overwritten once they contain no live data.\n+   * <p>\n+   * The default value is 45000 (45 seconds) when using the default file\n+   * store. It is assumed that a file system and hard disk will flush all\n+   * write buffers within this time. Using a lower value might be dangerous,\n+   * unless the file system and hard disk flush the buffers earlier. To\n+   * manually flush the buffers, use\n+   * <code>MVStore.getFile().force(true)</code>, however please note that\n+   * according to various tests this does not always work as expected\n+   * depending on the operating system and hardware.\n+   * <p>\n+   * The retention time needs to be long enough to allow reading old chunks\n+   * while traversing over the entries of a map.\n+   * <p>\n+   * This setting is not persisted.\n+   *\n+   * @param ms how many milliseconds to retain old chunks (0 to overwrite them\n+   *           as early as possible)\n+   */\n+  public void setRetentionTime(int ms) {\n+    this.retentionTime = ms;\n+  }\n+\n+  /**\n+   * How many versions to retain for in-memory stores. If not set, 5 old\n+   * versions are retained.\n+   *\n+   * @param count the number of versions to keep\n+   */\n+  public void setVersionsToKeep(int count) {\n+    this.versionsToKeep = count;\n+  }\n+\n+  /**\n+   * Get the oldest version to retain in memory (for in-memory stores).\n+   *\n+   * @return the version\n+   */\n+  public long getVersionsToKeep() {\n+    return versionsToKeep;\n+  }\n+\n+  /**\n+   * Get the oldest version to retain in memory, which is the manually set\n+   * retain version, or the current store version (whatever is older).\n+   *\n+   * @return the version\n+   */\n+  long getOldestVersionToKeep() {\n+    long v = currentVersion;\n+    if (fileStore == null) {\n+      return v - versionsToKeep;\n+    }\n+    long storeVersion = currentStoreVersion;\n+    if (storeVersion > -1) {\n+      v = Math.min(v, storeVersion);\n+    }\n+    return v;\n+  }\n+\n+  /**\n+   * Check whether all data can be read from this version. This requires that\n+   * all chunks referenced by this version are still available (not\n+   * overwritten).\n+   *\n+   * @param version the version\n+   * @return true if all data can be read\n+   */\n+  private boolean isKnownVersion(long version) {\n+    if (version > currentVersion || version < 0) {\n+      return false;\n+    }\n+    if (version == currentVersion || chunks.size() == 0) {\n+      // no stored data\n+      return true;\n+    }\n+    // need to check if a chunk for this version exists\n+    Chunk c = getChunkForVersion(version);\n+    if (c == null) {\n+      return false;\n+    }\n+    // also, all chunks referenced by this version\n+    // need to be available in the file\n+    MVMap<String, String> oldMeta = getMetaMap(version);\n+    if (oldMeta == null) {\n+      return false;\n+    }\n+    try {\n+      for (Iterator<String> it = oldMeta.keyIterator(\"chunk.\");\n+           it.hasNext(); ) {\n+        String chunkKey = it.next();\n+        if (!chunkKey.startsWith(\"chunk.\")) {\n+          break;\n+        }\n+        if (!meta.containsKey(chunkKey)) {\n+          String s = oldMeta.get(chunkKey);\n+          Chunk c2 = Chunk.fromString(s);\n+          Chunk test = readChunkHeaderAndFooter(c2.block);\n+          if (test == null || test.id != c2.id) {\n+            return false;\n+          }\n+          // we store this chunk\n+          chunks.put(c2.id, c2);\n+        }\n+      }\n+    } catch (IllegalStateException e) {\n+      // the chunk missing where the metadata is stored\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Increment the number of unsaved pages.\n+   *\n+   * @param memory the memory usage of the page\n+   */\n+  void registerUnsavedPage(int memory) {\n+    unsavedMemory += memory;\n+    int newValue = unsavedMemory;\n+    if (newValue > autoCommitMemory && autoCommitMemory > 0) {\n+      saveNeeded = true;\n+    }\n+  }\n+\n+  /**\n+   * This method is called before writing to a map.\n+   *\n+   * @param map the map\n+   */\n+  void beforeWrite(MVMap<?, ?> map) {\n+    if (saveNeeded) {\n+      if (map == meta) {\n+        // to, don't save while the metadata map is locked\n+        // this is to avoid deadlocks that could occur when we\n+        // synchronize on the store and then on the metadata map\n+        // TODO there should be no deadlocks possible\n+        return;\n+      }\n+      saveNeeded = false;\n+      // check again, because it could have been written by now\n+      if (unsavedMemory > autoCommitMemory && autoCommitMemory > 0) {\n+        commitAndSave();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get the store version. The store version is usually used to upgrade the\n+   * structure of the store after upgrading the application. Initially the\n+   * store version is 0, until it is changed.\n+   *\n+   * @return the store version\n+   */\n+  public int getStoreVersion() {\n+    checkOpen();\n+    String x = meta.get(\"setting.storeVersion\");\n+    return x == null ? 0 : DataUtils.parseHexInt(x);\n+  }\n+\n+  /**\n+   * Update the store version.\n+   *\n+   * @param version the new store version\n+   */\n+  public synchronized void setStoreVersion(int version) {\n+    checkOpen();\n+    markMetaChanged();\n+    meta.put(\"setting.storeVersion\", Integer.toHexString(version));\n+  }\n+\n+  /**\n+   * Revert to the beginning of the current version, reverting all uncommitted\n+   * changes.\n+   */\n+  public void rollback() {\n+    rollbackTo(currentVersion);\n+  }\n+\n+  /**\n+   * Revert to the beginning of the given version. All later changes (stored\n+   * or not) are forgotten. All maps that were created later are closed. A\n+   * rollback to a version before the last stored version is immediately\n+   * persisted. Rollback to version 0 means all data is removed.\n+   *\n+   * @param version the version to revert to\n+   */\n+  public synchronized void rollbackTo(long version) {\n+    checkOpen();\n+    if (version == 0) {\n+      // special case: remove all data\n+      for (MVMap<?, ?> m : maps.values()) {\n+        m.close();\n+      }\n+      meta.clear();\n+      chunks.clear();\n+      if (fileStore != null) {\n+        fileStore.clear();\n+      }\n+      maps.clear();\n+      freedPageSpace.clear();\n+      currentVersion = version;\n+      setWriteVersion(version);\n+      metaChanged = false;\n+      return;\n+    }\n+    DataUtils.checkArgument(\n+        isKnownVersion(version),\n+        \"Unknown version {0}\", version);\n+    for (MVMap<?, ?> m : maps.values()) {\n+      m.rollbackTo(version);\n+    }\n+    for (long v = currentVersion; v >= version; v--) {\n+      if (freedPageSpace.size() == 0) {\n+        break;\n+      }\n+      freedPageSpace.remove(v);\n+    }\n+    meta.rollbackTo(version);\n+    metaChanged = false;\n+    boolean loadFromFile = false;\n+    // find out which chunks to remove,\n+    // and which is the newest chunk to keep\n+    // (the chunk list can have gaps)\n+    ArrayList<Integer> remove = new ArrayList<Integer>();\n+    Chunk keep = null;\n+    for (Chunk c : chunks.values()) {\n+      if (c.version > version) {\n+        remove.add(c.id);\n+      } else if (keep == null || keep.id < c.id) {\n+        keep = c;\n+      }\n+    }\n+    if (remove.size() > 0) {\n+      // remove the youngest first, so we don't create gaps\n+      // (in case we remove many chunks)\n+      Collections.sort(remove, Collections.reverseOrder());\n+      revertTemp(version);\n+      loadFromFile = true;\n+      for (int id : remove) {\n+        Chunk c = chunks.remove(id);\n+        long start = c.block * BLOCK_SIZE;\n+        int length = c.len * BLOCK_SIZE;\n+        fileStore.free(start, length);\n+        // overwrite the chunk,\n+        // so it is not be used later on\n+        WriteBuffer buff = getWriteBuffer();\n+        buff.limit(length);\n+        // buff.clear() does not set the data\n+        Arrays.fill(buff.getBuffer().array(), (byte) 0);\n+        write(start, buff.getBuffer());\n+        releaseWriteBuffer(buff);\n+        // only really needed if we remove many chunks, when writes are\n+        // re-ordered - but we do it always, because rollback is not\n+        // performance critical\n+        sync();\n+      }\n+      lastChunk = keep;\n+      writeStoreHeader();\n+      readStoreHeader();\n+    }\n+    for (MVMap<?, ?> m : New.arrayList(maps.values())) {\n+      int id = m.getId();\n+      if (m.getCreateVersion() >= version) {\n+        m.close();\n+        maps.remove(id);\n+      } else {\n+        if (loadFromFile) {\n+          m.setRootPos(getRootPos(meta, id), -1);\n         }\n-        o = config.get(\"pageSplitSize\");\n-        pageSplitSize = o != null         ? (Integer) o :\n-                        fileStore == null ? 48 :\n-                                            16 * 1024;\n-        o = config.get(\"backgroundExceptionHandler\");\n-        this.backgroundExceptionHandler = (UncaughtExceptionHandler) o;\n-        meta = new MVMap<String, String>(StringDataType.INSTANCE,\n-                StringDataType.INSTANCE);\n-        HashMap<String, Object> c = New.hashMap();\n-        c.put(\"id\", 0);\n-        c.put(\"createVersion\", currentVersion);\n-        meta.init(this, c);\n-        if (fileStore == null) {\n-            cache = null;\n-            cacheChunkRef = null;\n-            return;\n+      }\n+    }\n+    // rollback might have rolled back the stored chunk metadata as well\n+    if (lastChunk != null) {\n+      for (Chunk c : chunks.values()) {\n+        meta.put(Chunk.getMetaKey(c.id), c.asString());\n+      }\n+    }\n+    currentVersion = version;\n+    setWriteVersion(version);\n+  }\n+\n+  private static long getRootPos(MVMap<String, String> map, int mapId) {\n+    String root = map.get(MVMap.getMapRootKey(mapId));\n+    return root == null ? 0 : DataUtils.parseHexLong(root);\n+  }\n+\n+  private void revertTemp(long storeVersion) {\n+    for (Iterator<Long> it = freedPageSpace.keySet().iterator();\n+         it.hasNext(); ) {\n+      long v = it.next();\n+      if (v > storeVersion) {\n+        continue;\n+      }\n+      it.remove();\n+    }\n+    for (MVMap<?, ?> m : maps.values()) {\n+      m.removeUnusedOldVersions();\n+    }\n+  }\n+\n+  /**\n+   * Get the current version of the data. When a new store is created, the\n+   * version is 0.\n+   *\n+   * @return the version\n+   */\n+  public long getCurrentVersion() {\n+    return currentVersion;\n+  }\n+\n+  /**\n+   * Get the file store.\n+   *\n+   * @return the file store\n+   */\n+  public FileStore getFileStore() {\n+    return fileStore;\n+  }\n+\n+  /**\n+   * Get the store header. This data is for informational purposes only. The\n+   * data is subject to change in future versions. The data should not be\n+   * modified (doing so may corrupt the store).\n+   *\n+   * @return the store header\n+   */\n+  public Map<String, Object> getStoreHeader() {\n+    return storeHeader;\n+  }\n+\n+  private void checkOpen() {\n+    if (closed) {\n+      throw DataUtils.newIllegalStateException(DataUtils.ERROR_CLOSED,\n+          \"This store is closed\", panicException);\n+    }\n+  }\n+\n+  /**\n+   * Rename a map.\n+   *\n+   * @param map     the map\n+   * @param newName the new name\n+   */\n+  public synchronized void renameMap(MVMap<?, ?> map, String newName) {\n+    checkOpen();\n+    DataUtils.checkArgument(map != meta,\n+        \"Renaming the meta map is not allowed\");\n+    int id = map.getId();\n+    String oldName = getMapName(id);\n+    if (oldName.equals(newName)) {\n+      return;\n+    }\n+    DataUtils.checkArgument(\n+        !meta.containsKey(\"name.\" + newName),\n+        \"A map named {0} already exists\", newName);\n+    markMetaChanged();\n+    String x = Integer.toHexString(id);\n+    meta.remove(\"name.\" + oldName);\n+    meta.put(MVMap.getMapKey(id), map.asString(newName));\n+    meta.put(\"name.\" + newName, x);\n+  }\n+\n+  /**\n+   * Remove a map. Please note rolling back this operation does not restore\n+   * the data; if you need this ability, use Map.clear().\n+   *\n+   * @param map the map to remove\n+   */\n+  public synchronized void removeMap(MVMap<?, ?> map) {\n+    checkOpen();\n+    DataUtils.checkArgument(map != meta,\n+        \"Removing the meta map is not allowed\");\n+    map.clear();\n+    int id = map.getId();\n+    String name = getMapName(id);\n+    markMetaChanged();\n+    meta.remove(MVMap.getMapKey(id));\n+    meta.remove(\"name.\" + name);\n+    meta.remove(MVMap.getMapRootKey(id));\n+    maps.remove(id);\n+  }\n+\n+  /**\n+   * Get the name of the given map.\n+   *\n+   * @param id the map id\n+   * @return the name, or null if not found\n+   */\n+  public synchronized String getMapName(int id) {\n+    checkOpen();\n+    String m = meta.get(MVMap.getMapKey(id));\n+    return m == null ? null : DataUtils.parseMap(m).get(\"name\");\n+  }\n+\n+  /**\n+   * Commit and save all changes, if there are any, and compact the store if\n+   * needed.\n+   */\n+  void writeInBackground() {\n+    if (closed) {\n+      return;\n+    }\n+\n+    // could also commit when there are many unsaved pages,\n+    // but according to a test it doesn't really help\n+\n+    long time = getTimeSinceCreation();\n+    if (time <= lastCommitTime + autoCommitDelay) {\n+      return;\n+    }\n+    if (hasUnsavedChanges()) {\n+      try {\n+        commitAndSave();\n+      } catch (Exception e) {\n+        if (backgroundExceptionHandler != null) {\n+          backgroundExceptionHandler.uncaughtException(null, e);\n+          return;\n+        }\n+      }\n+    }\n+    if (autoCompactFillRate > 0) {\n+      try {\n+        // whether there were file read or write operations since\n+        // the last time\n+        boolean fileOps;\n+        long fileOpCount = fileStore.getWriteCount() + fileStore.getReadCount();\n+        if (autoCompactLastFileOpCount != fileOpCount) {\n+          fileOps = true;\n+        } else {\n+          fileOps = false;\n+        }\n+        // use a lower fill rate if there were any file operations\n+        int fillRate = fileOps ? autoCompactFillRate / 3 : autoCompactFillRate;\n+        // TODO how to avoid endless compaction if there is a bug\n+        // in the bookkeeping?\n+        compact(fillRate, autoCommitMemory);\n+        autoCompactLastFileOpCount = fileStore.getWriteCount() + fileStore.getReadCount();\n+      } catch (Exception e) {\n+        if (backgroundExceptionHandler != null) {\n+          backgroundExceptionHandler.uncaughtException(null, e);\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Set the read cache size in MB.\n+   *\n+   * @param mb the cache size in MB.\n+   */\n+  public void setCacheSize(int mb) {\n+    final long bytes = (long) mb * 1024 * 1024;\n+    if (cache != null) {\n+      cache.setMaxMemory(bytes);\n+      cache.clear();\n+    }\n+    if (cacheChunkRef != null) {\n+      cacheChunkRef.setMaxMemory(bytes / 4);\n+      cacheChunkRef.clear();\n+    }\n+  }\n+\n+  public boolean isClosed() {\n+    return closed;\n+  }\n+\n+  private void stopBackgroundThread() {\n+    BackgroundWriterThread t = backgroundWriterThread;\n+    if (t == null) {\n+      return;\n+    }\n+    backgroundWriterThread = null;\n+    if (Thread.currentThread() == t) {\n+      // within the thread itself - can not join\n+      return;\n+    }\n+    synchronized (t.sync) {\n+      t.sync.notifyAll();\n+    }\n+    if (Thread.holdsLock(this)) {\n+      // called from storeNow: can not join,\n+      // because that could result in a deadlock\n+      return;\n+    }\n+    try {\n+      t.join();\n+    } catch (Exception e) {\n+      // ignore\n+    }\n+  }\n+\n+  /**\n+   * Set the maximum delay in milliseconds to auto-commit changes.\n+   * <p>\n+   * To disable auto-commit, set the value to 0. In this case, changes are\n+   * only committed when explicitly calling commit.\n+   * <p>\n+   * The default is 1000, meaning all changes are committed after at most one\n+   * second.\n+   *\n+   * @param millis the maximum delay\n+   */\n+  public void setAutoCommitDelay(int millis) {\n+    if (autoCommitDelay == millis) {\n+      return;\n+    }\n+    autoCommitDelay = millis;\n+    if (fileStore == null || fileStore.isReadOnly()) {\n+      return;\n+    }\n+    stopBackgroundThread();\n+    // start the background thread if needed\n+    if (millis > 0) {\n+      int sleep = Math.max(1, millis / 10);\n+      BackgroundWriterThread t =\n+          new BackgroundWriterThread(this, sleep,\n+              fileStore.toString());\n+      t.start();\n+      backgroundWriterThread = t;\n+    }\n+  }\n+\n+  /**\n+   * Get the auto-commit delay.\n+   *\n+   * @return the delay in milliseconds, or 0 if auto-commit is disabled.\n+   */\n+  public int getAutoCommitDelay() {\n+    return autoCommitDelay;\n+  }\n+\n+  /**\n+   * Get the maximum memory (in bytes) used for unsaved pages. If this number\n+   * is exceeded, unsaved changes are stored to disk.\n+   *\n+   * @return the memory in bytes\n+   */\n+  public int getAutoCommitMemory() {\n+    return autoCommitMemory;\n+  }\n+\n+  /**\n+   * Get the estimated memory (in bytes) of unsaved data. If the value exceeds\n+   * the auto-commit memory, the changes are committed.\n+   * <p>\n+   * The returned value is an estimation only.\n+   *\n+   * @return the memory in bytes\n+   */\n+  public int getUnsavedMemory() {\n+    return unsavedMemory;\n+  }\n+\n+  /**\n+   * Put the page in the cache.\n+   *\n+   * @param pos    the page position\n+   * @param page   the page\n+   * @param memory the memory used\n+   */\n+  void cachePage(long pos, Page page, int memory) {\n+    if (cache != null) {\n+      cache.put(pos, page, memory);\n+    }\n+  }\n+\n+  /**\n+   * Get the amount of memory used for caching, in MB.\n+   * Note that this does not include the page chunk references cache, which is\n+   * 25% of the size of the page cache.\n+   *\n+   * @return the amount of memory used for caching\n+   */\n+  public int getCacheSizeUsed() {\n+    if (cache == null) {\n+      return 0;\n+    }\n+    return (int) (cache.getUsedMemory() / 1024 / 1024);\n+  }\n+\n+  /**\n+   * Get the maximum cache size, in MB.\n+   * Note that this does not include the page chunk references cache, which is\n+   * 25% of the size of the page cache.\n+   *\n+   * @return the cache size\n+   */\n+  public int getCacheSize() {\n+    if (cache == null) {\n+      return 0;\n+    }\n+    return (int) (cache.getMaxMemory() / 1024 / 1024);\n+  }\n+\n+  /**\n+   * Get the cache.\n+   *\n+   * @return the cache\n+   */\n+  public CacheLongKeyLIRS<Page> getCache() {\n+    return cache;\n+  }\n+\n+  /**\n+   * Whether the store is read-only.\n+   *\n+   * @return true if it is\n+   */\n+  public boolean isReadOnly() {\n+    return fileStore == null ? false : fileStore.isReadOnly();\n+  }\n+\n+  /**\n+   * A background writer thread to automatically store changes from time to\n+   * time.\n+   */\n+  private static class BackgroundWriterThread extends Thread {\n+\n+    public final Object sync = new Object();\n+    private final MVStore store;\n+    private final int sleep;\n+\n+    BackgroundWriterThread(MVStore store, int sleep, String fileStoreName) {\n+      super(\"MVStore background writer \" + fileStoreName);\n+      this.store = store;\n+      this.sleep = sleep;\n+      setDaemon(true);\n+    }\n+\n+    @Override\n+    public void run() {\n+      while (true) {\n+        Thread t = store.backgroundWriterThread;\n+        if (t == null) {\n+          break;\n         }\n-        retentionTime = fileStore.getDefaultRetentionTime();\n-        boolean readOnly = config.containsKey(\"readOnly\");\n-        o = config.get(\"cacheSize\");\n-        int mb = o == null ? 16 : (Integer) o;\n-        if (mb > 0) {\n-            CacheLongKeyLIRS.Config cc = new CacheLongKeyLIRS.Config();\n-            cc.maxMemory = mb * 1024L * 1024L;\n-            o = config.get(\"cacheConcurrency\");\n-            if (o != null) {\n-                cc.segmentCount = (Integer) o;\n-            }\n-            cache = new CacheLongKeyLIRS<Page>(cc);\n-            cc.maxMemory /= 4;\n-            cacheChunkRef = new CacheLongKeyLIRS<PageChildren>(cc);\n+        synchronized (sync) {\n+          try {\n+            sync.wait(sleep);\n+          } catch (InterruptedException e) {\n+            continue;\n+          }\n         }\n-        o = config.get(\"autoCommitBufferSize\");\n-        int kb = o == null ? 1024 : (Integer) o;\n-        // 19 KB memory is about 1 KB storage\n-        autoCommitMemory = kb * 1024 * 19;\n+        store.writeInBackground();\n+      }\n+    }\n \n-        o = config.get(\"autoCompactFillRate\");\n-        autoCompactFillRate = o == null ? 50 : (Integer) o;\n+  }\n \n-        char[] encryptionKey = (char[]) config.get(\"encryptionKey\");\n-        try {\n-            if (!fileStoreIsProvided) {\n-                fileStore.open(fileName, readOnly, encryptionKey);\n-            }\n-            if (fileStore.size() == 0) {\n-                creationTime = getTimeAbsolute();\n-                lastCommitTime = creationTime;\n-                storeHeader.put(\"H\", 2);\n-                storeHeader.put(\"blockSize\", BLOCK_SIZE);\n-                storeHeader.put(\"format\", FORMAT_WRITE);\n-                storeHeader.put(\"created\", creationTime);\n-                writeStoreHeader();\n-            } else {\n-                readStoreHeader();\n-            }\n-        } catch (IllegalStateException e) {\n-            panic(e);\n-        } finally {\n-            if (encryptionKey != null) {\n-                Arrays.fill(encryptionKey, (char) 0);\n-            }\n-        }\n-        lastCommitTime = getTimeSinceCreation();\n+  /**\n+   * A builder for an MVStore.\n+   */\n+  public static class Builder {\n \n-        // setAutoCommitDelay starts the thread, but only if\n-        // the parameter is different from the old value\n-        o = config.get(\"autoCommitDelay\");\n-        int delay = o == null ? 1000 : (Integer) o;\n-        setAutoCommitDelay(delay);\n-    }\n+    private final HashMap<String, Object> config = New.hashMap();\n \n-    private void panic(IllegalStateException e) {\n-        if (backgroundExceptionHandler != null) {\n-            backgroundExceptionHandler.uncaughtException(null, e);\n-        }\n-        panicException = e;\n-        closeImmediately();\n-        throw e;\n+    private Builder set(String key, Object value) {\n+      config.put(key, value);\n+      return this;\n     }\n \n     /**\n-     * Open a store in exclusive mode. For a file-based store, the parent\n-     * directory must already exist.\n+     * Disable auto-commit, by setting the auto-commit delay and auto-commit\n+     * buffer size to 0.\n      *\n-     * @param fileName the file name (null for in-memory)\n-     * @return the store\n+     * @return this\n      */\n-    public static MVStore open(String fileName) {\n-        HashMap<String, Object> config = New.hashMap();\n-        config.put(\"fileName\", fileName);\n-        return new MVStore(config);\n+    public Builder autoCommitDisabled() {\n+      // we have a separate config option so that\n+      // no thread is started if the write delay is 0\n+      // (if we only had a setter in the MVStore,\n+      // the thread would need to be started in any case)\n+      set(\"autoCommitBufferSize\", 0);\n+      return set(\"autoCommitDelay\", 0);\n     }\n \n     /**\n-     * Open an old, stored version of a map.\n+     * Set the size of the write buffer, in KB disk space (for file-based\n+     * stores). Unless auto-commit is disabled, changes are automatically\n+     * saved if there are more than this amount of changes.\n+     * <p>\n+     * The default is 1024 KB.\n+     * <p>\n+     * When the value is set to 0 or lower, data is not automatically\n+     * stored.\n      *\n-     * @param version the version\n-     * @param mapId the map id\n-     * @param template the template map\n-     * @return the read-only map\n+     * @param kb the write buffer size, in kilobytes\n+     * @return this\n      */\n-    @SuppressWarnings(\"unchecked\")\n-    <T extends MVMap<?, ?>> T openMapVersion(long version, int mapId,\n-            MVMap<?, ?> template) {\n-        MVMap<String, String> oldMeta = getMetaMap(version);\n-        long rootPos = getRootPos(oldMeta, mapId);\n-        MVMap<?, ?> m = template.openReadOnly();\n-        m.setRootPos(rootPos, version);\n-        return (T) m;\n+    public Builder autoCommitBufferSize(int kb) {\n+      return set(\"autoCommitBufferSize\", kb);\n     }\n \n     /**\n-     * Open a map with the default settings. The map is automatically create if\n-     * it does not yet exist. If a map with this name is already open, this map\n-     * is returned.\n+     * Set the auto-compact target fill rate. If the average fill rate (the\n+     * percentage of the storage space that contains active data) of the\n+     * chunks is lower, then the chunks with a low fill rate are re-written.\n+     * Also, if the percentage of empty space between chunks is higher than\n+     * this value, then chunks at the end of the file are moved. Compaction\n+     * stops if the target fill rate is reached.\n+     * <p>\n+     * The default value is 50 (50%). The value 0 disables auto-compacting.\n+     * <p>\n      *\n-     * @param <K> the key type\n-     * @param <V> the value type\n-     * @param name the name of the map\n-     * @return the map\n+     * @param percent the target fill rate\n+     * @return this\n      */\n-    public <K, V> MVMap<K, V> openMap(String name) {\n-        return openMap(name, new MVMap.Builder<K, V>());\n+    public Builder autoCompactFillRate(int percent) {\n+      return set(\"autoCompactFillRate\", percent);\n     }\n \n     /**\n-     * Open a map with the given builder. The map is automatically create if it\n-     * does not yet exist. If a map with this name is already open, this map is\n-     * returned.\n+     * Use the following file name. If the file does not exist, it is\n+     * automatically created. The parent directory already must exist.\n      *\n-     * @param <K> the key type\n-     * @param <V> the value type\n-     * @param name the name of the map\n-     * @param builder the map builder\n-     * @return the map\n+     * @param fileName the file name\n+     * @return this\n      */\n-    public synchronized <M extends MVMap<K, V>, K, V> M openMap(\n-            String name, MVMap.MapBuilder<M, K, V> builder) {\n-        checkOpen();\n-        String x = meta.get(\"name.\" + name);\n-        int id;\n-        long root;\n-        HashMap<String, Object> c;\n-        M map;\n-        if (x != null) {\n-            id = DataUtils.parseHexInt(x);\n-            @SuppressWarnings(\"unchecked\")\n-            M old = (M) maps.get(id);\n-            if (old != null) {\n-                return old;\n-            }\n-            map = builder.create();\n-            String config = meta.get(MVMap.getMapKey(id));\n-            c = New.hashMap();\n-            c.putAll(DataUtils.parseMap(config));\n-            c.put(\"id\", id);\n-            map.init(this, c);\n-            root = getRootPos(meta, id);\n-        } else {\n-            c = New.hashMap();\n-            id = ++lastMapId;\n-            c.put(\"id\", id);\n-            c.put(\"createVersion\", currentVersion);\n-            map = builder.create();\n-            map.init(this, c);\n-            markMetaChanged();\n-            x = Integer.toHexString(id);\n-            meta.put(MVMap.getMapKey(id), map.asString(name));\n-            meta.put(\"name.\" + name, x);\n-            root = 0;\n-        }\n-        map.setRootPos(root, -1);\n-        maps.put(id, map);\n-        return map;\n+    public Builder fileName(String fileName) {\n+      return set(\"fileName\", fileName);\n     }\n \n     /**\n-     * Get the set of all map names.\n+     * Encrypt / decrypt the file using the given password. This method has\n+     * no effect for in-memory stores. The password is passed as a\n+     * char array so that it can be cleared as soon as possible. Please note\n+     * there is still a small risk that password stays in memory (due to\n+     * Java garbage collection). Also, the hashed encryption key is kept in\n+     * memory as long as the file is open.\n      *\n-     * @return the set of names\n+     * @param password the password\n+     * @return this\n      */\n-    public synchronized Set<String> getMapNames() {\n-        HashSet<String> set = New.hashSet();\n-        checkOpen();\n-        for (Iterator<String> it = meta.keyIterator(\"name.\"); it.hasNext();) {\n-            String x = it.next();\n-            if (!x.startsWith(\"name.\")) {\n-                break;\n-            }\n-            set.add(x.substring(\"name.\".length()));\n-        }\n-        return set;\n+    public Builder encryptionKey(char[] password) {\n+      return set(\"encryptionKey\", password);\n     }\n \n     /**\n-     * Get the metadata map. This data is for informational purposes only. The\n-     * data is subject to change in future versions.\n+     * Open the file in read-only mode. In this case, a shared lock will be\n+     * acquired to ensure the file is not concurrently opened in write mode.\n      * <p>\n-     * The data in this map should not be modified (changing system data may\n-     * corrupt the store). If modifications are needed, they need be\n-     * synchronized on the store.\n+     * If this option is not used, the file is locked exclusively.\n      * <p>\n-     * The metadata map contains the following entries:\n-     * <pre>\n-     * chunk.{chunkId} = {chunk metadata}\n-     * name.{name} = {mapId}\n-     * map.{mapId} = {map metadata}\n-     * root.{mapId} = {root position}\n-     * setting.storeVersion = {version}\n-     * </pre>\n+     * Please note a store may only be opened once in every JVM (no matter\n+     * whether it is opened in read-only or read-write mode), because each\n+     * file may be locked only once in a process.\n      *\n-     * @return the metadata map\n+     * @return this\n      */\n-    public MVMap<String, String> getMetaMap() {\n-        checkOpen();\n-        return meta;\n-    }\n-\n-    private MVMap<String, String> getMetaMap(long version) {\n-        Chunk c = getChunkForVersion(version);\n-        DataUtils.checkArgument(c != null, \"Unknown version {0}\", version);\n-        c = readChunkHeader(c.block);\n-        MVMap<String, String> oldMeta = meta.openReadOnly();\n-        oldMeta.setRootPos(c.metaRootPos, version);\n-        return oldMeta;\n-    }\n-\n-    private Chunk getChunkForVersion(long version) {\n-        Chunk newest = null;\n-        for (Chunk c : chunks.values()) {\n-            if (c.version <= version) {\n-                if (newest == null || c.id > newest.id) {\n-                    newest = c;\n-                }\n-            }\n-        }\n-        return newest;\n+    public Builder readOnly() {\n+      return set(\"readOnly\", 1);\n     }\n \n     /**\n-     * Check whether a given map exists.\n+     * Set the read cache size in MB. The default is 16 MB.\n      *\n-     * @param name the map name\n-     * @return true if it exists\n+     * @param mb the cache size in megabytes\n+     * @return this\n      */\n-    public boolean hasMap(String name) {\n-        return meta.containsKey(\"name.\" + name);\n-    }\n-\n-    private void markMetaChanged() {\n-        // changes in the metadata alone are usually not detected, as the meta\n-        // map is changed after storing\n-        metaChanged = true;\n-    }\n-\n-    private synchronized void readStoreHeader() {\n-        Chunk newest = null;\n-        boolean validStoreHeader = false;\n-        // find out which chunk and version are the newest\n-        // read the first two blocks\n-        ByteBuffer fileHeaderBlocks = fileStore.readFully(0, 2 * BLOCK_SIZE);\n-        byte[] buff = new byte[BLOCK_SIZE];\n-        for (int i = 0; i <= BLOCK_SIZE; i += BLOCK_SIZE) {\n-            fileHeaderBlocks.get(buff);\n-            // the following can fail for various reasons\n-            try {\n-                String s = new String(buff, 0, BLOCK_SIZE,\n-                        DataUtils.LATIN).trim();\n-                HashMap<String, String> m = DataUtils.parseMap(s);\n-                int blockSize = DataUtils.readHexInt(\n-                        m, \"blockSize\", BLOCK_SIZE);\n-                if (blockSize != BLOCK_SIZE) {\n-                    throw DataUtils.newIllegalStateException(\n-                            DataUtils.ERROR_UNSUPPORTED_FORMAT,\n-                            \"Block size {0} is currently not supported\",\n-                            blockSize);\n-                }\n-                int check = DataUtils.readHexInt(m, \"fletcher\", 0);\n-                m.remove(\"fletcher\");\n-                s = s.substring(0, s.lastIndexOf(\"fletcher\") - 1);\n-                byte[] bytes = s.getBytes(DataUtils.LATIN);\n-                int checksum = DataUtils.getFletcher32(bytes,\n-                        bytes.length);\n-                if (check != checksum) {\n-                    continue;\n-                }\n-                long version = DataUtils.readHexLong(m, \"version\", 0);\n-                if (newest == null || version > newest.version) {\n-                    validStoreHeader = true;\n-                    storeHeader.putAll(m);\n-                    creationTime = DataUtils.readHexLong(m, \"created\", 0);\n-                    int chunkId = DataUtils.readHexInt(m, \"chunk\", 0);\n-                    long block = DataUtils.readHexLong(m, \"block\", 0);\n-                    Chunk test = readChunkHeaderAndFooter(block);\n-                    if (test != null && test.id == chunkId) {\n-                        newest = test;\n-                    }\n-                }\n-            } catch (Exception e) {\n-                continue;\n-            }\n-        }\n-        if (!validStoreHeader) {\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_FILE_CORRUPT,\n-                    \"Store header is corrupt: {0}\", fileStore);\n-        }\n-        long format = DataUtils.readHexLong(storeHeader, \"format\", 1);\n-        if (format > FORMAT_WRITE && !fileStore.isReadOnly()) {\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_UNSUPPORTED_FORMAT,\n-                    \"The write format {0} is larger \" +\n-                    \"than the supported format {1}, \" +\n-                    \"and the file was not opened in read-only mode\",\n-                    format, FORMAT_WRITE);\n-        }\n-        format = DataUtils.readHexLong(storeHeader, \"formatRead\", format);\n-        if (format > FORMAT_READ) {\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_UNSUPPORTED_FORMAT,\n-                    \"The read format {0} is larger \" +\n-                    \"than the supported format {1}\",\n-                    format, FORMAT_READ);\n-        }\n-        lastStoredVersion = -1;\n-        chunks.clear();\n-        long now = System.currentTimeMillis();\n-        // calculate the year (doesn't have to be exact;\n-        // we assume 365.25 days per year, * 4 = 1461)\n-        int year =  1970 + (int) (now / (1000L * 60 * 60 * 6 * 1461));\n-        if (year < 2014) {\n-            // if the year is before 2014,\n-            // we assume the system doesn't have a real-time clock,\n-            // and we set the creationTime to the past, so that\n-            // existing chunks are overwritten\n-            creationTime = now - fileStore.getDefaultRetentionTime();\n-        } else if (now < creationTime) {\n-            // the system time was set to the past:\n-            // we change the creation time\n-            creationTime = now;\n-            storeHeader.put(\"created\", creationTime);\n-        }\n-        Chunk test = readChunkFooter(fileStore.size());\n-        if (test != null) {\n-            test = readChunkHeaderAndFooter(test.block);\n-            if (test != null) {\n-                if (newest == null || test.version > newest.version) {\n-                    newest = test;\n-                }\n-            }\n-        }\n-        if (newest == null) {\n-            // no chunk\n-            return;\n-        }\n-        // read the chunk header and footer,\n-        // and follow the chain of next chunks\n-        while (true) {\n-            if (newest.next == 0 ||\n-                    newest.next >= fileStore.size() / BLOCK_SIZE) {\n-                // no (valid) next\n-                break;\n-            }\n-            test = readChunkHeaderAndFooter(newest.next);\n-            if (test == null || test.id <= newest.id) {\n-                break;\n-            }\n-            newest = test;\n-        }\n-        setLastChunk(newest);\n-        loadChunkMeta();\n-        // read all chunk headers and footers within the retention time,\n-        // to detect unwritten data after a power failure\n-        verifyLastChunks();\n-        // build the free space list\n-        for (Chunk c : chunks.values()) {\n-            if (c.pageCountLive == 0) {\n-                // remove this chunk in the next save operation\n-                registerFreePage(currentVersion, c.id, 0, 0);\n-            }\n-            long start = c.block * BLOCK_SIZE;\n-            int length = c.len * BLOCK_SIZE;\n-            fileStore.markUsed(start, length);\n-        }\n-    }\n-\n-    private void loadChunkMeta() {\n-        // load the chunk metadata: we can load in any order,\n-        // because loading chunk metadata might recursively load another chunk\n-        for (Iterator<String> it = meta.keyIterator(\"chunk.\"); it.hasNext();) {\n-            String s = it.next();\n-            if (!s.startsWith(\"chunk.\")) {\n-                break;\n-            }\n-            s = meta.get(s);\n-            Chunk c = Chunk.fromString(s);\n-            if (!chunks.containsKey(c.id)) {\n-                if (c.block == Long.MAX_VALUE) {\n-                    throw DataUtils.newIllegalStateException(\n-                            DataUtils.ERROR_FILE_CORRUPT,\n-                            \"Chunk {0} is invalid\", c.id);\n-                }\n-                chunks.put(c.id, c);\n-            }\n-        }\n-    }\n-\n-    private void setLastChunk(Chunk last) {\n-        lastChunk = last;\n-        if (last == null) {\n-            // no valid chunk\n-            lastMapId = 0;\n-            currentVersion = 0;\n-            meta.setRootPos(0, -1);\n-        } else {\n-            lastMapId = last.mapId;\n-            currentVersion = last.version;\n-            chunks.put(last.id, last);\n-            meta.setRootPos(last.metaRootPos, -1);\n-        }\n-        setWriteVersion(currentVersion);\n-    }\n-\n-    private void verifyLastChunks() {\n-        long time = getTimeSinceCreation();\n-        ArrayList<Integer> ids = new ArrayList<Integer>(chunks.keySet());\n-        Collections.sort(ids);\n-        int newestValidChunk = -1;\n-        Chunk old = null;\n-        for (Integer chunkId : ids) {\n-            Chunk c = chunks.get(chunkId);\n-            if (old != null && c.time < old.time) {\n-                // old chunk (maybe leftover from a previous crash)\n-                break;\n-            }\n-            old = c;\n-            if (c.time + retentionTime < time) {\n-                // old chunk, no need to verify\n-                newestValidChunk = c.id;\n-                continue;\n-            }\n-            Chunk test = readChunkHeaderAndFooter(c.block);\n-            if (test == null || test.id != c.id) {\n-                break;\n-            }\n-            newestValidChunk = chunkId;\n-        }\n-        Chunk newest = chunks.get(newestValidChunk);\n-        if (newest != lastChunk) {\n-            // to avoid re-using newer chunks later on, we could clear\n-            // the headers and footers of those, but we might not know about all\n-            // of them, so that could be incomplete - but we check that newer\n-            // chunks are written after older chunks, so we are safe\n-            rollbackTo(newest == null ? 0 : newest.version);\n-        }\n+    public Builder cacheSize(int mb) {\n+      return set(\"cacheSize\", mb);\n     }\n \n     /**\n-     * Read a chunk header and footer, and verify the stored data is consistent.\n+     * Set the read cache concurrency. The default is 16, meaning 16\n+     * segments are used.\n      *\n-     * @param block the block\n-     * @return the chunk, or null if the header or footer don't match or are not\n-     *         consistent\n+     * @param concurrency the cache concurrency\n+     * @return this\n      */\n-    private Chunk readChunkHeaderAndFooter(long block) {\n-        Chunk header;\n-        try {\n-            header = readChunkHeader(block);\n-        } catch (Exception e) {\n-            // invalid chunk header: ignore, but stop\n-            return null;\n-        }\n-        if (header == null) {\n-            return null;\n-        }\n-        Chunk footer = readChunkFooter((block + header.len) * BLOCK_SIZE);\n-        if (footer == null || footer.id != header.id) {\n-            return null;\n-        }\n-        return header;\n+    public Builder cacheConcurrency(int concurrency) {\n+      return set(\"cacheConcurrency\", concurrency);\n     }\n \n     /**\n-     * Try to read a chunk footer.\n+     * Compress data before writing using the LZF algorithm. This will save\n+     * about 50% of the disk space, but will slow down read and write\n+     * operations slightly.\n+     * <p>\n+     * This setting only affects writes; it is not necessary to enable\n+     * compression when reading, even if compression was enabled when\n+     * writing.\n      *\n-     * @param end the end of the chunk\n-     * @return the chunk, or null if not successful\n-     */\n-    private Chunk readChunkFooter(long end) {\n-        // the following can fail for various reasons\n-        try {\n-            // read the chunk footer of the last block of the file\n-            ByteBuffer lastBlock = fileStore.readFully(\n-                    end - Chunk.FOOTER_LENGTH, Chunk.FOOTER_LENGTH);\n-            byte[] buff = new byte[Chunk.FOOTER_LENGTH];\n-            lastBlock.get(buff);\n-            String s = new String(buff, DataUtils.LATIN).trim();\n-            HashMap<String, String> m = DataUtils.parseMap(s);\n-            int check = DataUtils.readHexInt(m, \"fletcher\", 0);\n-            m.remove(\"fletcher\");\n-            s = s.substring(0, s.lastIndexOf(\"fletcher\") - 1);\n-            byte[] bytes = s.getBytes(DataUtils.LATIN);\n-            int checksum = DataUtils.getFletcher32(bytes, bytes.length);\n-            if (check == checksum) {\n-                int chunk = DataUtils.readHexInt(m, \"chunk\", 0);\n-                Chunk c = new Chunk(chunk);\n-                c.version = DataUtils.readHexLong(m, \"version\", 0);\n-                c.block = DataUtils.readHexLong(m, \"block\", 0);\n-                return c;\n-            }\n-        } catch (Exception e) {\n-            // ignore\n-        }\n-        return null;\n-    }\n-\n-    private void writeStoreHeader() {\n-        StringBuilder buff = new StringBuilder();\n-        if (lastChunk != null) {\n-            storeHeader.put(\"block\", lastChunk.block);\n-            storeHeader.put(\"chunk\", lastChunk.id);\n-            storeHeader.put(\"version\", lastChunk.version);\n-        }\n-        DataUtils.appendMap(buff, storeHeader);\n-        byte[] bytes = buff.toString().getBytes(DataUtils.LATIN);\n-        int checksum = DataUtils.getFletcher32(bytes, bytes.length);\n-        DataUtils.appendMap(buff, \"fletcher\", checksum);\n-        buff.append(\"\\n\");\n-        bytes = buff.toString().getBytes(DataUtils.LATIN);\n-        ByteBuffer header = ByteBuffer.allocate(2 * BLOCK_SIZE);\n-        header.put(bytes);\n-        header.position(BLOCK_SIZE);\n-        header.put(bytes);\n-        header.rewind();\n-        write(0, header);\n-    }\n-\n-    private void write(long pos, ByteBuffer buffer) {\n-        try {\n-            fileStore.writeFully(pos, buffer);\n-        } catch (IllegalStateException e) {\n-            panic(e);\n-            throw e;\n-        }\n-    }\n-\n-    /**\n-     * Close the file and the store. Unsaved changes are written to disk first.\n+     * @return this\n      */\n-    public void close() {\n-        if (closed) {\n-            return;\n-        }\n-        FileStore f = fileStore;\n-        if (f != null && !f.isReadOnly()) {\n-            stopBackgroundThread();\n-            if (hasUnsavedChanges()) {\n-                commitAndSave();\n-            }\n-        }\n-        closeStore(true);\n+    public Builder compress() {\n+      return set(\"compress\", 1);\n     }\n \n     /**\n-     * Close the file and the store, without writing anything. This will stop\n-     * the background thread. This method ignores all errors.\n+     * Compress data before writing using the Deflate algorithm. This will\n+     * save more disk space, but will slow down read and write operations\n+     * quite a bit.\n+     * <p>\n+     * This setting only affects writes; it is not necessary to enable\n+     * compression when reading, even if compression was enabled when\n+     * writing.\n+     *\n+     * @return this\n      */\n-    public void closeImmediately() {\n-        try {\n-            closeStore(false);\n-        } catch (Exception e) {\n-            if (backgroundExceptionHandler != null) {\n-                backgroundExceptionHandler.uncaughtException(null, e);\n-            }\n-        }\n-    }\n-\n-    private void closeStore(boolean shrinkIfPossible) {\n-        if (closed) {\n-            return;\n-        }\n-        // can not synchronize on this yet, because\n-        // the thread also synchronized on this, which\n-        // could result in a deadlock\n-        stopBackgroundThread();\n-        closed = true;\n-        synchronized (this) {\n-            if (fileStore != null && shrinkIfPossible) {\n-                shrinkFileIfPossible(0);\n-            }\n-            // release memory early - this is important when called\n-            // because of out of memory\n-            cache = null;\n-            cacheChunkRef = null;\n-            for (MVMap<?, ?> m : New.arrayList(maps.values())) {\n-                m.close();\n-            }\n-            meta = null;\n-            chunks.clear();\n-            maps.clear();\n-            if (fileStore != null) {\n-                try {\n-                    if (!fileStoreIsProvided) {\n-                        fileStore.close();\n-                    }\n-                } finally {\n-                    fileStore = null;\n-                }\n-            }\n-        }\n+    public Builder compressHigh() {\n+      return set(\"compress\", 2);\n     }\n \n     /**\n-     * Whether the chunk at the given position is live.\n+     * Set the amount of memory a page should contain at most, in bytes,\n+     * before it is split. The default is 16 KB for persistent stores and 4\n+     * KB for in-memory stores. This is not a limit in the page size, as\n+     * pages with one entry can get larger. It is just the point where pages\n+     * that contain more than one entry are split.\n      *\n-     * @param chunkId the chunk id\n-     * @return true if it is live\n+     * @param pageSplitSize the page size\n+     * @return this\n      */\n-    boolean isChunkLive(int chunkId) {\n-        String s = meta.get(Chunk.getMetaKey(chunkId));\n-        return s != null;\n+    public Builder pageSplitSize(int pageSplitSize) {\n+      return set(\"pageSplitSize\", pageSplitSize);\n     }\n \n     /**\n-     * Get the chunk for the given position.\n+     * Set the listener to be used for exceptions that occur when writing in\n+     * the background thread.\n      *\n-     * @param pos the position\n-     * @return the chunk\n+     * @param exceptionHandler the handler\n+     * @return this\n      */\n-    private Chunk getChunk(long pos) {\n-        Chunk c = getChunkIfFound(pos);\n-        if (c == null) {\n-            int chunkId = DataUtils.getPageChunkId(pos);\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_FILE_CORRUPT,\n-                    \"Chunk {0} not found\", chunkId);\n-        }\n-        return c;\n-    }\n-\n-    private Chunk getChunkIfFound(long pos) {\n-        int chunkId = DataUtils.getPageChunkId(pos);\n-        Chunk c = chunks.get(chunkId);\n-        if (c == null) {\n-            checkOpen();\n-            if (!Thread.holdsLock(this)) {\n-                // it could also be unsynchronized metadata\n-                // access (if synchronization on this was forgotten)\n-                throw DataUtils.newIllegalStateException(\n-                        DataUtils.ERROR_CHUNK_NOT_FOUND,\n-                        \"Chunk {0} no longer exists\",\n-                        chunkId);\n-            }\n-            String s = meta.get(Chunk.getMetaKey(chunkId));\n-            if (s == null) {\n-                return null;\n-            }\n-            c = Chunk.fromString(s);\n-            if (c.block == Long.MAX_VALUE) {\n-                throw DataUtils.newIllegalStateException(\n-                        DataUtils.ERROR_FILE_CORRUPT,\n-                        \"Chunk {0} is invalid\", chunkId);\n-            }\n-            chunks.put(c.id, c);\n-        }\n-        return c;\n-    }\n-\n-    private void setWriteVersion(long version) {\n-        for (MVMap<?, ?> map : maps.values()) {\n-            map.setWriteVersion(version);\n-        }\n-        MVMap<String, String> m = meta;\n-        if (m == null) {\n-            checkOpen();\n-        }\n-        m.setWriteVersion(version);\n+    public Builder backgroundExceptionHandler(\n+        Thread.UncaughtExceptionHandler exceptionHandler) {\n+      return set(\"backgroundExceptionHandler\", exceptionHandler);\n     }\n \n     /**\n-     * Commit the changes.\n+     * Use the provided file store instead of the default one.\n      * <p>\n-     * For in-memory stores, this method increments the version.\n+     * File stores passed in this way need to be open. They are not closed\n+     * when closing the store.\n      * <p>\n-     * For persistent stores, it also writes changes to disk. It does nothing if\n-     * there are no unsaved changes, and returns the old version. It is not\n-     * necessary to call this method when auto-commit is enabled (the default\n-     * setting), as in this case it is automatically called from time to time or\n-     * when enough changes have accumulated. However, it may still be called to\n-     * flush all changes to disk.\n+     * Please note that any kind of store (including an off-heap store) is\n+     * considered a \"persistence\", while an \"in-memory store\" means objects\n+     * are not persisted and fully kept in the JVM heap.\n      *\n-     * @return the new version\n+     * @param store the file store\n+     * @return this\n      */\n-    public synchronized long commit() {\n-        if (fileStore != null) {\n-            return commitAndSave();\n-        }\n-        long v = ++currentVersion;\n-        setWriteVersion(v);\n-        return v;\n+    public Builder fileStore(FileStore store) {\n+      return set(\"fileStore\", store);\n     }\n \n     /**\n-     * Commit all changes and persist them to disk. This method does nothing if\n-     * there are no unsaved changes, otherwise it increments the current version\n-     * and stores the data (for file based stores).\n-     * <p>\n-     * At most one store operation may run at any time.\n+     * Open the store.\n      *\n-     * @return the new version (incremented if there were changes)\n+     * @return the opened store\n      */\n-    private synchronized long commitAndSave() {\n-        if (closed) {\n-            return currentVersion;\n-        }\n-        if (fileStore == null) {\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_WRITING_FAILED,\n-                    \"This is an in-memory store\");\n-        }\n-        if (currentStoreVersion >= 0) {\n-            // store is possibly called within store, if the meta map changed\n-            return currentVersion;\n-        }\n-        if (!hasUnsavedChanges()) {\n-            return currentVersion;\n-        }\n-        if (fileStore.isReadOnly()) {\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_WRITING_FAILED, \"This store is read-only\");\n-        }\n-        try {\n-            currentStoreVersion = currentVersion;\n-            currentStoreThread = Thread.currentThread();\n-            return storeNow();\n-        } finally {\n-            // in any case reset the current store version,\n-            // to allow closing the store\n-            currentStoreVersion = -1;\n-            currentStoreThread = null;\n-        }\n+    public MVStore open() {\n+      return new MVStore(config);\n     }\n \n-    private long storeNow() {\n-        try {\n-            return storeNowTry();\n-        } catch (IllegalStateException e) {\n-            panic(e);\n-            return -1;\n-        }\n+    @Override\n+    public String toString() {\n+      return DataUtils.appendMap(new StringBuilder(), config).toString();\n     }\n \n-    private long storeNowTry() {\n-        long time = getTimeSinceCreation();\n-        int freeDelay = retentionTime / 10;\n-        if (time >= lastFreeUnusedChunks + freeDelay) {\n-            // set early in case it fails (out of memory or so)\n-            lastFreeUnusedChunks = time;\n-            freeUnusedChunks();\n-            // set it here as well, to avoid calling it often if it was slow\n-            lastFreeUnusedChunks = getTimeSinceCreation();\n-        }\n-        int currentUnsavedPageCount = unsavedMemory;\n-        long storeVersion = currentStoreVersion;\n-        long version = ++currentVersion;\n-        lastCommitTime = time;\n-        retainChunk = null;\n-\n-        // the metadata of the last chunk was not stored so far, and needs to be\n-        // set now (it's better not to update right after storing, because that\n-        // would modify the meta map again)\n-        int lastChunkId;\n-        if (lastChunk == null) {\n-            lastChunkId = 0;\n-        } else {\n-            lastChunkId = lastChunk.id;\n-            meta.put(Chunk.getMetaKey(lastChunkId), lastChunk.asString());\n-            // never go backward in time\n-            time = Math.max(lastChunk.time, time);\n-        }\n-        int newChunkId = lastChunkId;\n-        while (true) {\n-            newChunkId = (newChunkId + 1) % Chunk.MAX_ID;\n-            Chunk old = chunks.get(newChunkId);\n-            if (old == null) {\n-                break;\n-            }\n-            if (old.block == Long.MAX_VALUE) {\n-                IllegalStateException e = DataUtils.newIllegalStateException(\n-                        DataUtils.ERROR_INTERNAL,\n-                        \"Last block not stored, possibly due to out-of-memory\");\n-                panic(e);\n-            }\n-        }\n-        Chunk c = new Chunk(newChunkId);\n-\n-        c.pageCount = Integer.MAX_VALUE;\n-        c.pageCountLive = Integer.MAX_VALUE;\n-        c.maxLen = Long.MAX_VALUE;\n-        c.maxLenLive = Long.MAX_VALUE;\n-        c.metaRootPos = Long.MAX_VALUE;\n-        c.block = Long.MAX_VALUE;\n-        c.len = Integer.MAX_VALUE;\n-        c.time = time;\n-        c.version = version;\n-        c.mapId = lastMapId;\n-        c.next = Long.MAX_VALUE;\n-        chunks.put(c.id, c);\n-        // force a metadata update\n-        meta.put(Chunk.getMetaKey(c.id), c.asString());\n-        meta.remove(Chunk.getMetaKey(c.id));\n-        ArrayList<MVMap<?, ?>> list = New.arrayList(maps.values());\n-        ArrayList<MVMap<?, ?>> changed = New.arrayList();\n-        for (MVMap<?, ?> m : list) {\n-            m.setWriteVersion(version);\n-            long v = m.getVersion();\n-            if (m.getCreateVersion() > storeVersion) {\n-                // the map was created after storing started\n-                continue;\n-            }\n-            if (m.isVolatile()) {\n-                continue;\n-            }\n-            if (v >= 0 && v >= lastStoredVersion) {\n-                MVMap<?, ?> r = m.openVersion(storeVersion);\n-                if (r.getRoot().getPos() == 0) {\n-                    changed.add(r);\n-                }\n-            }\n-        }\n-        applyFreedSpace(storeVersion);\n-        WriteBuffer buff = getWriteBuffer();\n-        // need to patch the header later\n-        c.writeChunkHeader(buff, 0);\n-        int headerLength = buff.position();\n-        c.pageCount = 0;\n-        c.pageCountLive = 0;\n-        c.maxLen = 0;\n-        c.maxLenLive = 0;\n-        for (MVMap<?, ?> m : changed) {\n-            Page p = m.getRoot();\n-            String key = MVMap.getMapRootKey(m.getId());\n-            if (p.getTotalCount() == 0) {\n-                meta.put(key, \"0\");\n-            } else {\n-                p.writeUnsavedRecursive(c, buff);\n-                long root = p.getPos();\n-                meta.put(key, Long.toHexString(root));\n-            }\n-        }\n-        meta.setWriteVersion(version);\n-\n-        Page metaRoot = meta.getRoot();\n-        metaRoot.writeUnsavedRecursive(c, buff);\n-\n-        int chunkLength = buff.position();\n-\n-        // add the store header and round to the next block\n-        int length = MathUtils.roundUpInt(chunkLength +\n-                Chunk.FOOTER_LENGTH, BLOCK_SIZE);\n-        buff.limit(length);\n-\n-        // the length of the file that is still in use\n-        // (not necessarily the end of the file)\n-        long end = getFileLengthInUse();\n-        long filePos;\n-        if (reuseSpace) {\n-            filePos = fileStore.allocate(length);\n-        } else {\n-            filePos = end;\n-        }\n-        // end is not necessarily the end of the file\n-        boolean storeAtEndOfFile = filePos + length >= fileStore.size();\n-\n-        if (!reuseSpace) {\n-            // we can not mark it earlier, because it\n-            // might have been allocated by one of the\n-            // removed chunks\n-            fileStore.markUsed(end, length);\n-        }\n+    /**\n+     * Read the configuration from a string.\n+     *\n+     * @param s the string representation\n+     * @return the builder\n+     */\n+    public static Builder fromString(String s) {\n+      HashMap<String, String> config = DataUtils.parseMap(s);\n+      Builder builder = new Builder();\n+      builder.config.putAll(config);\n+      return builder;\n+    }\n \n-        c.block = filePos / BLOCK_SIZE;\n-        c.len = length / BLOCK_SIZE;\n-        c.metaRootPos = metaRoot.getPos();\n-        // calculate and set the likely next position\n-        if (reuseSpace) {\n-            int predictBlocks = c.len;\n-            long predictedNextStart = fileStore.allocate(\n-                    predictBlocks * BLOCK_SIZE);\n-            fileStore.free(predictedNextStart, predictBlocks * BLOCK_SIZE);\n-            c.next = predictedNextStart / BLOCK_SIZE;\n-        } else {\n-            // just after this chunk\n-            c.next = 0;\n-        }\n-        buff.position(0);\n-        c.writeChunkHeader(buff, headerLength);\n-        revertTemp(storeVersion);\n-\n-        buff.position(buff.limit() - Chunk.FOOTER_LENGTH);\n-        buff.put(c.getFooterBytes());\n-\n-        buff.position(0);\n-        write(filePos, buff.getBuffer());\n-        releaseWriteBuffer(buff);\n-\n-        // whether we need to write the store header\n-        boolean writeStoreHeader = false;\n-        if (!storeAtEndOfFile) {\n-            if (lastChunk == null) {\n-                writeStoreHeader = true;\n-            } else if (lastChunk.next != c.block) {\n-                // the last prediction did not matched\n-                writeStoreHeader = true;\n-            } else {\n-                long headerVersion = DataUtils.readHexLong(\n-                        storeHeader, \"version\", 0);\n-                if (lastChunk.version - headerVersion > 20) {\n-                    // we write after at least 20 entries\n-                    writeStoreHeader = true;\n-                } else {\n-                    int chunkId = DataUtils.readHexInt(storeHeader, \"chunk\", 0);\n-                    while (true) {\n-                        Chunk old = chunks.get(chunkId);\n-                        if (old == null) {\n-                            // one of the chunks in between\n-                            // was removed\n-                            writeStoreHeader = true;\n-                            break;\n-                        }\n-                        if (chunkId == lastChunk.id) {\n-                            break;\n-                        }\n-                        chunkId++;\n-                    }\n-                }\n-            }\n-        }\n-\n-        lastChunk = c;\n-        if (writeStoreHeader) {\n-            writeStoreHeader();\n-        }\n-        if (!storeAtEndOfFile) {\n-            // may only shrink after the store header was written\n-            shrinkFileIfPossible(1);\n-        }\n-        for (MVMap<?, ?> m : changed) {\n-            Page p = m.getRoot();\n-            if (p.getTotalCount() > 0) {\n-                p.writeEnd();\n-            }\n-        }\n-        metaRoot.writeEnd();\n-\n-        // some pages might have been changed in the meantime (in the newest\n-        // version)\n-        unsavedMemory = Math.max(0, unsavedMemory\n-                - currentUnsavedPageCount);\n-\n-        metaChanged = false;\n-        lastStoredVersion = storeVersion;\n-\n-        return version;\n-    }\n-\n-    private synchronized void freeUnusedChunks() {\n-        if (lastChunk == null || !reuseSpace) {\n-            return;\n-        }\n-        Set<Integer> referenced = collectReferencedChunks();\n-        ArrayList<Chunk> free = New.arrayList();\n-        long time = getTimeSinceCreation();\n-        for (Chunk c : chunks.values()) {\n-            if (!referenced.contains(c.id)) {\n-                free.add(c);\n-            }\n-        }\n-        for (Chunk c : free) {\n-            if (canOverwriteChunk(c, time)) {\n-                chunks.remove(c.id);\n-                markMetaChanged();\n-                meta.remove(Chunk.getMetaKey(c.id));\n-                long start = c.block * BLOCK_SIZE;\n-                int length = c.len * BLOCK_SIZE;\n-                fileStore.free(start, length);\n-            } else {\n-                if (c.unused == 0) {\n-                    c.unused = time;\n-                    meta.put(Chunk.getMetaKey(c.id), c.asString());\n-                    markMetaChanged();\n-                }\n-            }\n-        }\n-    }\n-\n-    private Set<Integer> collectReferencedChunks() {\n-        long testVersion = lastChunk.version;\n-        DataUtils.checkArgument(testVersion > 0, \"Collect references on version 0\");\n-        long readCount = getFileStore().readCount;\n-        Set<Integer> referenced = New.hashSet();\n-        for (Cursor<String, String> c = meta.cursor(\"root.\"); c.hasNext();) {\n-            String key = c.next();\n-            if (!key.startsWith(\"root.\")) {\n-                break;\n-            }\n-            long pos = DataUtils.parseHexLong(c.getValue());\n-            if (pos == 0) {\n-                continue;\n-            }\n-            int mapId = DataUtils.parseHexInt(key.substring(\"root.\".length()));\n-            collectReferencedChunks(referenced, mapId, pos, 0);\n-        }\n-        long pos = lastChunk.metaRootPos;\n-        collectReferencedChunks(referenced, 0, pos, 0);\n-        readCount = fileStore.readCount - readCount;\n-        return referenced;\n-    }\n-\n-    private void collectReferencedChunks(Set<Integer> targetChunkSet,\n-            int mapId, long pos, int level) {\n-        int c = DataUtils.getPageChunkId(pos);\n-        targetChunkSet.add(c);\n-        if (DataUtils.getPageType(pos) == DataUtils.PAGE_TYPE_LEAF) {\n-            return;\n-        }\n-        PageChildren refs = readPageChunkReferences(mapId, pos, -1);\n-        if (!refs.chunkList) {\n-            Set<Integer> target = New.hashSet();\n-            for (int i = 0; i < refs.children.length; i++) {\n-                long p = refs.children[i];\n-                collectReferencedChunks(target, mapId, p, level + 1);\n-            }\n-            // we don't need a reference to this chunk\n-            target.remove(c);\n-            long[] children = new long[target.size()];\n-            int i = 0;\n-            for (Integer p : target) {\n-                children[i++] = DataUtils.getPagePos(p, 0, 0,\n-                        DataUtils.PAGE_TYPE_LEAF);\n-            }\n-            refs.children = children;\n-            refs.chunkList = true;\n-            if (cacheChunkRef != null) {\n-                cacheChunkRef.put(refs.pos, refs, refs.getMemory());\n-            }\n-        }\n-        for (long p : refs.children) {\n-            targetChunkSet.add(DataUtils.getPageChunkId(p));\n-        }\n-    }\n-\n-    private PageChildren readPageChunkReferences(int mapId, long pos, int parentChunk) {\n-        if (DataUtils.getPageType(pos) == DataUtils.PAGE_TYPE_LEAF) {\n-            return null;\n-        }\n-        PageChildren r;\n-        if (cacheChunkRef != null) {\n-            r = cacheChunkRef.get(pos);\n-        } else {\n-            r = null;\n-        }\n-        if (r == null) {\n-            // if possible, create it from the cached page\n-            if (cache != null) {\n-                Page p = cache.get(pos);\n-                if (p != null) {\n-                    r = new PageChildren(p);\n-                }\n-            }\n-            if (r == null) {\n-                // page was not cached: read the data\n-                Chunk c = getChunk(pos);\n-                long filePos = c.block * BLOCK_SIZE;\n-                filePos += DataUtils.getPageOffset(pos);\n-                if (filePos < 0) {\n-                    throw DataUtils.newIllegalStateException(\n-                            DataUtils.ERROR_FILE_CORRUPT,\n-                            \"Negative position {0}; p={1}, c={2}\", filePos, pos, c.toString());\n-                }\n-                long maxPos = (c.block + c.len) * BLOCK_SIZE;\n-                r = PageChildren.read(fileStore, pos, mapId, filePos, maxPos);\n-            }\n-            r.removeDuplicateChunkReferences();\n-            if (cacheChunkRef != null) {\n-                cacheChunkRef.put(pos, r, r.getMemory());\n-            }\n-        }\n-        if (r.children.length == 0) {\n-            int chunk = DataUtils.getPageChunkId(pos);\n-            if (chunk == parentChunk) {\n-                return null;\n-            }\n-        }\n-        return r;\n-    }\n-\n-    /**\n-     * Get a buffer for writing. This caller must synchronize on the store\n-     * before calling the method and until after using the buffer.\n-     *\n-     * @return the buffer\n-     */\n-    private WriteBuffer getWriteBuffer() {\n-        WriteBuffer buff;\n-        if (writeBuffer != null) {\n-            buff = writeBuffer;\n-            buff.clear();\n-        } else {\n-            buff = new WriteBuffer();\n-        }\n-        return buff;\n-    }\n-\n-    /**\n-     * Release a buffer for writing. This caller must synchronize on the store\n-     * before calling the method and until after using the buffer.\n-     *\n-     * @param buff the buffer than can be re-used\n-     */\n-    private void releaseWriteBuffer(WriteBuffer buff) {\n-        if (buff.capacity() <= 4 * 1024 * 1024) {\n-            writeBuffer = buff;\n-        }\n-    }\n-\n-    private boolean canOverwriteChunk(Chunk c, long time) {\n-        if (retentionTime >= 0) {\n-            if (c.time + retentionTime > time) {\n-                return false;\n-            }\n-            if (c.unused == 0 || c.unused + retentionTime / 2 > time) {\n-                return false;\n-            }\n-        }\n-        Chunk r = retainChunk;\n-        if (r != null && c.version > r.version) {\n-            return false;\n-        }\n-        return true;\n-    }\n-\n-    private long getTimeSinceCreation() {\n-        return Math.max(0, getTimeAbsolute() - creationTime);\n-    }\n-\n-    private long getTimeAbsolute() {\n-        long now = System.currentTimeMillis();\n-        if (lastTimeAbsolute != 0 && now < lastTimeAbsolute) {\n-            // time seems to have run backwards - this can happen\n-            // when the system time is adjusted, for example\n-            // on a leap second\n-            now = lastTimeAbsolute;\n-        } else {\n-            lastTimeAbsolute = now;\n-        }\n-        return now;\n-    }\n-\n-    /**\n-     * Apply the freed space to the chunk metadata. The metadata is updated, but\n-     * completely free chunks are not removed from the set of chunks, and the\n-     * disk space is not yet marked as free.\n-     *\n-     * @param storeVersion apply up to the given version\n-     */\n-    private void applyFreedSpace(long storeVersion) {\n-        while (true) {\n-            ArrayList<Chunk> modified = New.arrayList();\n-            Iterator<Entry<Long, HashMap<Integer, Chunk>>> it;\n-            it = freedPageSpace.entrySet().iterator();\n-            while (it.hasNext()) {\n-                Entry<Long, HashMap<Integer, Chunk>> e = it.next();\n-                long v = e.getKey();\n-                if (v > storeVersion) {\n-                    continue;\n-                }\n-                HashMap<Integer, Chunk> freed = e.getValue();\n-                for (Chunk f : freed.values()) {\n-                    Chunk c = chunks.get(f.id);\n-                    if (c == null) {\n-                        // already removed\n-                        continue;\n-                    }\n-                    // no need to synchronize, as old entries\n-                    // are not concurrently modified\n-                    c.maxLenLive += f.maxLenLive;\n-                    c.pageCountLive += f.pageCountLive;\n-                    if (c.pageCountLive < 0 && c.pageCountLive > -MARKED_FREE) {\n-                        // can happen after a rollback\n-                        c.pageCountLive = 0;\n-                    }\n-                    if (c.maxLenLive < 0 && c.maxLenLive > -MARKED_FREE) {\n-                        // can happen after a rollback\n-                        c.maxLenLive = 0;\n-                    }\n-                    modified.add(c);\n-                }\n-                it.remove();\n-            }\n-            for (Chunk c : modified) {\n-                meta.put(Chunk.getMetaKey(c.id), c.asString());\n-            }\n-            if (modified.size() == 0) {\n-                break;\n-            }\n-        }\n-    }\n-\n-    /**\n-     * Shrink the file if possible, and if at least a given percentage can be\n-     * saved.\n-     *\n-     * @param minPercent the minimum percentage to save\n-     */\n-    private void shrinkFileIfPossible(int minPercent) {\n-        if (fileStore.isReadOnly()) {\n-            return;\n-        }\n-        long end = getFileLengthInUse();\n-        long fileSize = fileStore.size();\n-        if (end >= fileSize) {\n-            return;\n-        }\n-        if (minPercent > 0 && fileSize - end < BLOCK_SIZE) {\n-            return;\n-        }\n-        int savedPercent = (int) (100 - (end * 100 / fileSize));\n-        if (savedPercent < minPercent) {\n-            return;\n-        }\n-        if (!closed) {\n-            sync();\n-        }\n-        fileStore.truncate(end);\n-    }\n-\n-    /**\n-     * Get the position of the last used byte.\n-     *\n-     * @return the position\n-     */\n-    private long getFileLengthInUse() {\n-        long size = 2 * BLOCK_SIZE;\n-        for (Chunk c : chunks.values()) {\n-            if (c.len != Integer.MAX_VALUE) {\n-                long x = (c.block + c.len) * BLOCK_SIZE;\n-                size = Math.max(size, x);\n-            }\n-        }\n-        return size;\n-    }\n-\n-    /**\n-     * Check whether there are any unsaved changes.\n-     *\n-     * @return if there are any changes\n-     */\n-    public boolean hasUnsavedChanges() {\n-        checkOpen();\n-        if (metaChanged) {\n-            return true;\n-        }\n-        for (MVMap<?, ?> m : maps.values()) {\n-            if (!m.isClosed()) {\n-                long v = m.getVersion();\n-                if (v >= 0 && v > lastStoredVersion) {\n-                    return true;\n-                }\n-            }\n-        }\n-        return false;\n-    }\n-\n-    private Chunk readChunkHeader(long block) {\n-        long p = block * BLOCK_SIZE;\n-        ByteBuffer buff = fileStore.readFully(p, Chunk.MAX_HEADER_LENGTH);\n-        return Chunk.readChunkHeader(buff, p);\n-    }\n-\n-    /**\n-     * Compact the store by moving all live pages to new chunks.\n-     *\n-     * @return if anything was written\n-     */\n-    public synchronized boolean compactRewriteFully() {\n-        checkOpen();\n-        if (lastChunk == null) {\n-            // nothing to do\n-            return false;\n-        }\n-        for (MVMap<?, ?> m : maps.values()) {\n-            @SuppressWarnings(\"unchecked\")\n-            MVMap<Object, Object> map = (MVMap<Object, Object>) m;\n-            Cursor<Object, Object> cursor = map.cursor(null);\n-            Page lastPage = null;\n-            while (cursor.hasNext()) {\n-                cursor.next();\n-                Page p = cursor.getPage();\n-                if (p == lastPage) {\n-                    continue;\n-                }\n-                Object k = p.getKey(0);\n-                Object v = p.getValue(0);\n-                map.put(k, v);\n-                lastPage = p;\n-            }\n-        }\n-        commitAndSave();\n-        return true;\n-    }\n-\n-    /**\n-     * Compact by moving all chunks next to each other.\n-     *\n-     * @return if anything was written\n-     */\n-    public synchronized boolean compactMoveChunks() {\n-        return compactMoveChunks(100, Long.MAX_VALUE);\n-    }\n-\n-    /**\n-     * Compact the store by moving all chunks next to each other, if there is\n-     * free space between chunks. This might temporarily increase the file size.\n-     * Chunks are overwritten irrespective of the current retention time. Before\n-     * overwriting chunks and before resizing the file, syncFile() is called.\n-     *\n-     * @param targetFillRate do nothing if the file store fill rate is higher\n-     *            than this\n-     * @param moveSize the number of bytes to move\n-     * @return if anything was written\n-     */\n-    public synchronized boolean compactMoveChunks(int targetFillRate, long moveSize) {\n-        checkOpen();\n-        if (lastChunk == null || !reuseSpace) {\n-            // nothing to do\n-            return false;\n-        }\n-        int oldRetentionTime = retentionTime;\n-        boolean oldReuse = reuseSpace;\n-        try {\n-            retentionTime = -1;\n-            freeUnusedChunks();\n-            if (fileStore.getFillRate() > targetFillRate) {\n-                return false;\n-            }\n-            long start = fileStore.getFirstFree() / BLOCK_SIZE;\n-            ArrayList<Chunk> move = compactGetMoveBlocks(start, moveSize);\n-            compactMoveChunks(move);\n-            freeUnusedChunks();\n-            storeNow();\n-        } finally {\n-            reuseSpace = oldReuse;\n-            retentionTime = oldRetentionTime;\n-        }\n-        return true;\n-    }\n-\n-    private ArrayList<Chunk> compactGetMoveBlocks(long startBlock, long moveSize) {\n-        ArrayList<Chunk> move = New.arrayList();\n-        for (Chunk c : chunks.values()) {\n-            if (c.block > startBlock) {\n-                move.add(c);\n-            }\n-        }\n-        // sort by block\n-        Collections.sort(move, new Comparator<Chunk>() {\n-            @Override\n-            public int compare(Chunk o1, Chunk o2) {\n-                return Long.signum(o1.block - o2.block);\n-            }\n-        });\n-        // find which is the last block to keep\n-        int count = 0;\n-        long size = 0;\n-        for (Chunk c : move) {\n-            long chunkSize = c.len * (long) BLOCK_SIZE;\n-            if (size + chunkSize > moveSize) {\n-                break;\n-            }\n-            size += chunkSize;\n-            count++;\n-        }\n-        // move the first block (so the first gap is moved),\n-        // and the one at the end (so the file shrinks)\n-        while (move.size() > count && move.size() > 1) {\n-            move.remove(1);\n-        }\n-\n-        return move;\n-    }\n-\n-    private void compactMoveChunks(ArrayList<Chunk> move) {\n-        for (Chunk c : move) {\n-            WriteBuffer buff = getWriteBuffer();\n-            long start = c.block * BLOCK_SIZE;\n-            int length = c.len * BLOCK_SIZE;\n-            buff.limit(length);\n-            ByteBuffer readBuff = fileStore.readFully(start, length);\n-            Chunk.readChunkHeader(readBuff, start);\n-            int chunkHeaderLen = readBuff.position();\n-            buff.position(chunkHeaderLen);\n-            buff.put(readBuff);\n-            long end = getFileLengthInUse();\n-            fileStore.markUsed(end, length);\n-            fileStore.free(start, length);\n-            c.block = end / BLOCK_SIZE;\n-            c.next = 0;\n-            buff.position(0);\n-            c.writeChunkHeader(buff, chunkHeaderLen);\n-            buff.position(length - Chunk.FOOTER_LENGTH);\n-            buff.put(c.getFooterBytes());\n-            buff.position(0);\n-            write(end, buff.getBuffer());\n-            releaseWriteBuffer(buff);\n-            markMetaChanged();\n-            meta.put(Chunk.getMetaKey(c.id), c.asString());\n-        }\n-\n-        // update the metadata (store at the end of the file)\n-        reuseSpace = false;\n-        commitAndSave();\n-        sync();\n-\n-        // now re-use the empty space\n-        reuseSpace = true;\n-        for (Chunk c : move) {\n-            if (!chunks.containsKey(c.id)) {\n-                // already removed during the\n-                // previous store operation\n-                continue;\n-            }\n-            WriteBuffer buff = getWriteBuffer();\n-            long start = c.block * BLOCK_SIZE;\n-            int length = c.len * BLOCK_SIZE;\n-            buff.limit(length);\n-            ByteBuffer readBuff = fileStore.readFully(start, length);\n-            Chunk.readChunkHeader(readBuff, 0);\n-            int chunkHeaderLen = readBuff.position();\n-            buff.position(chunkHeaderLen);\n-            buff.put(readBuff);\n-            long pos = fileStore.allocate(length);\n-            fileStore.free(start, length);\n-            buff.position(0);\n-            c.block = pos / BLOCK_SIZE;\n-            c.writeChunkHeader(buff, chunkHeaderLen);\n-            buff.position(length - Chunk.FOOTER_LENGTH);\n-            buff.put(c.getFooterBytes());\n-            buff.position(0);\n-            write(pos, buff.getBuffer());\n-            releaseWriteBuffer(buff);\n-            markMetaChanged();\n-            meta.put(Chunk.getMetaKey(c.id), c.asString());\n-        }\n-\n-        // update the metadata (within the file)\n-        commitAndSave();\n-        sync();\n-        shrinkFileIfPossible(0);\n-    }\n-\n-    /**\n-     * Force all stored changes to be written to the storage. The default\n-     * implementation calls FileChannel.force(true).\n-     */\n-    public void sync() {\n-        checkOpen();\n-        FileStore f = fileStore;\n-        if (f != null) {\n-            f.sync();\n-        }\n-    }\n-\n-    /**\n-     * Try to increase the fill rate by re-writing partially full chunks. Chunks\n-     * with a low number of live items are re-written.\n-     * <p>\n-     * If the current fill rate is higher than the target fill rate, nothing is\n-     * done.\n-     * <p>\n-     * Please note this method will not necessarily reduce the file size, as\n-     * empty chunks are not overwritten.\n-     * <p>\n-     * Only data of open maps can be moved. For maps that are not open, the old\n-     * chunk is still referenced. Therefore, it is recommended to open all maps\n-     * before calling this method.\n-     *\n-     * @param targetFillRate the minimum percentage of live entries\n-     * @param write the minimum number of bytes to write\n-     * @return if a chunk was re-written\n-     */\n-    public boolean compact(int targetFillRate, int write) {\n-        if (!reuseSpace) {\n-            return false;\n-        }\n-        synchronized (compactSync) {\n-            checkOpen();\n-            ArrayList<Chunk> old;\n-            synchronized (this) {\n-                old = compactGetOldChunks(targetFillRate, write);\n-            }\n-            if (old == null || old.size() == 0) {\n-                return false;\n-            }\n-            compactRewrite(old);\n-            return true;\n-        }\n-    }\n-\n-    private ArrayList<Chunk> compactGetOldChunks(int targetFillRate, int write) {\n-        if (lastChunk == null) {\n-            // nothing to do\n-            return null;\n-        }\n-\n-        // calculate the fill rate\n-        long maxLengthSum = 0;\n-        long maxLengthLiveSum = 0;\n-\n-        long time = getTimeSinceCreation();\n-\n-        for (Chunk c : chunks.values()) {\n-            // ignore young chunks, because we don't optimize those\n-            if (c.time + retentionTime > time) {\n-                continue;\n-            }\n-            maxLengthSum += c.maxLen;\n-            maxLengthLiveSum += c.maxLenLive;\n-        }\n-        if (maxLengthLiveSum < 0) {\n-            // no old data\n-            return null;\n-        }\n-        // the fill rate of all chunks combined\n-        if (maxLengthSum <= 0) {\n-            // avoid division by 0\n-            maxLengthSum = 1;\n-        }\n-        int fillRate = (int) (100 * maxLengthLiveSum / maxLengthSum);\n-        if (fillRate >= targetFillRate) {\n-            return null;\n-        }\n-\n-        // the 'old' list contains the chunks we want to free up\n-        ArrayList<Chunk> old = New.arrayList();\n-        Chunk last = chunks.get(lastChunk.id);\n-        for (Chunk c : chunks.values()) {\n-            // only look at chunk older than the retention time\n-            // (it's possible to compact chunks earlier, but right\n-            // now we don't do that)\n-            if (c.time + retentionTime > time) {\n-                continue;\n-            }\n-            long age = last.version - c.version + 1;\n-            c.collectPriority = (int) (c.getFillRate() * 1000 / age);\n-            old.add(c);\n-        }\n-        if (old.size() == 0) {\n-            return null;\n-        }\n-\n-        // sort the list, so the first entry should be collected first\n-        Collections.sort(old, new Comparator<Chunk>() {\n-            @Override\n-            public int compare(Chunk o1, Chunk o2) {\n-                int comp = new Integer(o1.collectPriority).\n-                        compareTo(o2.collectPriority);\n-                if (comp == 0) {\n-                    comp = new Long(o1.maxLenLive).\n-                        compareTo(o2.maxLenLive);\n-                }\n-                return comp;\n-            }\n-        });\n-        // find out up to were in the old list we need to move\n-        long written = 0;\n-        int chunkCount = 0;\n-        Chunk move = null;\n-        for (Chunk c : old) {\n-            if (move != null) {\n-                if (c.collectPriority > 0 && written > write) {\n-                    break;\n-                }\n-            }\n-            written += c.maxLenLive;\n-            chunkCount++;\n-            move = c;\n-        }\n-        if (chunkCount < 1) {\n-            return null;\n-        }\n-        // remove the chunks we want to keep from this list\n-        boolean remove = false;\n-        for (Iterator<Chunk> it = old.iterator(); it.hasNext();) {\n-            Chunk c = it.next();\n-            if (move == c) {\n-                remove = true;\n-            } else if (remove) {\n-                it.remove();\n-            }\n-        }\n-        return old;\n-    }\n-\n-    private void compactRewrite(ArrayList<Chunk> old) {\n-        HashSet<Integer> set = New.hashSet();\n-        for (Chunk c : old) {\n-            set.add(c.id);\n-        }\n-        for (MVMap<?, ?> m : maps.values()) {\n-            @SuppressWarnings(\"unchecked\")\n-            MVMap<Object, Object> map = (MVMap<Object, Object>) m;\n-            if (!map.rewrite(set)) {\n-                return;\n-            }\n-        }\n-        if (!meta.rewrite(set)) {\n-            return;\n-        }\n-        freeUnusedChunks();\n-        commitAndSave();\n-    }\n-\n-    /**\n-     * Read a page.\n-     *\n-     * @param map the map\n-     * @param pos the page position\n-     * @return the page\n-     */\n-    Page readPage(MVMap<?, ?> map, long pos) {\n-        if (pos == 0) {\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_FILE_CORRUPT, \"Position 0\");\n-        }\n-        Page p = cache == null ? null : cache.get(pos);\n-        if (p == null) {\n-            Chunk c = getChunk(pos);\n-            long filePos = c.block * BLOCK_SIZE;\n-            filePos += DataUtils.getPageOffset(pos);\n-            if (filePos < 0) {\n-                throw DataUtils.newIllegalStateException(\n-                        DataUtils.ERROR_FILE_CORRUPT,\n-                        \"Negative position {0}\", filePos);\n-            }\n-            long maxPos = (c.block + c.len) * BLOCK_SIZE;\n-            p = Page.read(fileStore, pos, map, filePos, maxPos);\n-            cachePage(pos, p, p.getMemory());\n-        }\n-        return p;\n-    }\n-\n-    /**\n-     * Remove a page.\n-     *\n-     * @param map the map the page belongs to\n-     * @param pos the position of the page\n-     * @param memory the memory usage\n-     */\n-    void removePage(MVMap<?, ?> map, long pos, int memory) {\n-        // we need to keep temporary pages,\n-        // to support reading old versions and rollback\n-        if (pos == 0) {\n-            // the page was not yet stored:\n-            // just using \"unsavedMemory -= memory\" could result in negative\n-            // values, because in some cases a page is allocated, but never\n-            // stored, so we need to use max\n-            unsavedMemory = Math.max(0, unsavedMemory - memory);\n-            return;\n-        }\n-\n-        // This could result in a cache miss if the operation is rolled back,\n-        // but we don't optimize for rollback.\n-        // We could also keep the page in the cache, as somebody\n-        // could still read it (reading the old version).\n-        if (cache != null) {\n-            if (DataUtils.getPageType(pos) == DataUtils.PAGE_TYPE_LEAF) {\n-                // keep nodes in the cache, because they are still used for\n-                // garbage collection\n-                cache.remove(pos);\n-            }\n-        }\n-\n-        Chunk c = getChunk(pos);\n-        long version = currentVersion;\n-        if (map == meta && currentStoreVersion >= 0) {\n-            if (Thread.currentThread() == currentStoreThread) {\n-                // if the meta map is modified while storing,\n-                // then this freed page needs to be registered\n-                // with the stored chunk, so that the old chunk\n-                // can be re-used\n-                version = currentStoreVersion;\n-            }\n-        }\n-        registerFreePage(version, c.id,\n-                DataUtils.getPageMaxLength(pos), 1);\n-    }\n-\n-    private void registerFreePage(long version, int chunkId,\n-            long maxLengthLive, int pageCount) {\n-        HashMap<Integer, Chunk> freed = freedPageSpace.get(version);\n-        if (freed == null) {\n-            freed = New.hashMap();\n-            HashMap<Integer, Chunk> f2 = freedPageSpace.putIfAbsent(version,\n-                    freed);\n-            if (f2 != null) {\n-                freed = f2;\n-            }\n-        }\n-        // synchronize, because pages could be freed concurrently\n-        synchronized (freed) {\n-            Chunk f = freed.get(chunkId);\n-            if (f == null) {\n-                f = new Chunk(chunkId);\n-                freed.put(chunkId, f);\n-            }\n-            f.maxLenLive -= maxLengthLive;\n-            f.pageCountLive -= pageCount;\n-        }\n-    }\n-\n-    Compressor getCompressorFast() {\n-        if (compressorFast == null) {\n-            compressorFast = new CompressLZF();\n-        }\n-        return compressorFast;\n-    }\n-\n-    Compressor getCompressorHigh() {\n-        if (compressorHigh == null) {\n-            compressorHigh = new CompressDeflate();\n-        }\n-        return compressorHigh;\n-    }\n-\n-    int getCompressionLevel() {\n-        return compressionLevel;\n-    }\n-\n-    public int getPageSplitSize() {\n-        return pageSplitSize;\n-    }\n-\n-    public boolean getReuseSpace() {\n-        return reuseSpace;\n-    }\n-\n-    /**\n-     * Whether empty space in the file should be re-used. If enabled, old data\n-     * is overwritten (default). If disabled, writes are appended at the end of\n-     * the file.\n-     * <p>\n-     * This setting is specially useful for online backup. To create an online\n-     * backup, disable this setting, then copy the file (starting at the\n-     * beginning of the file). In this case, concurrent backup and write\n-     * operations are possible (obviously the backup process needs to be faster\n-     * than the write operations).\n-     *\n-     * @param reuseSpace the new value\n-     */\n-    public void setReuseSpace(boolean reuseSpace) {\n-        this.reuseSpace = reuseSpace;\n-    }\n-\n-    public int getRetentionTime() {\n-        return retentionTime;\n-    }\n-\n-    /**\n-     * How long to retain old, persisted chunks, in milliseconds. Chunks that\n-     * are older may be overwritten once they contain no live data.\n-     * <p>\n-     * The default value is 45000 (45 seconds) when using the default file\n-     * store. It is assumed that a file system and hard disk will flush all\n-     * write buffers within this time. Using a lower value might be dangerous,\n-     * unless the file system and hard disk flush the buffers earlier. To\n-     * manually flush the buffers, use\n-     * <code>MVStore.getFile().force(true)</code>, however please note that\n-     * according to various tests this does not always work as expected\n-     * depending on the operating system and hardware.\n-     * <p>\n-     * The retention time needs to be long enough to allow reading old chunks\n-     * while traversing over the entries of a map.\n-     * <p>\n-     * This setting is not persisted.\n-     *\n-     * @param ms how many milliseconds to retain old chunks (0 to overwrite them\n-     *            as early as possible)\n-     */\n-    public void setRetentionTime(int ms) {\n-        this.retentionTime = ms;\n-    }\n-\n-    /**\n-     * How many versions to retain for in-memory stores. If not set, 5 old\n-     * versions are retained.\n-     *\n-     * @param count the number of versions to keep\n-     */\n-    public void setVersionsToKeep(int count) {\n-        this.versionsToKeep = count;\n-    }\n-\n-    /**\n-     * Get the oldest version to retain in memory (for in-memory stores).\n-     *\n-     * @return the version\n-     */\n-    public long getVersionsToKeep() {\n-        return versionsToKeep;\n-    }\n-\n-    /**\n-     * Get the oldest version to retain in memory, which is the manually set\n-     * retain version, or the current store version (whatever is older).\n-     *\n-     * @return the version\n-     */\n-    long getOldestVersionToKeep() {\n-        long v = currentVersion;\n-        if (fileStore == null) {\n-            return v - versionsToKeep;\n-        }\n-        long storeVersion = currentStoreVersion;\n-        if (storeVersion > -1) {\n-            v = Math.min(v, storeVersion);\n-        }\n-        return v;\n-    }\n-\n-    /**\n-     * Check whether all data can be read from this version. This requires that\n-     * all chunks referenced by this version are still available (not\n-     * overwritten).\n-     *\n-     * @param version the version\n-     * @return true if all data can be read\n-     */\n-    private boolean isKnownVersion(long version) {\n-        if (version > currentVersion || version < 0) {\n-            return false;\n-        }\n-        if (version == currentVersion || chunks.size() == 0) {\n-            // no stored data\n-            return true;\n-        }\n-        // need to check if a chunk for this version exists\n-        Chunk c = getChunkForVersion(version);\n-        if (c == null) {\n-            return false;\n-        }\n-        // also, all chunks referenced by this version\n-        // need to be available in the file\n-        MVMap<String, String> oldMeta = getMetaMap(version);\n-        if (oldMeta == null) {\n-            return false;\n-        }\n-        try {\n-            for (Iterator<String> it = oldMeta.keyIterator(\"chunk.\");\n-                    it.hasNext();) {\n-                String chunkKey = it.next();\n-                if (!chunkKey.startsWith(\"chunk.\")) {\n-                    break;\n-                }\n-                if (!meta.containsKey(chunkKey)) {\n-                    String s = oldMeta.get(chunkKey);\n-                    Chunk c2 = Chunk.fromString(s);\n-                    Chunk test = readChunkHeaderAndFooter(c2.block);\n-                    if (test == null || test.id != c2.id) {\n-                        return false;\n-                    }\n-                    // we store this chunk\n-                    chunks.put(c2.id, c2);\n-                }\n-            }\n-        } catch (IllegalStateException e) {\n-            // the chunk missing where the metadata is stored\n-            return false;\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Increment the number of unsaved pages.\n-     *\n-     * @param memory the memory usage of the page\n-     */\n-    void registerUnsavedPage(int memory) {\n-        unsavedMemory += memory;\n-        int newValue = unsavedMemory;\n-        if (newValue > autoCommitMemory && autoCommitMemory > 0) {\n-            saveNeeded = true;\n-        }\n-    }\n-\n-    /**\n-     * This method is called before writing to a map.\n-     *\n-     * @param map the map\n-     */\n-    void beforeWrite(MVMap<?, ?> map) {\n-        if (saveNeeded) {\n-            if (map == meta) {\n-                // to, don't save while the metadata map is locked\n-                // this is to avoid deadlocks that could occur when we\n-                // synchronize on the store and then on the metadata map\n-                // TODO there should be no deadlocks possible\n-                return;\n-            }\n-            saveNeeded = false;\n-            // check again, because it could have been written by now\n-            if (unsavedMemory > autoCommitMemory && autoCommitMemory > 0) {\n-                commitAndSave();\n-            }\n-        }\n-    }\n-\n-    /**\n-     * Get the store version. The store version is usually used to upgrade the\n-     * structure of the store after upgrading the application. Initially the\n-     * store version is 0, until it is changed.\n-     *\n-     * @return the store version\n-     */\n-    public int getStoreVersion() {\n-        checkOpen();\n-        String x = meta.get(\"setting.storeVersion\");\n-        return x == null ? 0 : DataUtils.parseHexInt(x);\n-    }\n-\n-    /**\n-     * Update the store version.\n-     *\n-     * @param version the new store version\n-     */\n-    public synchronized void setStoreVersion(int version) {\n-        checkOpen();\n-        markMetaChanged();\n-        meta.put(\"setting.storeVersion\", Integer.toHexString(version));\n-    }\n-\n-    /**\n-     * Revert to the beginning of the current version, reverting all uncommitted\n-     * changes.\n-     */\n-    public void rollback() {\n-        rollbackTo(currentVersion);\n-    }\n-\n-    /**\n-     * Revert to the beginning of the given version. All later changes (stored\n-     * or not) are forgotten. All maps that were created later are closed. A\n-     * rollback to a version before the last stored version is immediately\n-     * persisted. Rollback to version 0 means all data is removed.\n-     *\n-     * @param version the version to revert to\n-     */\n-    public synchronized void rollbackTo(long version) {\n-        checkOpen();\n-        if (version == 0) {\n-            // special case: remove all data\n-            for (MVMap<?, ?> m : maps.values()) {\n-                m.close();\n-            }\n-            meta.clear();\n-            chunks.clear();\n-            if (fileStore != null) {\n-                fileStore.clear();\n-            }\n-            maps.clear();\n-            freedPageSpace.clear();\n-            currentVersion = version;\n-            setWriteVersion(version);\n-            metaChanged = false;\n-            return;\n-        }\n-        DataUtils.checkArgument(\n-                isKnownVersion(version),\n-                \"Unknown version {0}\", version);\n-        for (MVMap<?, ?> m : maps.values()) {\n-            m.rollbackTo(version);\n-        }\n-        for (long v = currentVersion; v >= version; v--) {\n-            if (freedPageSpace.size() == 0) {\n-                break;\n-            }\n-            freedPageSpace.remove(v);\n-        }\n-        meta.rollbackTo(version);\n-        metaChanged = false;\n-        boolean loadFromFile = false;\n-        // find out which chunks to remove,\n-        // and which is the newest chunk to keep\n-        // (the chunk list can have gaps)\n-        ArrayList<Integer> remove = new ArrayList<Integer>();\n-        Chunk keep = null;\n-        for (Chunk c : chunks.values()) {\n-            if (c.version > version) {\n-                remove.add(c.id);\n-            } else if (keep == null || keep.id < c.id) {\n-                keep = c;\n-            }\n-        }\n-        if (remove.size() > 0) {\n-            // remove the youngest first, so we don't create gaps\n-            // (in case we remove many chunks)\n-            Collections.sort(remove, Collections.reverseOrder());\n-            revertTemp(version);\n-            loadFromFile = true;\n-            for (int id : remove) {\n-                Chunk c = chunks.remove(id);\n-                long start = c.block * BLOCK_SIZE;\n-                int length = c.len * BLOCK_SIZE;\n-                fileStore.free(start, length);\n-                // overwrite the chunk,\n-                // so it is not be used later on\n-                WriteBuffer buff = getWriteBuffer();\n-                buff.limit(length);\n-                // buff.clear() does not set the data\n-                Arrays.fill(buff.getBuffer().array(), (byte) 0);\n-                write(start, buff.getBuffer());\n-                releaseWriteBuffer(buff);\n-                // only really needed if we remove many chunks, when writes are\n-                // re-ordered - but we do it always, because rollback is not\n-                // performance critical\n-                sync();\n-            }\n-            lastChunk = keep;\n-            writeStoreHeader();\n-            readStoreHeader();\n-        }\n-        for (MVMap<?, ?> m : New.arrayList(maps.values())) {\n-            int id = m.getId();\n-            if (m.getCreateVersion() >= version) {\n-                m.close();\n-                maps.remove(id);\n-            } else {\n-                if (loadFromFile) {\n-                    m.setRootPos(getRootPos(meta, id), -1);\n-                }\n-            }\n-        }\n-        // rollback might have rolled back the stored chunk metadata as well\n-        if (lastChunk != null) {\n-            for (Chunk c : chunks.values()) {\n-                meta.put(Chunk.getMetaKey(c.id), c.asString());\n-            }\n-        }\n-        currentVersion = version;\n-        setWriteVersion(version);\n-    }\n-\n-    private static long getRootPos(MVMap<String, String> map, int mapId) {\n-        String root = map.get(MVMap.getMapRootKey(mapId));\n-        return root == null ? 0 : DataUtils.parseHexLong(root);\n-    }\n-\n-    private void revertTemp(long storeVersion) {\n-        for (Iterator<Long> it = freedPageSpace.keySet().iterator();\n-                it.hasNext();) {\n-            long v = it.next();\n-            if (v > storeVersion) {\n-                continue;\n-            }\n-            it.remove();\n-        }\n-        for (MVMap<?, ?> m : maps.values()) {\n-            m.removeUnusedOldVersions();\n-        }\n-    }\n-\n-    /**\n-     * Get the current version of the data. When a new store is created, the\n-     * version is 0.\n-     *\n-     * @return the version\n-     */\n-    public long getCurrentVersion() {\n-        return currentVersion;\n-    }\n-\n-    /**\n-     * Get the file store.\n-     *\n-     * @return the file store\n-     */\n-    public FileStore getFileStore() {\n-        return fileStore;\n-    }\n-\n-    /**\n-     * Get the store header. This data is for informational purposes only. The\n-     * data is subject to change in future versions. The data should not be\n-     * modified (doing so may corrupt the store).\n-     *\n-     * @return the store header\n-     */\n-    public Map<String, Object> getStoreHeader() {\n-        return storeHeader;\n-    }\n-\n-    private void checkOpen() {\n-        if (closed) {\n-            throw DataUtils.newIllegalStateException(DataUtils.ERROR_CLOSED,\n-                    \"This store is closed\", panicException);\n-        }\n-    }\n-\n-    /**\n-     * Rename a map.\n-     *\n-     * @param map the map\n-     * @param newName the new name\n-     */\n-    public synchronized void renameMap(MVMap<?, ?> map, String newName) {\n-        checkOpen();\n-        DataUtils.checkArgument(map != meta,\n-                \"Renaming the meta map is not allowed\");\n-        int id = map.getId();\n-        String oldName = getMapName(id);\n-        if (oldName.equals(newName)) {\n-            return;\n-        }\n-        DataUtils.checkArgument(\n-                !meta.containsKey(\"name.\" + newName),\n-                \"A map named {0} already exists\", newName);\n-        markMetaChanged();\n-        String x = Integer.toHexString(id);\n-        meta.remove(\"name.\" + oldName);\n-        meta.put(MVMap.getMapKey(id), map.asString(newName));\n-        meta.put(\"name.\" + newName, x);\n-    }\n-\n-    /**\n-     * Remove a map. Please note rolling back this operation does not restore\n-     * the data; if you need this ability, use Map.clear().\n-     *\n-     * @param map the map to remove\n-     */\n-    public synchronized void removeMap(MVMap<?, ?> map) {\n-        checkOpen();\n-        DataUtils.checkArgument(map != meta,\n-                \"Removing the meta map is not allowed\");\n-        map.clear();\n-        int id = map.getId();\n-        String name = getMapName(id);\n-        markMetaChanged();\n-        meta.remove(MVMap.getMapKey(id));\n-        meta.remove(\"name.\" + name);\n-        meta.remove(MVMap.getMapRootKey(id));\n-        maps.remove(id);\n-    }\n-\n-    /**\n-     * Get the name of the given map.\n-     *\n-     * @param id the map id\n-     * @return the name, or null if not found\n-     */\n-    public synchronized String getMapName(int id) {\n-        checkOpen();\n-        String m = meta.get(MVMap.getMapKey(id));\n-        return m == null ? null : DataUtils.parseMap(m).get(\"name\");\n-    }\n-\n-    /**\n-     * Commit and save all changes, if there are any, and compact the store if\n-     * needed.\n-     */\n-    void writeInBackground() {\n-        if (closed) {\n-            return;\n-        }\n-\n-        // could also commit when there are many unsaved pages,\n-        // but according to a test it doesn't really help\n-\n-        long time = getTimeSinceCreation();\n-        if (time <= lastCommitTime + autoCommitDelay) {\n-            return;\n-        }\n-        if (hasUnsavedChanges()) {\n-            try {\n-                commitAndSave();\n-            } catch (Exception e) {\n-                if (backgroundExceptionHandler != null) {\n-                    backgroundExceptionHandler.uncaughtException(null, e);\n-                    return;\n-                }\n-            }\n-        }\n-        if (autoCompactFillRate > 0) {\n-            try {\n-                // whether there were file read or write operations since\n-                // the last time\n-                boolean fileOps;\n-                long fileOpCount = fileStore.getWriteCount() + fileStore.getReadCount();\n-                if (autoCompactLastFileOpCount != fileOpCount) {\n-                    fileOps = true;\n-                } else {\n-                    fileOps = false;\n-                }\n-                // use a lower fill rate if there were any file operations\n-                int fillRate = fileOps ? autoCompactFillRate / 3 : autoCompactFillRate;\n-                // TODO how to avoid endless compaction if there is a bug\n-                // in the bookkeeping?\n-                compact(fillRate, autoCommitMemory);\n-                autoCompactLastFileOpCount = fileStore.getWriteCount() + fileStore.getReadCount();\n-            } catch (Exception e) {\n-                if (backgroundExceptionHandler != null) {\n-                    backgroundExceptionHandler.uncaughtException(null, e);\n-                }\n-            }\n-        }\n-    }\n-\n-    /**\n-     * Set the read cache size in MB.\n-     *\n-     * @param mb the cache size in MB.\n-     */\n-    public void setCacheSize(int mb) {\n-        final long bytes = (long) mb * 1024 * 1024;\n-        if (cache != null) {\n-            cache.setMaxMemory(bytes);\n-            cache.clear();\n-        }\n-        if (cacheChunkRef != null) {\n-            cacheChunkRef.setMaxMemory(bytes / 4);\n-            cacheChunkRef.clear();\n-        }\n-    }\n-\n-    public boolean isClosed() {\n-        return closed;\n-    }\n-\n-    private void stopBackgroundThread() {\n-        BackgroundWriterThread t = backgroundWriterThread;\n-        if (t == null) {\n-            return;\n-        }\n-        backgroundWriterThread = null;\n-        if (Thread.currentThread() == t) {\n-            // within the thread itself - can not join\n-            return;\n-        }\n-        synchronized (t.sync) {\n-            t.sync.notifyAll();\n-        }\n-        if (Thread.holdsLock(this)) {\n-            // called from storeNow: can not join,\n-            // because that could result in a deadlock\n-            return;\n-        }\n-        try {\n-            t.join();\n-        } catch (Exception e) {\n-            // ignore\n-        }\n-    }\n-\n-    /**\n-     * Set the maximum delay in milliseconds to auto-commit changes.\n-     * <p>\n-     * To disable auto-commit, set the value to 0. In this case, changes are\n-     * only committed when explicitly calling commit.\n-     * <p>\n-     * The default is 1000, meaning all changes are committed after at most one\n-     * second.\n-     *\n-     * @param millis the maximum delay\n-     */\n-    public void setAutoCommitDelay(int millis) {\n-        if (autoCommitDelay == millis) {\n-            return;\n-        }\n-        autoCommitDelay = millis;\n-        if (fileStore == null || fileStore.isReadOnly()) {\n-            return;\n-        }\n-        stopBackgroundThread();\n-        // start the background thread if needed\n-        if (millis > 0) {\n-            int sleep = Math.max(1, millis / 10);\n-            BackgroundWriterThread t =\n-                    new BackgroundWriterThread(this, sleep,\n-                            fileStore.toString());\n-            t.start();\n-            backgroundWriterThread = t;\n-        }\n-    }\n-\n-    /**\n-     * Get the auto-commit delay.\n-     *\n-     * @return the delay in milliseconds, or 0 if auto-commit is disabled.\n-     */\n-    public int getAutoCommitDelay() {\n-        return autoCommitDelay;\n-    }\n-\n-    /**\n-     * Get the maximum memory (in bytes) used for unsaved pages. If this number\n-     * is exceeded, unsaved changes are stored to disk.\n-     *\n-     * @return the memory in bytes\n-     */\n-    public int getAutoCommitMemory() {\n-        return autoCommitMemory;\n-    }\n-\n-    /**\n-     * Get the estimated memory (in bytes) of unsaved data. If the value exceeds\n-     * the auto-commit memory, the changes are committed.\n-     * <p>\n-     * The returned value is an estimation only.\n-     *\n-     * @return the memory in bytes\n-     */\n-    public int getUnsavedMemory() {\n-        return unsavedMemory;\n-    }\n-\n-    /**\n-     * Put the page in the cache.\n-     *\n-     * @param pos the page position\n-     * @param page the page\n-     * @param memory the memory used\n-     */\n-    void cachePage(long pos, Page page, int memory) {\n-        if (cache != null) {\n-            cache.put(pos, page, memory);\n-        }\n-    }\n-\n-    /**\n-     * Get the amount of memory used for caching, in MB.\n-     * Note that this does not include the page chunk references cache, which is\n-     * 25% of the size of the page cache.\n-     *\n-     * @return the amount of memory used for caching\n-     */\n-    public int getCacheSizeUsed() {\n-        if (cache == null) {\n-            return 0;\n-        }\n-        return (int) (cache.getUsedMemory() / 1024 / 1024);\n-    }\n-\n-    /**\n-     * Get the maximum cache size, in MB.\n-     * Note that this does not include the page chunk references cache, which is\n-     * 25% of the size of the page cache.\n-     *\n-     * @return the cache size\n-     */\n-    public int getCacheSize() {\n-        if (cache == null) {\n-            return 0;\n-        }\n-        return (int) (cache.getMaxMemory() / 1024 / 1024);\n-    }\n-\n-    /**\n-     * Get the cache.\n-     *\n-     * @return the cache\n-     */\n-    public CacheLongKeyLIRS<Page> getCache() {\n-        return cache;\n-    }\n-\n-    /**\n-     * Whether the store is read-only.\n-     *\n-     * @return true if it is\n-     */\n-    public boolean isReadOnly() {\n-        return fileStore == null ? false : fileStore.isReadOnly();\n-    }\n-\n-    /**\n-     * A background writer thread to automatically store changes from time to\n-     * time.\n-     */\n-    private static class BackgroundWriterThread extends Thread {\n-\n-        public final Object sync = new Object();\n-        private final MVStore store;\n-        private final int sleep;\n-\n-        BackgroundWriterThread(MVStore store, int sleep, String fileStoreName) {\n-            super(\"MVStore background writer \" + fileStoreName);\n-            this.store = store;\n-            this.sleep = sleep;\n-            setDaemon(true);\n-        }\n-\n-        @Override\n-        public void run() {\n-            while (true) {\n-                Thread t = store.backgroundWriterThread;\n-                if (t == null) {\n-                    break;\n-                }\n-                synchronized (sync) {\n-                    try {\n-                        sync.wait(sleep);\n-                    } catch (InterruptedException e) {\n-                        continue;\n-                    }\n-                }\n-                store.writeInBackground();\n-            }\n-        }\n-\n-    }\n-\n-    /**\n-     * A builder for an MVStore.\n-     */\n-    public static class Builder {\n-\n-        private final HashMap<String, Object> config = New.hashMap();\n-\n-        private Builder set(String key, Object value) {\n-            config.put(key, value);\n-            return this;\n-        }\n-\n-        /**\n-         * Disable auto-commit, by setting the auto-commit delay and auto-commit\n-         * buffer size to 0.\n-         *\n-         * @return this\n-         */\n-        public Builder autoCommitDisabled() {\n-            // we have a separate config option so that\n-            // no thread is started if the write delay is 0\n-            // (if we only had a setter in the MVStore,\n-            // the thread would need to be started in any case)\n-            set(\"autoCommitBufferSize\", 0);\n-            return set(\"autoCommitDelay\", 0);\n-        }\n-\n-        /**\n-         * Set the size of the write buffer, in KB disk space (for file-based\n-         * stores). Unless auto-commit is disabled, changes are automatically\n-         * saved if there are more than this amount of changes.\n-         * <p>\n-         * The default is 1024 KB.\n-         * <p>\n-         * When the value is set to 0 or lower, data is not automatically\n-         * stored.\n-         *\n-         * @param kb the write buffer size, in kilobytes\n-         * @return this\n-         */\n-        public Builder autoCommitBufferSize(int kb) {\n-            return set(\"autoCommitBufferSize\", kb);\n-        }\n-\n-        /**\n-         * Set the auto-compact target fill rate. If the average fill rate (the\n-         * percentage of the storage space that contains active data) of the\n-         * chunks is lower, then the chunks with a low fill rate are re-written.\n-         * Also, if the percentage of empty space between chunks is higher than\n-         * this value, then chunks at the end of the file are moved. Compaction\n-         * stops if the target fill rate is reached.\n-         * <p>\n-         * The default value is 50 (50%). The value 0 disables auto-compacting.\n-         * <p>\n-         *\n-         * @param percent the target fill rate\n-         * @return this\n-         */\n-        public Builder autoCompactFillRate(int percent) {\n-            return set(\"autoCompactFillRate\", percent);\n-        }\n-\n-        /**\n-         * Use the following file name. If the file does not exist, it is\n-         * automatically created. The parent directory already must exist.\n-         *\n-         * @param fileName the file name\n-         * @return this\n-         */\n-        public Builder fileName(String fileName) {\n-            return set(\"fileName\", fileName);\n-        }\n-\n-        /**\n-         * Encrypt / decrypt the file using the given password. This method has\n-         * no effect for in-memory stores. The password is passed as a\n-         * char array so that it can be cleared as soon as possible. Please note\n-         * there is still a small risk that password stays in memory (due to\n-         * Java garbage collection). Also, the hashed encryption key is kept in\n-         * memory as long as the file is open.\n-         *\n-         * @param password the password\n-         * @return this\n-         */\n-        public Builder encryptionKey(char[] password) {\n-            return set(\"encryptionKey\", password);\n-        }\n-\n-        /**\n-         * Open the file in read-only mode. In this case, a shared lock will be\n-         * acquired to ensure the file is not concurrently opened in write mode.\n-         * <p>\n-         * If this option is not used, the file is locked exclusively.\n-         * <p>\n-         * Please note a store may only be opened once in every JVM (no matter\n-         * whether it is opened in read-only or read-write mode), because each\n-         * file may be locked only once in a process.\n-         *\n-         * @return this\n-         */\n-        public Builder readOnly() {\n-            return set(\"readOnly\", 1);\n-        }\n-\n-        /**\n-         * Set the read cache size in MB. The default is 16 MB.\n-         *\n-         * @param mb the cache size in megabytes\n-         * @return this\n-         */\n-        public Builder cacheSize(int mb) {\n-            return set(\"cacheSize\", mb);\n-        }\n-\n-        /**\n-         * Set the read cache concurrency. The default is 16, meaning 16\n-         * segments are used.\n-         *\n-         * @param concurrency the cache concurrency\n-         * @return this\n-         */\n-        public Builder cacheConcurrency(int concurrency) {\n-            return set(\"cacheConcurrency\", concurrency);\n-        }\n-\n-        /**\n-         * Compress data before writing using the LZF algorithm. This will save\n-         * about 50% of the disk space, but will slow down read and write\n-         * operations slightly.\n-         * <p>\n-         * This setting only affects writes; it is not necessary to enable\n-         * compression when reading, even if compression was enabled when\n-         * writing.\n-         *\n-         * @return this\n-         */\n-        public Builder compress() {\n-            return set(\"compress\", 1);\n-        }\n-\n-        /**\n-         * Compress data before writing using the Deflate algorithm. This will\n-         * save more disk space, but will slow down read and write operations\n-         * quite a bit.\n-         * <p>\n-         * This setting only affects writes; it is not necessary to enable\n-         * compression when reading, even if compression was enabled when\n-         * writing.\n-         *\n-         * @return this\n-         */\n-        public Builder compressHigh() {\n-            return set(\"compress\", 2);\n-        }\n-\n-        /**\n-         * Set the amount of memory a page should contain at most, in bytes,\n-         * before it is split. The default is 16 KB for persistent stores and 4\n-         * KB for in-memory stores. This is not a limit in the page size, as\n-         * pages with one entry can get larger. It is just the point where pages\n-         * that contain more than one entry are split.\n-         *\n-         * @param pageSplitSize the page size\n-         * @return this\n-         */\n-        public Builder pageSplitSize(int pageSplitSize) {\n-            return set(\"pageSplitSize\", pageSplitSize);\n-        }\n-\n-        /**\n-         * Set the listener to be used for exceptions that occur when writing in\n-         * the background thread.\n-         *\n-         * @param exceptionHandler the handler\n-         * @return this\n-         */\n-        public Builder backgroundExceptionHandler(\n-                Thread.UncaughtExceptionHandler exceptionHandler) {\n-            return set(\"backgroundExceptionHandler\", exceptionHandler);\n-        }\n-\n-        /**\n-         * Use the provided file store instead of the default one.\n-         * <p>\n-         * File stores passed in this way need to be open. They are not closed\n-         * when closing the store.\n-         * <p>\n-         * Please note that any kind of store (including an off-heap store) is\n-         * considered a \"persistence\", while an \"in-memory store\" means objects\n-         * are not persisted and fully kept in the JVM heap.\n-         *\n-         * @param store the file store\n-         * @return this\n-         */\n-        public Builder fileStore(FileStore store) {\n-            return set(\"fileStore\", store);\n-        }\n-\n-        /**\n-         * Open the store.\n-         *\n-         * @return the opened store\n-         */\n-        public MVStore open() {\n-            return new MVStore(config);\n-        }\n-\n-        @Override\n-        public String toString() {\n-            return DataUtils.appendMap(new StringBuilder(), config).toString();\n-        }\n-\n-        /**\n-         * Read the configuration from a string.\n-         *\n-         * @param s the string representation\n-         * @return the builder\n-         */\n-        public static Builder fromString(String s) {\n-            HashMap<String, String> config = DataUtils.parseMap(s);\n-            Builder builder = new Builder();\n-            builder.config.putAll(config);\n-            return builder;\n-        }\n-\n-    }\n+  }\n \n }\n",
            "diff_size": 5297
        },
        {
            "tool": "naturalize",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/errored/1/223/MVStore.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/naturalize/223/MVStore.java\nindex 9906203edd7..7b00f167616 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/errored/1/223/MVStore.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/naturalize/223/MVStore.java\n@@ -294,14 +294,12 @@ public final class MVStore {\n         String fileName = (String) config.get(\"fileName\");\n         fileStore = (FileStore) config.get(\"fileStore\");\n         fileStoreIsProvided = fileStore != null;\n-        if(fileStore == null && fileName != null) {\n+        if (fileStore == null && fileName != null) {\n             fileStore = new FileStore();\n         }\n         o = config.get(\"pageSplitSize\");\n-        pageSplitSize = o != null         ? (Integer) o :\n-                        fileStore == null ? 48 :\n-                                            16 * 1024;\n-        o = config.get(\"backgroundExceptionHandler\");\n+        pageSplitSize = o != null ? (Integer) o : fileStore == null ? 48 : 16 * 1024;\n+o = config.get(\"backgroundExceptionHandler\");\n         this.backgroundExceptionHandler = (UncaughtExceptionHandler) o;\n         meta = new MVMap<String, String>(StringDataType.INSTANCE,\n                 StringDataType.INSTANCE);\n@@ -482,7 +480,8 @@ public final class MVStore {\n     public synchronized Set<String> getMapNames() {\n         HashSet<String> set = New.hashSet();\n         checkOpen();\n-        for (Iterator<String> it = meta.keyIterator(\"name.\"); it.hasNext();) {\n+        for (Iterator<String> it = meta.keyIterator(\"name.\");\n+it.hasNext();) {\n             String x = it.next();\n             if (!x.startsWith(\"name.\")) {\n                 break;\n@@ -687,7 +686,8 @@ public final class MVStore {\n     private void loadChunkMeta() {\n         // load the chunk metadata: we can load in any order,\n         // because loading chunk metadata might recursively load another chunk\n-        for (Iterator<String> it = meta.keyIterator(\"chunk.\"); it.hasNext();) {\n+        for (Iterator<String> it = meta.keyIterator(\"chunk.\");\n+it.hasNext();) {\n             String s = it.next();\n             if (!s.startsWith(\"chunk.\")) {\n                 break;\n@@ -1299,7 +1299,8 @@ public final class MVStore {\n         DataUtils.checkArgument(testVersion > 0, \"Collect references on version 0\");\n         long readCount = getFileStore().readCount;\n         Set<Integer> referenced = New.hashSet();\n-        for (Cursor<String, String> c = meta.cursor(\"root.\"); c.hasNext();) {\n+        for (Cursor<String, String> c = meta.cursor(\"root.\");\n+c.hasNext();) {\n             String key = c.next();\n             if (!key.startsWith(\"root.\")) {\n                 break;\n@@ -1891,7 +1892,8 @@ public final class MVStore {\n         }\n         // remove the chunks we want to keep from this list\n         boolean remove = false;\n-        for (Iterator<Chunk> it = old.iterator(); it.hasNext();) {\n+        for (Iterator<Chunk> it = old.iterator();\n+it.hasNext();) {\n             Chunk c = it.next();\n             if (move == c) {\n                 remove = true;\n@@ -2157,7 +2159,7 @@ public final class MVStore {\n         }\n         try {\n             for (Iterator<String> it = oldMeta.keyIterator(\"chunk.\");\n-                    it.hasNext();) {\n+it.hasNext();) {\n                 String chunkKey = it.next();\n                 if (!chunkKey.startsWith(\"chunk.\")) {\n                     break;\n@@ -2357,7 +2359,7 @@ public final class MVStore {\n \n     private void revertTemp(long storeVersion) {\n         for (Iterator<Long> it = freedPageSpace.keySet().iterator();\n-                it.hasNext();) {\n+it.hasNext();) {\n             long v = it.next();\n             if (v > storeVersion) {\n                 continue;\n@@ -2945,4 +2947,4 @@ public final class MVStore {\n \n     }\n \n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 16
        },
        {
            "tool": "codebuff",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/errored/1/223/MVStore.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/codebuff/223/MVStore.java\nindex 9906203edd7..b6e99b8250d 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/errored/1/223/MVStore.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/codebuff/223/MVStore.java\n@@ -120,19 +120,24 @@ MVStore:\n /**\n  * A persistent storage for maps.\n  */\n+\n+\n public final class MVStore {\n \n     /**\n      * Whether assertions are enabled.\n      */\n+\n+\n     public static final boolean ASSERT = false;\n \n     /**\n      * The block size (physical sector size) of the disk. The store header is\n      * written twice, one copy in each block, to ensure it survives a crash.\n      */\n-    static final int BLOCK_SIZE = 4 * 1024;\n \n+\n+    static final int BLOCK_SIZE = 4 * 1024;\n     private static final int FORMAT_WRITE = 1;\n     private static final int FORMAT_READ = 1;\n \n@@ -145,15 +150,13 @@ public final class MVStore {\n     /**\n      * The background thread, if any.\n      */\n-    volatile BackgroundWriterThread backgroundWriterThread;\n \n-    private volatile boolean reuseSpace = true;\n \n+    volatile BackgroundWriterThread backgroundWriterThread;\n+    private volatile boolean reuseSpace = true;\n     private boolean closed;\n-\n     private FileStore fileStore;\n     private boolean fileStoreIsProvided;\n-\n     private final int pageSplitSize;\n \n     /**\n@@ -178,33 +181,24 @@ public final class MVStore {\n     /**\n      * The map of chunks.\n      */\n-    private final ConcurrentHashMap<Integer, Chunk> chunks =\n-            new ConcurrentHashMap<Integer, Chunk>();\n+    private final ConcurrentHashMap<Integer, Chunk> chunks = new ConcurrentHashMap<Integer, Chunk>();\n \n     /**\n      * The map of temporarily freed storage space caused by freed pages. The key\n      * is the unsaved version, the value is the map of chunks. The maps contains\n      * the number of freed entries per chunk. Access is synchronized.\n      */\n-    private final ConcurrentHashMap<Long,\n-            HashMap<Integer, Chunk>> freedPageSpace =\n-            new ConcurrentHashMap<Long, HashMap<Integer, Chunk>>();\n+    private final ConcurrentHashMap<Long, HashMap<Integer, Chunk>> freedPageSpace = new ConcurrentHashMap<Long, HashMap<Integer, Chunk>>();\n \n     /**\n      * The metadata map. Write access to this map needs to be synchronized on\n      * the store.\n      */\n     private MVMap<String, String> meta;\n-\n-    private final ConcurrentHashMap<Integer, MVMap<?, ?>> maps =\n-            new ConcurrentHashMap<Integer, MVMap<?, ?>>();\n-\n+    private final ConcurrentHashMap<Integer, MVMap<?, ?>> maps = new ConcurrentHashMap<Integer, MVMap<?, ?>>();\n     private HashMap<String, Object> storeHeader = New.hashMap();\n-\n     private WriteBuffer writeBuffer;\n-\n     private int lastMapId;\n-\n     private int versionsToKeep = 5;\n \n     /**\n@@ -212,13 +206,9 @@ public final class MVStore {\n      * high). Even if disabled, the store may contain (old) compressed pages.\n      */\n     private final int compressionLevel;\n-\n     private Compressor compressorFast;\n-\n     private Compressor compressorHigh;\n-\n     private final UncaughtExceptionHandler backgroundExceptionHandler;\n-\n     private long currentVersion;\n \n     /**\n@@ -247,7 +237,6 @@ public final class MVStore {\n      * overwritten if unused.\n      */\n     private int retentionTime;\n-\n     private long lastCommitTime;\n \n     /**\n@@ -259,25 +248,18 @@ public final class MVStore {\n      * The version of the current store operation (if any).\n      */\n     private volatile long currentStoreVersion = -1;\n-\n     private Thread currentStoreThread;\n-\n     private volatile boolean metaChanged;\n \n     /**\n      * The delay in milliseconds to automatically commit and write changes.\n      */\n     private int autoCommitDelay;\n-\n     private int autoCompactFillRate;\n     private long autoCompactLastFileOpCount;\n-\n     private Object compactSync = new Object();\n-\n     private IllegalStateException panicException;\n-\n     private long lastTimeAbsolute;\n-\n     private long lastFreeUnusedChunks;\n \n     /**\n@@ -288,23 +270,21 @@ public final class MVStore {\n      *             occurred while opening\n      * @throws IllegalArgumentException if the directory does not exist\n      */\n+\n     MVStore(HashMap<String, Object> config) {\n         Object o = config.get(\"compress\");\n         this.compressionLevel = o == null ? 0 : (Integer) o;\n         String fileName = (String) config.get(\"fileName\");\n         fileStore = (FileStore) config.get(\"fileStore\");\n         fileStoreIsProvided = fileStore != null;\n-        if(fileStore == null && fileName != null) {\n+        if (fileStore == null && fileName != null) {\n             fileStore = new FileStore();\n         }\n         o = config.get(\"pageSplitSize\");\n-        pageSplitSize = o != null         ? (Integer) o :\n-                        fileStore == null ? 48 :\n-                                            16 * 1024;\n+        pageSplitSize = o != null ? (Integer) o : fileStore == null ? 48 : 16 * 1024;\n         o = config.get(\"backgroundExceptionHandler\");\n         this.backgroundExceptionHandler = (UncaughtExceptionHandler) o;\n-        meta = new MVMap<String, String>(StringDataType.INSTANCE,\n-                StringDataType.INSTANCE);\n+        meta = new MVMap<String, String>(StringDataType.INSTANCE, StringDataType.INSTANCE);\n         HashMap<String, Object> c = New.hashMap();\n         c.put(\"id\", 0);\n         c.put(\"createVersion\", currentVersion);\n@@ -333,10 +313,8 @@ public final class MVStore {\n         int kb = o == null ? 1024 : (Integer) o;\n         // 19 KB memory is about 1 KB storage\n         autoCommitMemory = kb * 1024 * 19;\n-\n         o = config.get(\"autoCompactFillRate\");\n         autoCompactFillRate = o == null ? 50 : (Integer) o;\n-\n         char[] encryptionKey = (char[]) config.get(\"encryptionKey\");\n         try {\n             if (!fileStoreIsProvided) {\n@@ -385,6 +363,8 @@ public final class MVStore {\n      * @param fileName the file name (null for in-memory)\n      * @return the store\n      */\n+\n+\n     public static MVStore open(String fileName) {\n         HashMap<String, Object> config = New.hashMap();\n         config.put(\"fileName\", fileName);\n@@ -399,9 +379,9 @@ public final class MVStore {\n      * @param template the template map\n      * @return the read-only map\n      */\n+\n     @SuppressWarnings(\"unchecked\")\n-    <T extends MVMap<?, ?>> T openMapVersion(long version, int mapId,\n-            MVMap<?, ?> template) {\n+    <T extends MVMap<?, ?>> T openMapVersion(long version, int mapId, MVMap<?, ?> template) {\n         MVMap<String, String> oldMeta = getMetaMap(version);\n         long rootPos = getRootPos(oldMeta, mapId);\n         MVMap<?, ?> m = template.openReadOnly();\n@@ -419,6 +399,8 @@ public final class MVStore {\n      * @param name the name of the map\n      * @return the map\n      */\n+\n+\n     public <K, V> MVMap<K, V> openMap(String name) {\n         return openMap(name, new MVMap.Builder<K, V>());\n     }\n@@ -434,8 +416,9 @@ public final class MVStore {\n      * @param builder the map builder\n      * @return the map\n      */\n-    public synchronized <M extends MVMap<K, V>, K, V> M openMap(\n-            String name, MVMap.MapBuilder<M, K, V> builder) {\n+\n+\n+    public synchronized <M extends MVMap<K, V>, K, V> M openMap(String name, MVMap.MapBuilder<M, K, V> builder) {\n         checkOpen();\n         String x = meta.get(\"name.\" + name);\n         int id;\n@@ -479,6 +462,8 @@ public final class MVStore {\n      *\n      * @return the set of names\n      */\n+\n+\n     public synchronized Set<String> getMapNames() {\n         HashSet<String> set = New.hashSet();\n         checkOpen();\n@@ -511,6 +496,8 @@ public final class MVStore {\n      *\n      * @return the metadata map\n      */\n+\n+\n     public MVMap<String, String> getMetaMap() {\n         checkOpen();\n         return meta;\n@@ -543,6 +530,8 @@ public final class MVStore {\n      * @param name the map name\n      * @return true if it exists\n      */\n+\n+\n     public boolean hasMap(String name) {\n         return meta.containsKey(\"name.\" + name);\n     }\n@@ -564,23 +553,17 @@ public final class MVStore {\n             fileHeaderBlocks.get(buff);\n             // the following can fail for various reasons\n             try {\n-                String s = new String(buff, 0, BLOCK_SIZE,\n-                        DataUtils.LATIN).trim();\n+                String s = new String(buff, 0, BLOCK_SIZE, DataUtils.LATIN).trim();\n                 HashMap<String, String> m = DataUtils.parseMap(s);\n-                int blockSize = DataUtils.readHexInt(\n-                        m, \"blockSize\", BLOCK_SIZE);\n+                int blockSize = DataUtils.readHexInt(m, \"blockSize\", BLOCK_SIZE);\n                 if (blockSize != BLOCK_SIZE) {\n-                    throw DataUtils.newIllegalStateException(\n-                            DataUtils.ERROR_UNSUPPORTED_FORMAT,\n-                            \"Block size {0} is currently not supported\",\n-                            blockSize);\n+                    throw DataUtils.newIllegalStateException(DataUtils.ERROR_UNSUPPORTED_FORMAT, \"Block size {0} is currently not supported\", blockSize);\n                 }\n                 int check = DataUtils.readHexInt(m, \"fletcher\", 0);\n                 m.remove(\"fletcher\");\n                 s = s.substring(0, s.lastIndexOf(\"fletcher\") - 1);\n                 byte[] bytes = s.getBytes(DataUtils.LATIN);\n-                int checksum = DataUtils.getFletcher32(bytes,\n-                        bytes.length);\n+                int checksum = DataUtils.getFletcher32(bytes, bytes.length);\n                 if (check != checksum) {\n                     continue;\n                 }\n@@ -601,33 +584,23 @@ public final class MVStore {\n             }\n         }\n         if (!validStoreHeader) {\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_FILE_CORRUPT,\n-                    \"Store header is corrupt: {0}\", fileStore);\n+            throw DataUtils.newIllegalStateException(DataUtils.ERROR_FILE_CORRUPT,\n+                                                     \"Store header is corrupt: {0}\", fileStore);\n         }\n         long format = DataUtils.readHexLong(storeHeader, \"format\", 1);\n         if (format > FORMAT_WRITE && !fileStore.isReadOnly()) {\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_UNSUPPORTED_FORMAT,\n-                    \"The write format {0} is larger \" +\n-                    \"than the supported format {1}, \" +\n-                    \"and the file was not opened in read-only mode\",\n-                    format, FORMAT_WRITE);\n+            throw DataUtils.newIllegalStateException(DataUtils.ERROR_UNSUPPORTED_FORMAT, \"The write format {0} is larger \" + \"than the supported format {1}, \" + \"and the file was not opened in read-only mode\", format, FORMAT_WRITE);\n         }\n         format = DataUtils.readHexLong(storeHeader, \"formatRead\", format);\n         if (format > FORMAT_READ) {\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_UNSUPPORTED_FORMAT,\n-                    \"The read format {0} is larger \" +\n-                    \"than the supported format {1}\",\n-                    format, FORMAT_READ);\n+            throw DataUtils.newIllegalStateException(DataUtils.ERROR_UNSUPPORTED_FORMAT, \"The read format {0} is larger \" + \"than the supported format {1}\", format, FORMAT_READ);\n         }\n         lastStoredVersion = -1;\n         chunks.clear();\n         long now = System.currentTimeMillis();\n         // calculate the year (doesn't have to be exact;\n         // we assume 365.25 days per year, * 4 = 1461)\n-        int year =  1970 + (int) (now / (1000L * 60 * 60 * 6 * 1461));\n+        int year = 1970 + (int) (now / (1000L * 60 * 60 * 6 * 1461));\n         if (year < 2014) {\n             // if the year is before 2014,\n             // we assume the system doesn't have a real-time clock,\n@@ -656,8 +629,7 @@ public final class MVStore {\n         // read the chunk header and footer,\n         // and follow the chain of next chunks\n         while (true) {\n-            if (newest.next == 0 ||\n-                    newest.next >= fileStore.size() / BLOCK_SIZE) {\n+            if (newest.next == 0 || newest.next >= fileStore.size() / BLOCK_SIZE) {\n                 // no (valid) next\n                 break;\n             }\n@@ -696,9 +668,8 @@ public final class MVStore {\n             Chunk c = Chunk.fromString(s);\n             if (!chunks.containsKey(c.id)) {\n                 if (c.block == Long.MAX_VALUE) {\n-                    throw DataUtils.newIllegalStateException(\n-                            DataUtils.ERROR_FILE_CORRUPT,\n-                            \"Chunk {0} is invalid\", c.id);\n+                    throw DataUtils.newIllegalStateException(DataUtils.ERROR_FILE_CORRUPT,\n+                                                             \"Chunk {0} is invalid\", c.id);\n                 }\n                 chunks.put(c.id, c);\n             }\n@@ -762,6 +733,7 @@ public final class MVStore {\n      * @return the chunk, or null if the header or footer don't match or are not\n      *         consistent\n      */\n+\n     private Chunk readChunkHeaderAndFooter(long block) {\n         Chunk header;\n         try {\n@@ -786,12 +758,12 @@ public final class MVStore {\n      * @param end the end of the chunk\n      * @return the chunk, or null if not successful\n      */\n+\n     private Chunk readChunkFooter(long end) {\n         // the following can fail for various reasons\n         try {\n             // read the chunk footer of the last block of the file\n-            ByteBuffer lastBlock = fileStore.readFully(\n-                    end - Chunk.FOOTER_LENGTH, Chunk.FOOTER_LENGTH);\n+            ByteBuffer lastBlock = fileStore.readFully(end - Chunk.FOOTER_LENGTH, Chunk.FOOTER_LENGTH);\n             byte[] buff = new byte[Chunk.FOOTER_LENGTH];\n             lastBlock.get(buff);\n             String s = new String(buff, DataUtils.LATIN).trim();\n@@ -810,6 +782,7 @@ public final class MVStore {\n             }\n         } catch (Exception e) {\n             // ignore\n+\n         }\n         return null;\n     }\n@@ -847,6 +820,8 @@ public final class MVStore {\n     /**\n      * Close the file and the store. Unsaved changes are written to disk first.\n      */\n+\n+\n     public void close() {\n         if (closed) {\n             return;\n@@ -865,6 +840,8 @@ public final class MVStore {\n      * Close the file and the store, without writing anything. This will stop\n      * the background thread. This method ignores all errors.\n      */\n+\n+\n     public void closeImmediately() {\n         try {\n             closeStore(false);\n@@ -916,6 +893,8 @@ public final class MVStore {\n      * @param chunkId the chunk id\n      * @return true if it is live\n      */\n+\n+\n     boolean isChunkLive(int chunkId) {\n         String s = meta.get(Chunk.getMetaKey(chunkId));\n         return s != null;\n@@ -927,13 +906,13 @@ public final class MVStore {\n      * @param pos the position\n      * @return the chunk\n      */\n+\n     private Chunk getChunk(long pos) {\n         Chunk c = getChunkIfFound(pos);\n         if (c == null) {\n             int chunkId = DataUtils.getPageChunkId(pos);\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_FILE_CORRUPT,\n-                    \"Chunk {0} not found\", chunkId);\n+            throw DataUtils.newIllegalStateException(DataUtils.ERROR_FILE_CORRUPT,\n+                                                     \"Chunk {0} not found\", chunkId);\n         }\n         return c;\n     }\n@@ -946,10 +925,8 @@ public final class MVStore {\n             if (!Thread.holdsLock(this)) {\n                 // it could also be unsynchronized metadata\n                 // access (if synchronization on this was forgotten)\n-                throw DataUtils.newIllegalStateException(\n-                        DataUtils.ERROR_CHUNK_NOT_FOUND,\n-                        \"Chunk {0} no longer exists\",\n-                        chunkId);\n+                throw DataUtils.newIllegalStateException(DataUtils.ERROR_CHUNK_NOT_FOUND,\n+                                                         \"Chunk {0} no longer exists\", chunkId);\n             }\n             String s = meta.get(Chunk.getMetaKey(chunkId));\n             if (s == null) {\n@@ -957,9 +934,8 @@ public final class MVStore {\n             }\n             c = Chunk.fromString(s);\n             if (c.block == Long.MAX_VALUE) {\n-                throw DataUtils.newIllegalStateException(\n-                        DataUtils.ERROR_FILE_CORRUPT,\n-                        \"Chunk {0} is invalid\", chunkId);\n+                throw DataUtils.newIllegalStateException(DataUtils.ERROR_FILE_CORRUPT,\n+                                                         \"Chunk {0} is invalid\", chunkId);\n             }\n             chunks.put(c.id, c);\n         }\n@@ -991,6 +967,8 @@ public final class MVStore {\n      *\n      * @return the new version\n      */\n+\n+\n     public synchronized long commit() {\n         if (fileStore != null) {\n             return commitAndSave();\n@@ -1009,14 +987,14 @@ public final class MVStore {\n      *\n      * @return the new version (incremented if there were changes)\n      */\n+\n     private synchronized long commitAndSave() {\n         if (closed) {\n             return currentVersion;\n         }\n         if (fileStore == null) {\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_WRITING_FAILED,\n-                    \"This is an in-memory store\");\n+            throw DataUtils.newIllegalStateException(DataUtils.ERROR_WRITING_FAILED,\n+                                                     \"This is an in-memory store\");\n         }\n         if (currentStoreVersion >= 0) {\n             // store is possibly called within store, if the meta map changed\n@@ -1026,8 +1004,8 @@ public final class MVStore {\n             return currentVersion;\n         }\n         if (fileStore.isReadOnly()) {\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_WRITING_FAILED, \"This store is read-only\");\n+            throw DataUtils.newIllegalStateException(DataUtils.ERROR_WRITING_FAILED,\n+                                                     \"This store is read-only\");\n         }\n         try {\n             currentStoreVersion = currentVersion;\n@@ -1086,14 +1064,11 @@ public final class MVStore {\n                 break;\n             }\n             if (old.block == Long.MAX_VALUE) {\n-                IllegalStateException e = DataUtils.newIllegalStateException(\n-                        DataUtils.ERROR_INTERNAL,\n-                        \"Last block not stored, possibly due to out-of-memory\");\n+                IllegalStateException e = DataUtils.newIllegalStateException(DataUtils.ERROR_INTERNAL, \"Last block not stored, possibly due to out-of-memory\");\n                 panic(e);\n             }\n         }\n         Chunk c = new Chunk(newChunkId);\n-\n         c.pageCount = Integer.MAX_VALUE;\n         c.pageCountLive = Integer.MAX_VALUE;\n         c.maxLen = Long.MAX_VALUE;\n@@ -1149,15 +1124,12 @@ public final class MVStore {\n             }\n         }\n         meta.setWriteVersion(version);\n-\n         Page metaRoot = meta.getRoot();\n         metaRoot.writeUnsavedRecursive(c, buff);\n-\n         int chunkLength = buff.position();\n \n         // add the store header and round to the next block\n-        int length = MathUtils.roundUpInt(chunkLength +\n-                Chunk.FOOTER_LENGTH, BLOCK_SIZE);\n+        int length = MathUtils.roundUpInt(chunkLength + Chunk.FOOTER_LENGTH, BLOCK_SIZE);\n         buff.limit(length);\n \n         // the length of the file that is still in use\n@@ -1171,22 +1143,19 @@ public final class MVStore {\n         }\n         // end is not necessarily the end of the file\n         boolean storeAtEndOfFile = filePos + length >= fileStore.size();\n-\n         if (!reuseSpace) {\n             // we can not mark it earlier, because it\n             // might have been allocated by one of the\n             // removed chunks\n             fileStore.markUsed(end, length);\n         }\n-\n         c.block = filePos / BLOCK_SIZE;\n         c.len = length / BLOCK_SIZE;\n         c.metaRootPos = metaRoot.getPos();\n         // calculate and set the likely next position\n         if (reuseSpace) {\n             int predictBlocks = c.len;\n-            long predictedNextStart = fileStore.allocate(\n-                    predictBlocks * BLOCK_SIZE);\n+            long predictedNextStart = fileStore.allocate(predictBlocks * BLOCK_SIZE);\n             fileStore.free(predictedNextStart, predictBlocks * BLOCK_SIZE);\n             c.next = predictedNextStart / BLOCK_SIZE;\n         } else {\n@@ -1196,10 +1165,8 @@ public final class MVStore {\n         buff.position(0);\n         c.writeChunkHeader(buff, headerLength);\n         revertTemp(storeVersion);\n-\n         buff.position(buff.limit() - Chunk.FOOTER_LENGTH);\n         buff.put(c.getFooterBytes());\n-\n         buff.position(0);\n         write(filePos, buff.getBuffer());\n         releaseWriteBuffer(buff);\n@@ -1212,9 +1179,8 @@ public final class MVStore {\n             } else if (lastChunk.next != c.block) {\n                 // the last prediction did not matched\n                 writeStoreHeader = true;\n-            } else {\n-                long headerVersion = DataUtils.readHexLong(\n-                        storeHeader, \"version\", 0);\n+                               } else {\n+                long headerVersion = DataUtils.readHexLong(storeHeader, \"version\", 0);\n                 if (lastChunk.version - headerVersion > 20) {\n                     // we write after at least 20 entries\n                     writeStoreHeader = true;\n@@ -1236,7 +1202,6 @@ public final class MVStore {\n                 }\n             }\n         }\n-\n         lastChunk = c;\n         if (writeStoreHeader) {\n             writeStoreHeader();\n@@ -1255,12 +1220,9 @@ public final class MVStore {\n \n         // some pages might have been changed in the meantime (in the newest\n         // version)\n-        unsavedMemory = Math.max(0, unsavedMemory\n-                - currentUnsavedPageCount);\n-\n+        unsavedMemory = Math.max(0, unsavedMemory - currentUnsavedPageCount);\n         metaChanged = false;\n         lastStoredVersion = storeVersion;\n-\n         return version;\n     }\n \n@@ -1317,8 +1279,8 @@ public final class MVStore {\n         return referenced;\n     }\n \n-    private void collectReferencedChunks(Set<Integer> targetChunkSet,\n-            int mapId, long pos, int level) {\n+    private void collectReferencedChunks(Set<Integer> targetChunkSet, int mapId,\n+            long pos, int level) {\n         int c = DataUtils.getPageChunkId(pos);\n         targetChunkSet.add(c);\n         if (DataUtils.getPageType(pos) == DataUtils.PAGE_TYPE_LEAF) {\n@@ -1336,8 +1298,7 @@ public final class MVStore {\n             long[] children = new long[target.size()];\n             int i = 0;\n             for (Integer p : target) {\n-                children[i++] = DataUtils.getPagePos(p, 0, 0,\n-                        DataUtils.PAGE_TYPE_LEAF);\n+                children[i++] = DataUtils.getPagePos(p, 0, 0, DataUtils.PAGE_TYPE_LEAF);\n             }\n             refs.children = children;\n             refs.chunkList = true;\n@@ -1374,9 +1335,7 @@ public final class MVStore {\n                 long filePos = c.block * BLOCK_SIZE;\n                 filePos += DataUtils.getPageOffset(pos);\n                 if (filePos < 0) {\n-                    throw DataUtils.newIllegalStateException(\n-                            DataUtils.ERROR_FILE_CORRUPT,\n-                            \"Negative position {0}; p={1}, c={2}\", filePos, pos, c.toString());\n+                    throw DataUtils.newIllegalStateException(DataUtils.ERROR_FILE_CORRUPT, \"Negative position {0}; p={1}, c={2}\", filePos, pos, c.toString());\n                 }\n                 long maxPos = (c.block + c.len) * BLOCK_SIZE;\n                 r = PageChildren.read(fileStore, pos, mapId, filePos, maxPos);\n@@ -1401,6 +1360,7 @@ public final class MVStore {\n      *\n      * @return the buffer\n      */\n+\n     private WriteBuffer getWriteBuffer() {\n         WriteBuffer buff;\n         if (writeBuffer != null) {\n@@ -1418,6 +1378,7 @@ public final class MVStore {\n      *\n      * @param buff the buffer than can be re-used\n      */\n+\n     private void releaseWriteBuffer(WriteBuffer buff) {\n         if (buff.capacity() <= 4 * 1024 * 1024) {\n             writeBuffer = buff;\n@@ -1464,6 +1425,7 @@ public final class MVStore {\n      *\n      * @param storeVersion apply up to the given version\n      */\n+\n     private void applyFreedSpace(long storeVersion) {\n         while (true) {\n             ArrayList<Chunk> modified = New.arrayList();\n@@ -1513,6 +1475,7 @@ public final class MVStore {\n      *\n      * @param minPercent the minimum percentage to save\n      */\n+\n     private void shrinkFileIfPossible(int minPercent) {\n         if (fileStore.isReadOnly()) {\n             return;\n@@ -1540,6 +1503,7 @@ public final class MVStore {\n      *\n      * @return the position\n      */\n+\n     private long getFileLengthInUse() {\n         long size = 2 * BLOCK_SIZE;\n         for (Chunk c : chunks.values()) {\n@@ -1556,6 +1520,8 @@ public final class MVStore {\n      *\n      * @return if there are any changes\n      */\n+\n+\n     public boolean hasUnsavedChanges() {\n         checkOpen();\n         if (metaChanged) {\n@@ -1583,6 +1549,8 @@ public final class MVStore {\n      *\n      * @return if anything was written\n      */\n+\n+\n     public synchronized boolean compactRewriteFully() {\n         checkOpen();\n         if (lastChunk == null) {\n@@ -1615,6 +1583,8 @@ public final class MVStore {\n      *\n      * @return if anything was written\n      */\n+\n+\n     public synchronized boolean compactMoveChunks() {\n         return compactMoveChunks(100, Long.MAX_VALUE);\n     }\n@@ -1630,6 +1600,8 @@ public final class MVStore {\n      * @param moveSize the number of bytes to move\n      * @return if anything was written\n      */\n+\n+\n     public synchronized boolean compactMoveChunks(int targetFillRate, long moveSize) {\n         checkOpen();\n         if (lastChunk == null || !reuseSpace) {\n@@ -1665,10 +1637,10 @@ public final class MVStore {\n         }\n         // sort by block\n         Collections.sort(move, new Comparator<Chunk>() {\n-            @Override\n-            public int compare(Chunk o1, Chunk o2) {\n-                return Long.signum(o1.block - o2.block);\n-            }\n+@Override\n+public int compare(Chunk o1, Chunk o2) {\n+    return Long.signum(o1.block - o2.block);\n+}\n         });\n         // find which is the last block to keep\n         int count = 0;\n@@ -1686,7 +1658,6 @@ public final class MVStore {\n         while (move.size() > count && move.size() > 1) {\n             move.remove(1);\n         }\n-\n         return move;\n     }\n \n@@ -1763,6 +1734,8 @@ public final class MVStore {\n      * Force all stored changes to be written to the storage. The default\n      * implementation calls FileChannel.force(true).\n      */\n+\n+\n     public void sync() {\n         checkOpen();\n         FileStore f = fileStore;\n@@ -1789,6 +1762,8 @@ public final class MVStore {\n      * @param write the minimum number of bytes to write\n      * @return if a chunk was re-written\n      */\n+\n+\n     public boolean compact(int targetFillRate, int write) {\n         if (!reuseSpace) {\n             return false;\n@@ -1816,9 +1791,7 @@ public final class MVStore {\n         // calculate the fill rate\n         long maxLengthSum = 0;\n         long maxLengthLiveSum = 0;\n-\n         long time = getTimeSinceCreation();\n-\n         for (Chunk c : chunks.values()) {\n             // ignore young chunks, because we don't optimize those\n             if (c.time + retentionTime > time) {\n@@ -1861,16 +1834,14 @@ public final class MVStore {\n \n         // sort the list, so the first entry should be collected first\n         Collections.sort(old, new Comparator<Chunk>() {\n-            @Override\n-            public int compare(Chunk o1, Chunk o2) {\n-                int comp = new Integer(o1.collectPriority).\n-                        compareTo(o2.collectPriority);\n-                if (comp == 0) {\n-                    comp = new Long(o1.maxLenLive).\n-                        compareTo(o2.maxLenLive);\n-                }\n-                return comp;\n-            }\n+@Override\n+public int compare(Chunk o1, Chunk o2) {\n+    int comp = new Integer(o1.collectPriority).compareTo(o2.collectPriority);\n+    if (comp == 0) {\n+        comp = new Long(o1.maxLenLive).compareTo(o2.maxLenLive);\n+    }\n+    return comp;\n+}\n         });\n         // find out up to were in the old list we need to move\n         long written = 0;\n@@ -1928,10 +1899,11 @@ public final class MVStore {\n      * @param pos the page position\n      * @return the page\n      */\n+\n+\n     Page readPage(MVMap<?, ?> map, long pos) {\n         if (pos == 0) {\n-            throw DataUtils.newIllegalStateException(\n-                    DataUtils.ERROR_FILE_CORRUPT, \"Position 0\");\n+            throw DataUtils.newIllegalStateException(DataUtils.ERROR_FILE_CORRUPT, \"Position 0\");\n         }\n         Page p = cache == null ? null : cache.get(pos);\n         if (p == null) {\n@@ -1939,9 +1911,8 @@ public final class MVStore {\n             long filePos = c.block * BLOCK_SIZE;\n             filePos += DataUtils.getPageOffset(pos);\n             if (filePos < 0) {\n-                throw DataUtils.newIllegalStateException(\n-                        DataUtils.ERROR_FILE_CORRUPT,\n-                        \"Negative position {0}\", filePos);\n+                throw DataUtils.newIllegalStateException(DataUtils.ERROR_FILE_CORRUPT,\n+                                                         \"Negative position {0}\", filePos);\n             }\n             long maxPos = (c.block + c.len) * BLOCK_SIZE;\n             p = Page.read(fileStore, pos, map, filePos, maxPos);\n@@ -1957,6 +1928,8 @@ public final class MVStore {\n      * @param pos the position of the page\n      * @param memory the memory usage\n      */\n+\n+\n     void removePage(MVMap<?, ?> map, long pos, int memory) {\n         // we need to keep temporary pages,\n         // to support reading old versions and rollback\n@@ -1980,7 +1953,6 @@ public final class MVStore {\n                 cache.remove(pos);\n             }\n         }\n-\n         Chunk c = getChunk(pos);\n         long version = currentVersion;\n         if (map == meta && currentStoreVersion >= 0) {\n@@ -1992,8 +1964,7 @@ public final class MVStore {\n                 version = currentStoreVersion;\n             }\n         }\n-        registerFreePage(version, c.id,\n-                DataUtils.getPageMaxLength(pos), 1);\n+        registerFreePage(version, c.id, DataUtils.getPageMaxLength(pos), 1);\n     }\n \n     private void registerFreePage(long version, int chunkId,\n@@ -2001,8 +1972,7 @@ public final class MVStore {\n         HashMap<Integer, Chunk> freed = freedPageSpace.get(version);\n         if (freed == null) {\n             freed = New.hashMap();\n-            HashMap<Integer, Chunk> f2 = freedPageSpace.putIfAbsent(version,\n-                    freed);\n+            HashMap<Integer, Chunk> f2 = freedPageSpace.putIfAbsent(version, freed);\n             if (f2 != null) {\n                 freed = f2;\n             }\n@@ -2019,6 +1989,7 @@ public final class MVStore {\n         }\n     }\n \n+\n     Compressor getCompressorFast() {\n         if (compressorFast == null) {\n             compressorFast = new CompressLZF();\n@@ -2026,6 +1997,7 @@ public final class MVStore {\n         return compressorFast;\n     }\n \n+\n     Compressor getCompressorHigh() {\n         if (compressorHigh == null) {\n             compressorHigh = new CompressDeflate();\n@@ -2033,14 +2005,17 @@ public final class MVStore {\n         return compressorHigh;\n     }\n \n+\n     int getCompressionLevel() {\n         return compressionLevel;\n     }\n \n+\n     public int getPageSplitSize() {\n         return pageSplitSize;\n     }\n \n+\n     public boolean getReuseSpace() {\n         return reuseSpace;\n     }\n@@ -2058,10 +2033,13 @@ public final class MVStore {\n      *\n      * @param reuseSpace the new value\n      */\n+\n+\n     public void setReuseSpace(boolean reuseSpace) {\n         this.reuseSpace = reuseSpace;\n     }\n \n+\n     public int getRetentionTime() {\n         return retentionTime;\n     }\n@@ -2087,6 +2065,8 @@ public final class MVStore {\n      * @param ms how many milliseconds to retain old chunks (0 to overwrite them\n      *            as early as possible)\n      */\n+\n+\n     public void setRetentionTime(int ms) {\n         this.retentionTime = ms;\n     }\n@@ -2097,6 +2077,8 @@ public final class MVStore {\n      *\n      * @param count the number of versions to keep\n      */\n+\n+\n     public void setVersionsToKeep(int count) {\n         this.versionsToKeep = count;\n     }\n@@ -2106,6 +2088,8 @@ public final class MVStore {\n      *\n      * @return the version\n      */\n+\n+\n     public long getVersionsToKeep() {\n         return versionsToKeep;\n     }\n@@ -2116,6 +2100,8 @@ public final class MVStore {\n      *\n      * @return the version\n      */\n+\n+\n     long getOldestVersionToKeep() {\n         long v = currentVersion;\n         if (fileStore == null) {\n@@ -2136,6 +2122,7 @@ public final class MVStore {\n      * @param version the version\n      * @return true if all data can be read\n      */\n+\n     private boolean isKnownVersion(long version) {\n         if (version > currentVersion || version < 0) {\n             return false;\n@@ -2156,8 +2143,7 @@ public final class MVStore {\n             return false;\n         }\n         try {\n-            for (Iterator<String> it = oldMeta.keyIterator(\"chunk.\");\n-                    it.hasNext();) {\n+            for (Iterator<String> it = oldMeta.keyIterator(\"chunk.\"); it.hasNext();) {\n                 String chunkKey = it.next();\n                 if (!chunkKey.startsWith(\"chunk.\")) {\n                     break;\n@@ -2185,6 +2171,8 @@ public final class MVStore {\n      *\n      * @param memory the memory usage of the page\n      */\n+\n+\n     void registerUnsavedPage(int memory) {\n         unsavedMemory += memory;\n         int newValue = unsavedMemory;\n@@ -2198,6 +2186,8 @@ public final class MVStore {\n      *\n      * @param map the map\n      */\n+\n+\n     void beforeWrite(MVMap<?, ?> map) {\n         if (saveNeeded) {\n             if (map == meta) {\n@@ -2222,6 +2212,8 @@ public final class MVStore {\n      *\n      * @return the store version\n      */\n+\n+\n     public int getStoreVersion() {\n         checkOpen();\n         String x = meta.get(\"setting.storeVersion\");\n@@ -2233,6 +2225,8 @@ public final class MVStore {\n      *\n      * @param version the new store version\n      */\n+\n+\n     public synchronized void setStoreVersion(int version) {\n         checkOpen();\n         markMetaChanged();\n@@ -2243,6 +2237,8 @@ public final class MVStore {\n      * Revert to the beginning of the current version, reverting all uncommitted\n      * changes.\n      */\n+\n+\n     public void rollback() {\n         rollbackTo(currentVersion);\n     }\n@@ -2255,6 +2251,8 @@ public final class MVStore {\n      *\n      * @param version the version to revert to\n      */\n+\n+\n     public synchronized void rollbackTo(long version) {\n         checkOpen();\n         if (version == 0) {\n@@ -2274,9 +2272,7 @@ public final class MVStore {\n             metaChanged = false;\n             return;\n         }\n-        DataUtils.checkArgument(\n-                isKnownVersion(version),\n-                \"Unknown version {0}\", version);\n+        DataUtils.checkArgument(isKnownVersion(version), \"Unknown version {0}\", version);\n         for (MVMap<?, ?> m : maps.values()) {\n             m.rollbackTo(version);\n         }\n@@ -2356,8 +2352,7 @@ public final class MVStore {\n     }\n \n     private void revertTemp(long storeVersion) {\n-        for (Iterator<Long> it = freedPageSpace.keySet().iterator();\n-                it.hasNext();) {\n+        for (Iterator<Long> it = freedPageSpace.keySet().iterator(); it.hasNext();) {\n             long v = it.next();\n             if (v > storeVersion) {\n                 continue;\n@@ -2375,6 +2370,8 @@ public final class MVStore {\n      *\n      * @return the version\n      */\n+\n+\n     public long getCurrentVersion() {\n         return currentVersion;\n     }\n@@ -2384,6 +2381,8 @@ public final class MVStore {\n      *\n      * @return the file store\n      */\n+\n+\n     public FileStore getFileStore() {\n         return fileStore;\n     }\n@@ -2395,6 +2394,8 @@ public final class MVStore {\n      *\n      * @return the store header\n      */\n+\n+\n     public Map<String, Object> getStoreHeader() {\n         return storeHeader;\n     }\n@@ -2402,7 +2403,7 @@ public final class MVStore {\n     private void checkOpen() {\n         if (closed) {\n             throw DataUtils.newIllegalStateException(DataUtils.ERROR_CLOSED,\n-                    \"This store is closed\", panicException);\n+                                                     \"This store is closed\", panicException);\n         }\n     }\n \n@@ -2412,18 +2413,18 @@ public final class MVStore {\n      * @param map the map\n      * @param newName the new name\n      */\n+\n+\n     public synchronized void renameMap(MVMap<?, ?> map, String newName) {\n         checkOpen();\n-        DataUtils.checkArgument(map != meta,\n-                \"Renaming the meta map is not allowed\");\n+        DataUtils.checkArgument(map != meta, \"Renaming the meta map is not allowed\");\n         int id = map.getId();\n         String oldName = getMapName(id);\n         if (oldName.equals(newName)) {\n             return;\n         }\n-        DataUtils.checkArgument(\n-                !meta.containsKey(\"name.\" + newName),\n-                \"A map named {0} already exists\", newName);\n+        DataUtils.checkArgument(!meta.containsKey(\"name.\" + newName),\n+                                \"A map named {0} already exists\", newName);\n         markMetaChanged();\n         String x = Integer.toHexString(id);\n         meta.remove(\"name.\" + oldName);\n@@ -2437,10 +2438,11 @@ public final class MVStore {\n      *\n      * @param map the map to remove\n      */\n+\n+\n     public synchronized void removeMap(MVMap<?, ?> map) {\n         checkOpen();\n-        DataUtils.checkArgument(map != meta,\n-                \"Removing the meta map is not allowed\");\n+        DataUtils.checkArgument(map != meta, \"Removing the meta map is not allowed\");\n         map.clear();\n         int id = map.getId();\n         String name = getMapName(id);\n@@ -2457,6 +2459,8 @@ public final class MVStore {\n      * @param id the map id\n      * @return the name, or null if not found\n      */\n+\n+\n     public synchronized String getMapName(int id) {\n         checkOpen();\n         String m = meta.get(MVMap.getMapKey(id));\n@@ -2467,6 +2471,8 @@ public final class MVStore {\n      * Commit and save all changes, if there are any, and compact the store if\n      * needed.\n      */\n+\n+\n     void writeInBackground() {\n         if (closed) {\n             return;\n@@ -2474,7 +2480,6 @@ public final class MVStore {\n \n         // could also commit when there are many unsaved pages,\n         // but according to a test it doesn't really help\n-\n         long time = getTimeSinceCreation();\n         if (time <= lastCommitTime + autoCommitDelay) {\n             return;\n@@ -2519,6 +2524,8 @@ public final class MVStore {\n      *\n      * @param mb the cache size in MB.\n      */\n+\n+\n     public void setCacheSize(int mb) {\n         final long bytes = (long) mb * 1024 * 1024;\n         if (cache != null) {\n@@ -2531,6 +2538,7 @@ public final class MVStore {\n         }\n     }\n \n+\n     public boolean isClosed() {\n         return closed;\n     }\n@@ -2557,6 +2565,7 @@ public final class MVStore {\n             t.join();\n         } catch (Exception e) {\n             // ignore\n+\n         }\n     }\n \n@@ -2571,6 +2580,8 @@ public final class MVStore {\n      *\n      * @param millis the maximum delay\n      */\n+\n+\n     public void setAutoCommitDelay(int millis) {\n         if (autoCommitDelay == millis) {\n             return;\n@@ -2583,9 +2594,7 @@ public final class MVStore {\n         // start the background thread if needed\n         if (millis > 0) {\n             int sleep = Math.max(1, millis / 10);\n-            BackgroundWriterThread t =\n-                    new BackgroundWriterThread(this, sleep,\n-                            fileStore.toString());\n+            BackgroundWriterThread t = new BackgroundWriterThread(this, sleep, fileStore.toString());\n             t.start();\n             backgroundWriterThread = t;\n         }\n@@ -2596,6 +2605,8 @@ public final class MVStore {\n      *\n      * @return the delay in milliseconds, or 0 if auto-commit is disabled.\n      */\n+\n+\n     public int getAutoCommitDelay() {\n         return autoCommitDelay;\n     }\n@@ -2606,6 +2617,8 @@ public final class MVStore {\n      *\n      * @return the memory in bytes\n      */\n+\n+\n     public int getAutoCommitMemory() {\n         return autoCommitMemory;\n     }\n@@ -2618,6 +2631,8 @@ public final class MVStore {\n      *\n      * @return the memory in bytes\n      */\n+\n+\n     public int getUnsavedMemory() {\n         return unsavedMemory;\n     }\n@@ -2629,6 +2644,8 @@ public final class MVStore {\n      * @param page the page\n      * @param memory the memory used\n      */\n+\n+\n     void cachePage(long pos, Page page, int memory) {\n         if (cache != null) {\n             cache.put(pos, page, memory);\n@@ -2642,6 +2659,8 @@ public final class MVStore {\n      *\n      * @return the amount of memory used for caching\n      */\n+\n+\n     public int getCacheSizeUsed() {\n         if (cache == null) {\n             return 0;\n@@ -2656,6 +2675,8 @@ public final class MVStore {\n      *\n      * @return the cache size\n      */\n+\n+\n     public int getCacheSize() {\n         if (cache == null) {\n             return 0;\n@@ -2668,6 +2689,8 @@ public final class MVStore {\n      *\n      * @return the cache\n      */\n+\n+\n     public CacheLongKeyLIRS<Page> getCache() {\n         return cache;\n     }\n@@ -2677,6 +2700,8 @@ public final class MVStore {\n      *\n      * @return true if it is\n      */\n+\n+\n     public boolean isReadOnly() {\n         return fileStore == null ? false : fileStore.isReadOnly();\n     }\n@@ -2685,8 +2710,10 @@ public final class MVStore {\n      * A background writer thread to automatically store changes from time to\n      * time.\n      */\n+\n     private static class BackgroundWriterThread extends Thread {\n \n+\n         public final Object sync = new Object();\n         private final MVStore store;\n         private final int sleep;\n@@ -2721,10 +2748,11 @@ public final class MVStore {\n     /**\n      * A builder for an MVStore.\n      */\n+\n+\n     public static class Builder {\n \n         private final HashMap<String, Object> config = New.hashMap();\n-\n         private Builder set(String key, Object value) {\n             config.put(key, value);\n             return this;\n@@ -2736,6 +2764,8 @@ public final class MVStore {\n          *\n          * @return this\n          */\n+\n+\n         public Builder autoCommitDisabled() {\n             // we have a separate config option so that\n             // no thread is started if the write delay is 0\n@@ -2758,6 +2788,8 @@ public final class MVStore {\n          * @param kb the write buffer size, in kilobytes\n          * @return this\n          */\n+\n+\n         public Builder autoCommitBufferSize(int kb) {\n             return set(\"autoCommitBufferSize\", kb);\n         }\n@@ -2776,6 +2808,8 @@ public final class MVStore {\n          * @param percent the target fill rate\n          * @return this\n          */\n+\n+\n         public Builder autoCompactFillRate(int percent) {\n             return set(\"autoCompactFillRate\", percent);\n         }\n@@ -2787,6 +2821,8 @@ public final class MVStore {\n          * @param fileName the file name\n          * @return this\n          */\n+\n+\n         public Builder fileName(String fileName) {\n             return set(\"fileName\", fileName);\n         }\n@@ -2802,6 +2838,8 @@ public final class MVStore {\n          * @param password the password\n          * @return this\n          */\n+\n+\n         public Builder encryptionKey(char[] password) {\n             return set(\"encryptionKey\", password);\n         }\n@@ -2818,6 +2856,8 @@ public final class MVStore {\n          *\n          * @return this\n          */\n+\n+\n         public Builder readOnly() {\n             return set(\"readOnly\", 1);\n         }\n@@ -2828,6 +2868,8 @@ public final class MVStore {\n          * @param mb the cache size in megabytes\n          * @return this\n          */\n+\n+\n         public Builder cacheSize(int mb) {\n             return set(\"cacheSize\", mb);\n         }\n@@ -2839,6 +2881,8 @@ public final class MVStore {\n          * @param concurrency the cache concurrency\n          * @return this\n          */\n+\n+\n         public Builder cacheConcurrency(int concurrency) {\n             return set(\"cacheConcurrency\", concurrency);\n         }\n@@ -2854,6 +2898,8 @@ public final class MVStore {\n          *\n          * @return this\n          */\n+\n+\n         public Builder compress() {\n             return set(\"compress\", 1);\n         }\n@@ -2869,6 +2915,8 @@ public final class MVStore {\n          *\n          * @return this\n          */\n+\n+\n         public Builder compressHigh() {\n             return set(\"compress\", 2);\n         }\n@@ -2883,6 +2931,8 @@ public final class MVStore {\n          * @param pageSplitSize the page size\n          * @return this\n          */\n+\n+\n         public Builder pageSplitSize(int pageSplitSize) {\n             return set(\"pageSplitSize\", pageSplitSize);\n         }\n@@ -2894,8 +2944,9 @@ public final class MVStore {\n          * @param exceptionHandler the handler\n          * @return this\n          */\n-        public Builder backgroundExceptionHandler(\n-                Thread.UncaughtExceptionHandler exceptionHandler) {\n+\n+\n+        public Builder backgroundExceptionHandler(Thread.UncaughtExceptionHandler exceptionHandler) {\n             return set(\"backgroundExceptionHandler\", exceptionHandler);\n         }\n \n@@ -2912,6 +2963,8 @@ public final class MVStore {\n          * @param store the file store\n          * @return this\n          */\n+\n+\n         public Builder fileStore(FileStore store) {\n             return set(\"fileStore\", store);\n         }\n@@ -2921,6 +2974,8 @@ public final class MVStore {\n          *\n          * @return the opened store\n          */\n+\n+\n         public MVStore open() {\n             return new MVStore(config);\n         }\n@@ -2936,6 +2991,8 @@ public final class MVStore {\n          * @param s the string representation\n          * @return the builder\n          */\n+\n+\n         public static Builder fromString(String s) {\n             HashMap<String, String> config = DataUtils.parseMap(s);\n             Builder builder = new Builder();\n@@ -2945,4 +3002,4 @@ public final class MVStore {\n \n     }\n \n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 320
        },
        {
            "tool": "styler_random",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/errored/1/223/MVStore.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/styler_random/223/MVStore.java\nindex 9906203edd7..b646f44c169 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/errored/1/223/MVStore.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/styler_random/223/MVStore.java\n@@ -294,7 +294,7 @@ public final class MVStore {\n         String fileName = (String) config.get(\"fileName\");\n         fileStore = (FileStore) config.get(\"fileStore\");\n         fileStoreIsProvided = fileStore != null;\n-        if(fileStore == null && fileName != null) {\n+        if (fileStore == null && fileName != null) {\n             fileStore = new FileStore();\n         }\n         o = config.get(\"pageSplitSize\");\n",
            "diff_size": 1
        },
        {
            "tool": "styler_three_grams",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/errored/1/223/MVStore.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/styler_three_grams/223/MVStore.java\nindex 9906203edd7..b646f44c169 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/errored/1/223/MVStore.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/codefollower-H2-Research/styler_three_grams/223/MVStore.java\n@@ -294,7 +294,7 @@ public final class MVStore {\n         String fileName = (String) config.get(\"fileName\");\n         fileStore = (FileStore) config.get(\"fileStore\");\n         fileStoreIsProvided = fileStore != null;\n-        if(fileStore == null && fileName != null) {\n+        if (fileStore == null && fileName != null) {\n             fileStore = new FileStore();\n         }\n         o = config.get(\"pageSplitSize\");\n",
            "diff_size": 1
        }
    ],
    "repaired_by": [
        "styler",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ],
    "not_repaired_by": [
        "intellij"
    ]
}