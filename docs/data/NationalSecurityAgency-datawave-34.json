{
    "project_name": "NationalSecurityAgency-datawave",
    "error_id": "34",
    "information": {
        "errors": [
            {
                "line": "25",
                "severity": "error",
                "message": "Accumulo non-public classes imported",
                "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
            }
        ]
    },
    "source_code": "import org.apache.accumulo.core.client.admin.InstanceOperations;\nimport org.apache.accumulo.core.client.security.tokens.PasswordToken;\nimport org.apache.accumulo.core.conf.Property;\nimport org.apache.accumulo.core.data.Key;\nimport org.apache.accumulo.core.data.Range;\nimport org.apache.accumulo.core.data.Value;",
    "results": [
        {
            "tool": "styler",
            "errors": [
                {
                    "line": "25",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "intellij",
            "errors": [
                {
                    "line": "25",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/34/MultiRfileInputformat.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/intellij/34/MultiRfileInputformat.java\nindex 69589684ebb..a212905a3d7 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/34/MultiRfileInputformat.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/intellij/34/MultiRfileInputformat.java\n@@ -45,254 +45,262 @@ import com.google.common.collect.Sets;\n import com.google.common.collect.TreeMultimap;\n \n public class MultiRfileInputformat extends RFileInputFormat {\n-    /**\n-     * The following can be set via your configuration to override the default namespace\n-     */\n-    public static final String FS_DEFAULT_NAMESPACE = \"fs.default.namespace\";\n-    /**\n-     * Merge ranges can be overrided to the default of true so that you don't merge on the hosting tablet server\n-     */\n-    public static final String MERGE_RANGE = \"merge.range\";\n-    public static final String CACHE_METADATA = \"rfile.cache.metdata\";\n-    public static final String CACHE_METADATA_EXPIRE_SECONDS = \"rfile.cache.expire.seconds\";\n-    public static final String CACHE_RETRIEVE_SIZE = \"rfile.size.compute\";\n-    public static final String CACHE_METADATA_SIZE = \"rfile.cache.metdata.size\";\n-    private static final String HDFS_BASE = \"hdfs://\";\n-    private static final String ACCUMULO_BASE_PATH = \"/accumulo\";\n-    \n-    private static final String FS_DEFAULT_NAME = \"fs.default.name\";\n-    private static final Logger log = Logger.getLogger(MultiRfileInputformat.class);\n-    public static final String tableStr = Path.SEPARATOR + \"tables\" + Path.SEPARATOR;\n-    \n-    private static LoadingCache<Range,Set<Tuple2<String,Set<String>>>> locationMap = null;\n-    \n-    protected static Map<String,String> dfsUriMap = new ConcurrentHashMap<>();\n-    protected static Map<String,String> dfsDirMap = new ConcurrentHashMap<>();\n-    \n-    @Override\n-    public RecordReader<Key,Value> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {\n-        return new RangeRecordReader();\n+  /**\n+   * The following can be set via your configuration to override the default namespace\n+   */\n+  public static final String FS_DEFAULT_NAMESPACE = \"fs.default.namespace\";\n+  /**\n+   * Merge ranges can be overrided to the default of true so that you don't merge on the hosting tablet server\n+   */\n+  public static final String MERGE_RANGE = \"merge.range\";\n+  public static final String CACHE_METADATA = \"rfile.cache.metdata\";\n+  public static final String CACHE_METADATA_EXPIRE_SECONDS = \"rfile.cache.expire.seconds\";\n+  public static final String CACHE_RETRIEVE_SIZE = \"rfile.size.compute\";\n+  public static final String CACHE_METADATA_SIZE = \"rfile.cache.metdata.size\";\n+  private static final String HDFS_BASE = \"hdfs://\";\n+  private static final String ACCUMULO_BASE_PATH = \"/accumulo\";\n+\n+  private static final String FS_DEFAULT_NAME = \"fs.default.name\";\n+  private static final Logger log = Logger.getLogger(MultiRfileInputformat.class);\n+  public static final String tableStr = Path.SEPARATOR + \"tables\" + Path.SEPARATOR;\n+\n+  private static LoadingCache<Range, Set<Tuple2<String, Set<String>>>> locationMap = null;\n+\n+  protected static Map<String, String> dfsUriMap = new ConcurrentHashMap<>();\n+  protected static Map<String, String> dfsDirMap = new ConcurrentHashMap<>();\n+\n+  @Override\n+  public RecordReader<Key, Value> createRecordReader(InputSplit split, TaskAttemptContext context)\n+      throws IOException, InterruptedException {\n+    return new RangeRecordReader();\n+  }\n+\n+  /**\n+   * Return the lists of computed slit points\n+   */\n+  public List<InputSplit> getSplits(JobContext job) throws IOException {\n+\n+    AccumuloHelper cbHelper = new AccumuloHelper();\n+    cbHelper.setup(job.getConfiguration());\n+\n+    String tableName = BulkInputFormat.getTablename(job.getConfiguration());\n+    boolean autoAdjust = BulkInputFormat.getAutoAdjustRanges(job.getConfiguration());\n+    List<Range> ranges = autoAdjust ? Range.mergeOverlapping(BulkInputFormat.getRanges(job.getConfiguration())) :\n+        BulkInputFormat.getRanges(job\n+            .getConfiguration());\n+\n+    if (ranges.isEmpty()) {\n+      ranges = Lists.newArrayListWithCapacity(1);\n+      ranges.add(new Range());\n+    }\n+\n+    List<InputSplit> inputSplits = Lists.newArrayList();\n+    try {\n+      inputSplits = computeSplitPoints(job, tableName, ranges);\n+    } catch (TableNotFoundException | AccumuloException | AccumuloSecurityException | InterruptedException e) {\n+      throw new IOException(e);\n     }\n-    \n+\n+    return inputSplits;\n+  }\n+\n+  List<InputSplit> computeSplitPoints(JobContext job, String tableName, List<Range> ranges)\n+      throws TableNotFoundException, AccumuloException,\n+      AccumuloSecurityException, IOException, InterruptedException {\n+    return computeSplitPoints(job.getConfiguration(), tableName, ranges);\n+  }\n+\n+  public static void clearMetadataCache() {\n+    synchronized (MultiRfileInputformat.class) {\n+      if (null == locationMap) {\n+        return;\n+      }\n+    }\n+    locationMap.invalidateAll();\n+  }\n+\n+  public static void clearMetadataCache(Range range) {\n+\n+    synchronized (MultiRfileInputformat.class) {\n+      if (null == locationMap) {\n+        return;\n+      }\n+    }\n+\n+    locationMap.invalidate(range);\n+    locationMap.refresh(range);\n+  }\n+\n+  public static List<InputSplit> computeSplitPoints(Configuration conf, String tableName, List<Range> ranges)\n+      throws TableNotFoundException,\n+      AccumuloException, AccumuloSecurityException, IOException, InterruptedException {\n+    final Instance instance = BulkInputFormat.getInstance(conf);\n+    final PasswordToken token = new PasswordToken(BulkInputFormat.getPassword(conf));\n+    return computeSplitPoints(instance.getConnector(BulkInputFormat.getUsername(conf), token), conf, tableName, ranges);\n+  }\n+\n+  public static List<InputSplit> computeSplitPoints(Connector conn, Configuration conf, String tableName,\n+                                                    List<Range> ranges) throws TableNotFoundException,\n+      AccumuloException, AccumuloSecurityException, IOException, InterruptedException {\n+\n+    final Multimap<Range, RfileSplit> binnedRanges = ArrayListMultimap.create();\n+\n+    final Instance instance = conn.getInstance();\n+    final PasswordToken token = new PasswordToken(BulkInputFormat.getPassword(conf));\n+\n+    final String tableId = conn.tableOperations().tableIdMap().get(tableName);\n+\n+    final List<InputSplit> inputSplitList = Lists.newArrayList();\n+\n+    Multimap<Text, Range> rowMap = TreeMultimap.create();\n+\n+    String defaultNamespace = null, basePath = null;\n+\n     /**\n-     * Return the lists of computed slit points\n+     * Attempt the following 1) try to get the default namespace from accumulo 2) Use the custom config option 3) use default name in the hdfs configuration\n      */\n-    public List<InputSplit> getSplits(JobContext job) throws IOException {\n-        \n-        AccumuloHelper cbHelper = new AccumuloHelper();\n-        cbHelper.setup(job.getConfiguration());\n-        \n-        String tableName = BulkInputFormat.getTablename(job.getConfiguration());\n-        boolean autoAdjust = BulkInputFormat.getAutoAdjustRanges(job.getConfiguration());\n-        List<Range> ranges = autoAdjust ? Range.mergeOverlapping(BulkInputFormat.getRanges(job.getConfiguration())) : BulkInputFormat.getRanges(job\n-                        .getConfiguration());\n-        \n-        if (ranges.isEmpty()) {\n-            ranges = Lists.newArrayListWithCapacity(1);\n-            ranges.add(new Range());\n-        }\n-        \n-        List<InputSplit> inputSplits = Lists.newArrayList();\n-        try {\n-            inputSplits = computeSplitPoints(job, tableName, ranges);\n-        } catch (TableNotFoundException | AccumuloException | AccumuloSecurityException | InterruptedException e) {\n-            throw new IOException(e);\n-        }\n-        \n-        return inputSplits;\n+    if (dfsUriMap.get(tableId) == null || dfsDirMap.get(tableId) == null) {\n+\n+      synchronized (MultiRfileInputformat.class) {\n+        final InstanceOperations instOps = conn.instanceOperations();\n+        dfsUriMap.put(tableId, instOps.getSystemConfiguration().get(Property.INSTANCE_DFS_URI.getKey()));\n+        dfsDirMap.put(tableId, instOps.getSystemConfiguration().get(Property.INSTANCE_DFS_DIR.getKey()));\n+      }\n     }\n-    \n-    List<InputSplit> computeSplitPoints(JobContext job, String tableName, List<Range> ranges) throws TableNotFoundException, AccumuloException,\n-                    AccumuloSecurityException, IOException, InterruptedException {\n-        return computeSplitPoints(job.getConfiguration(), tableName, ranges);\n+\n+    defaultNamespace = dfsUriMap.get(tableId);\n+\n+    if (StringUtils.isEmpty(defaultNamespace)) {\n+      defaultNamespace = conf.get(FS_DEFAULT_NAMESPACE);\n+\n+      if (StringUtils.isEmpty(defaultNamespace)) {\n+        defaultNamespace = conf.get(FS_DEFAULT_NAME);\n+      }\n     }\n-    \n-    public static void clearMetadataCache() {\n-        synchronized (MultiRfileInputformat.class) {\n-            if (null == locationMap) {\n-                return;\n-            }\n-        }\n-        locationMap.invalidateAll();\n+\n+    basePath = dfsDirMap.get(tableId);\n+\n+    if (StringUtils.isEmpty(basePath)) {\n+      basePath = ACCUMULO_BASE_PATH;\n     }\n-    \n-    public static void clearMetadataCache(Range range) {\n-        \n-        synchronized (MultiRfileInputformat.class) {\n-            if (null == locationMap) {\n-                return;\n-            }\n-        }\n-        \n-        locationMap.invalidate(range);\n-        locationMap.refresh(range);\n+\n+    // ensure we have a separator\n+    if (!basePath.startsWith(Path.SEPARATOR)) {\n+      basePath = Path.SEPARATOR + basePath;\n     }\n-    \n-    public static List<InputSplit> computeSplitPoints(Configuration conf, String tableName, List<Range> ranges) throws TableNotFoundException,\n-                    AccumuloException, AccumuloSecurityException, IOException, InterruptedException {\n-        final Instance instance = BulkInputFormat.getInstance(conf);\n-        final PasswordToken token = new PasswordToken(BulkInputFormat.getPassword(conf));\n-        return computeSplitPoints(instance.getConnector(BulkInputFormat.getUsername(conf), token), conf, tableName, ranges);\n+\n+    // must get the default base path since accumulo only stores the full namespace path\n+    // when one is not stored on the default.\n+    final String defaultBasePath = defaultNamespace + basePath;\n+\n+    if (conf.getBoolean(CACHE_METADATA, false) == true) {\n+      synchronized (MultiRfileInputformat.class) {\n+        if (null == locationMap) {\n+          final long size = conf.getLong(CACHE_METADATA_SIZE, 10000);\n+          final long seconds = conf.getInt(CACHE_METADATA_EXPIRE_SECONDS, 7200);\n+          locationMap = CacheBuilder.newBuilder().maximumSize(size).expireAfterWrite(seconds, TimeUnit.SECONDS)\n+              .build(new MetadataCacheLoader(instance.getConnector(BulkInputFormat.getUsername(conf), token),\n+                  defaultBasePath));\n+        }\n+      }\n     }\n-    \n-    public static List<InputSplit> computeSplitPoints(Connector conn, Configuration conf, String tableName, List<Range> ranges) throws TableNotFoundException,\n-                    AccumuloException, AccumuloSecurityException, IOException, InterruptedException {\n-        \n-        final Multimap<Range,RfileSplit> binnedRanges = ArrayListMultimap.create();\n-        \n-        final Instance instance = conn.getInstance();\n-        final PasswordToken token = new PasswordToken(BulkInputFormat.getPassword(conf));\n-        \n-        final String tableId = conn.tableOperations().tableIdMap().get(tableName);\n-        \n-        final List<InputSplit> inputSplitList = Lists.newArrayList();\n-        \n-        Multimap<Text,Range> rowMap = TreeMultimap.create();\n-        \n-        String defaultNamespace = null, basePath = null;\n-        \n-        /**\n-         * Attempt the following 1) try to get the default namespace from accumulo 2) Use the custom config option 3) use default name in the hdfs configuration\n-         */\n-        if (dfsUriMap.get(tableId) == null || dfsDirMap.get(tableId) == null) {\n-            \n-            synchronized (MultiRfileInputformat.class) {\n-                final InstanceOperations instOps = conn.instanceOperations();\n-                dfsUriMap.put(tableId, instOps.getSystemConfiguration().get(Property.INSTANCE_DFS_URI.getKey()));\n-                dfsDirMap.put(tableId, instOps.getSystemConfiguration().get(Property.INSTANCE_DFS_DIR.getKey()));\n-            }\n+\n+    for (Range range : ranges) {\n+      // turn this range into a range of rows against the accumulo metadata: (e.g. <tableId>;row)\n+      Range metadataRange = MetadataCacheLoader.createMetadataRange(tableId, range);\n+\n+      Set<Tuple2<String, Set<String>>> metadataEntries;\n+      try {\n+        if (null == locationMap) {\n+          metadataEntries = new MetadataCacheLoader(conn, defaultBasePath).load(metadataRange);\n+        } else {\n+          metadataEntries = locationMap.get(metadataRange);\n         }\n-        \n-        defaultNamespace = dfsUriMap.get(tableId);\n-        \n-        if (StringUtils.isEmpty(defaultNamespace)) {\n-            defaultNamespace = conf.get(FS_DEFAULT_NAMESPACE);\n-            \n-            if (StringUtils.isEmpty(defaultNamespace)) {\n-                defaultNamespace = conf.get(FS_DEFAULT_NAME);\n+      } catch (Exception e) {\n+        throw new RuntimeException(\"Unable to get rfile locations from accumulo metadata\", e);\n+      }\n+\n+      if (metadataEntries == null || metadataEntries.isEmpty()) {\n+        throw new IOException(\"Unable to find location or files associated with \" + range);\n+      }\n+\n+      for (Tuple2<String, Set<String>> entry : metadataEntries) {\n+        String location = entry.first();\n+        Set<String> fileLocations = entry.second();\n+        if (fileLocations != null && !fileLocations.isEmpty()) {\n+\n+          if (location == null || location.isEmpty()) {\n+            log.warn(\"Unable to find a location associated with \" + range + \" : ? -> \" + fileLocations);\n+          }\n+\n+          for (String fileLocation : fileLocations) {\n+\n+            Path path = new Path(fileLocation);\n+\n+            boolean pullSize = conf.getBoolean(CACHE_RETRIEVE_SIZE, false);\n+            long length = Long.MAX_VALUE;\n+            if (pullSize) {\n+              length = path.getFileSystem(conf).getFileStatus(path).getLen();\n             }\n+\n+            String[] locations = new String[] {location};\n+\n+            binnedRanges.put(range, new RfileSplit(path, 0, length, locations));\n+\n+            rowMap.put(range.getStartKey().getRow(), range);\n+          }\n+        } else {\n+          log.warn(\"Unable to find a some files associated with \" + range + \" : \" + location);\n         }\n-        \n-        basePath = dfsDirMap.get(tableId);\n-        \n-        if (StringUtils.isEmpty(basePath)) {\n-            basePath = ACCUMULO_BASE_PATH;\n-        }\n-        \n-        // ensure we have a separator\n-        if (!basePath.startsWith(Path.SEPARATOR)) {\n-            basePath = Path.SEPARATOR + basePath;\n+      }\n+    }\n+\n+    boolean mergeRanges = conf.getBoolean(MERGE_RANGE, true);\n+\n+    if (!mergeRanges) {\n+      for (Range range : binnedRanges.keySet()) {\n+        Collection<RfileSplit> rangeSplits = binnedRanges.get(range);\n+\n+        if (rangeSplits.isEmpty()) {\n+          continue;\n         }\n-        \n-        // must get the default base path since accumulo only stores the full namespace path\n-        // when one is not stored on the default.\n-        final String defaultBasePath = defaultNamespace + basePath;\n-        \n-        if (conf.getBoolean(CACHE_METADATA, false) == true) {\n-            synchronized (MultiRfileInputformat.class) {\n-                if (null == locationMap) {\n-                    final long size = conf.getLong(CACHE_METADATA_SIZE, 10000);\n-                    final long seconds = conf.getInt(CACHE_METADATA_EXPIRE_SECONDS, 7200);\n-                    locationMap = CacheBuilder.newBuilder().maximumSize(size).expireAfterWrite(seconds, TimeUnit.SECONDS)\n-                                    .build(new MetadataCacheLoader(instance.getConnector(BulkInputFormat.getUsername(conf), token), defaultBasePath));\n-                }\n-            }\n+        TabletSplitSplit compositeInputSplit = new TabletSplitSplit(rangeSplits.size());\n+        compositeInputSplit.setTable(tableName);\n+        for (RfileSplit split : rangeSplits) {\n+          compositeInputSplit.add(new FileRangeSplit(range, split.path, 0, split.length, split.hosts));\n         }\n-        \n-        for (Range range : ranges) {\n-            // turn this range into a range of rows against the accumulo metadata: (e.g. <tableId>;row)\n-            Range metadataRange = MetadataCacheLoader.createMetadataRange(tableId, range);\n-            \n-            Set<Tuple2<String,Set<String>>> metadataEntries;\n-            try {\n-                if (null == locationMap) {\n-                    metadataEntries = new MetadataCacheLoader(conn, defaultBasePath).load(metadataRange);\n-                } else {\n-                    metadataEntries = locationMap.get(metadataRange);\n-                }\n-            } catch (Exception e) {\n-                throw new RuntimeException(\"Unable to get rfile locations from accumulo metadata\", e);\n-            }\n-            \n-            if (metadataEntries == null || metadataEntries.isEmpty()) {\n-                throw new IOException(\"Unable to find location or files associated with \" + range);\n-            }\n-            \n-            for (Tuple2<String,Set<String>> entry : metadataEntries) {\n-                String location = entry.first();\n-                Set<String> fileLocations = entry.second();\n-                if (fileLocations != null && !fileLocations.isEmpty()) {\n-                    \n-                    if (location == null || location.isEmpty()) {\n-                        log.warn(\"Unable to find a location associated with \" + range + \" : ? -> \" + fileLocations);\n-                    }\n-                    \n-                    for (String fileLocation : fileLocations) {\n-                        \n-                        Path path = new Path(fileLocation);\n-                        \n-                        boolean pullSize = conf.getBoolean(CACHE_RETRIEVE_SIZE, false);\n-                        long length = Long.MAX_VALUE;\n-                        if (pullSize) {\n-                            length = path.getFileSystem(conf).getFileStatus(path).getLen();\n-                        }\n-                        \n-                        String[] locations = new String[] {location};\n-                        \n-                        binnedRanges.put(range, new RfileSplit(path, 0, length, locations));\n-                        \n-                        rowMap.put(range.getStartKey().getRow(), range);\n-                    }\n-                } else {\n-                    log.warn(\"Unable to find a some files associated with \" + range + \" : \" + location);\n-                }\n-            }\n+\n+        inputSplitList.add(compositeInputSplit);\n+      }\n+    } else {\n+\n+      for (Text row : rowMap.keySet()) {\n+\n+        Collection<Range> rangeColl = rowMap.get(row);\n+\n+        Set<RfileSplit> rfiles = Sets.newHashSet();\n+\n+        for (Range range : rangeColl) {\n+          Collection<RfileSplit> rangeSplits = binnedRanges.get(range);\n+          rfiles.addAll(rangeSplits);\n         }\n-        \n-        boolean mergeRanges = conf.getBoolean(MERGE_RANGE, true);\n-        \n-        if (!mergeRanges) {\n-            for (Range range : binnedRanges.keySet()) {\n-                Collection<RfileSplit> rangeSplits = binnedRanges.get(range);\n-                \n-                if (rangeSplits.isEmpty())\n-                    continue;\n-                TabletSplitSplit compositeInputSplit = new TabletSplitSplit(rangeSplits.size());\n-                compositeInputSplit.setTable(tableName);\n-                for (RfileSplit split : rangeSplits) {\n-                    compositeInputSplit.add(new FileRangeSplit(range, split.path, 0, split.length, split.hosts));\n-                }\n-                \n-                inputSplitList.add(compositeInputSplit);\n-            }\n-        } else {\n-            \n-            for (Text row : rowMap.keySet()) {\n-                \n-                Collection<Range> rangeColl = rowMap.get(row);\n-                \n-                Set<RfileSplit> rfiles = Sets.newHashSet();\n-                \n-                for (Range range : rangeColl) {\n-                    Collection<RfileSplit> rangeSplits = binnedRanges.get(range);\n-                    rfiles.addAll(rangeSplits);\n-                }\n-                \n-                TabletSplitSplit compositeInputSplit = new TabletSplitSplit(rfiles.size());\n-                compositeInputSplit.setTable(tableName);\n-                for (RfileSplit split : rfiles) {\n-                    \n-                    compositeInputSplit.add(new FileRangeSplit(rangeColl, split.path, 0, split.length, split.hosts));\n-                    \n-                }\n-                \n-                inputSplitList.add(compositeInputSplit);\n-            }\n+\n+        TabletSplitSplit compositeInputSplit = new TabletSplitSplit(rfiles.size());\n+        compositeInputSplit.setTable(tableName);\n+        for (RfileSplit split : rfiles) {\n+\n+          compositeInputSplit.add(new FileRangeSplit(rangeColl, split.path, 0, split.length, split.hosts));\n+\n         }\n-        \n-        if (log.isTraceEnabled())\n-            log.trace(\"Size is \" + inputSplitList.size());\n-        \n-        return inputSplitList;\n+\n+        inputSplitList.add(compositeInputSplit);\n+      }\n     }\n+\n+    if (log.isTraceEnabled()) {\n+      log.trace(\"Size is \" + inputSplitList.size());\n+    }\n+\n+    return inputSplitList;\n+  }\n }\n",
            "diff_size": 364
        },
        {
            "tool": "naturalize",
            "errors": [
                {
                    "line": "25",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/34/MultiRfileInputformat.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/naturalize/34/MultiRfileInputformat.java\nindex 69589684ebb..a6e6c10f040 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/34/MultiRfileInputformat.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/naturalize/34/MultiRfileInputformat.java\n@@ -84,8 +84,7 @@ public class MultiRfileInputformat extends RFileInputFormat {\n         \n         String tableName = BulkInputFormat.getTablename(job.getConfiguration());\n         boolean autoAdjust = BulkInputFormat.getAutoAdjustRanges(job.getConfiguration());\n-        List<Range> ranges = autoAdjust ? Range.mergeOverlapping(BulkInputFormat.getRanges(job.getConfiguration())) : BulkInputFormat.getRanges(job\n-                        .getConfiguration());\n+        List<Range> ranges = autoAdjust ? Range.mergeOverlapping(BulkInputFormat.getRanges(job.getConfiguration())) : BulkInputFormat.getRanges(job.getConfiguration());\n         \n         if (ranges.isEmpty()) {\n             ranges = Lists.newArrayListWithCapacity(1);\n@@ -137,8 +136,7 @@ public class MultiRfileInputformat extends RFileInputFormat {\n     \n     public static List<InputSplit> computeSplitPoints(Connector conn, Configuration conf, String tableName, List<Range> ranges) throws TableNotFoundException,\n                     AccumuloException, AccumuloSecurityException, IOException, InterruptedException {\n-        \n-        final Multimap<Range,RfileSplit> binnedRanges = ArrayListMultimap.create();\n+    final Multimap<Range,RfileSplit> binnedRanges = ArrayListMultimap.create();\n         \n         final Instance instance = conn.getInstance();\n         final PasswordToken token = new PasswordToken(BulkInputFormat.getPassword(conf));\n@@ -155,8 +153,7 @@ public class MultiRfileInputformat extends RFileInputFormat {\n          * Attempt the following 1) try to get the default namespace from accumulo 2) Use the custom config option 3) use default name in the hdfs configuration\n          */\n         if (dfsUriMap.get(tableId) == null || dfsDirMap.get(tableId) == null) {\n-            \n-            synchronized (MultiRfileInputformat.class) {\n+    synchronized (MultiRfileInputformat.class) {\n                 final InstanceOperations instOps = conn.instanceOperations();\n                 dfsUriMap.put(tableId, instOps.getSystemConfiguration().get(Property.INSTANCE_DFS_URI.getKey()));\n                 dfsDirMap.put(tableId, instOps.getSystemConfiguration().get(Property.INSTANCE_DFS_DIR.getKey()));\n@@ -232,7 +229,7 @@ public class MultiRfileInputformat extends RFileInputFormat {\n                         Path path = new Path(fileLocation);\n                         \n                         boolean pullSize = conf.getBoolean(CACHE_RETRIEVE_SIZE, false);\n-                        long length = Long.MAX_VALUE;\n+long length = Long.MAX_VALUE;\n                         if (pullSize) {\n                             length = path.getFileSystem(conf).getFileStatus(path).getLen();\n                         }\n@@ -295,4 +292,4 @@ public class MultiRfileInputformat extends RFileInputFormat {\n         \n         return inputSplitList;\n     }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 8
        },
        {
            "tool": "codebuff",
            "errors": [
                {
                    "line": "23",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/34/MultiRfileInputformat.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/codebuff/34/MultiRfileInputformat.java\nindex 69589684ebb..2d4e8e3a7b7 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/errored/1/34/MultiRfileInputformat.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/NationalSecurityAgency-datawave/codebuff/34/MultiRfileInputformat.java\n@@ -6,7 +6,6 @@ import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n-\n import java.util.concurrent.TimeUnit;\n import datawave.ingest.data.config.ingest.AccumuloHelper;\n import datawave.ingest.mapreduce.job.RFileInputFormat;\n@@ -14,7 +13,6 @@ import datawave.mr.bulk.split.FileRangeSplit;\n import datawave.mr.bulk.split.RfileSplit;\n import datawave.mr.bulk.split.TabletSplitSplit;\n import datawave.query.util.Tuple2;\n-\n import org.apache.accumulo.core.client.AccumuloException;\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.core.client.Connector;\n@@ -35,7 +33,6 @@ import org.apache.hadoop.mapreduce.JobContext;\n import org.apache.hadoop.mapreduce.RecordReader;\n import org.apache.hadoop.mapreduce.TaskAttemptContext;\n import org.apache.log4j.Logger;\n-\n import com.google.common.cache.CacheBuilder;\n import com.google.common.cache.LoadingCache;\n import com.google.common.collect.ArrayListMultimap;\n@@ -44,6 +41,7 @@ import com.google.common.collect.Multimap;\n import com.google.common.collect.Sets;\n import com.google.common.collect.TreeMultimap;\n \n+\n public class MultiRfileInputformat extends RFileInputFormat {\n     /**\n      * The following can be set via your configuration to override the default namespace\n@@ -52,23 +50,26 @@ public class MultiRfileInputformat extends RFileInputFormat {\n     /**\n      * Merge ranges can be overrided to the default of true so that you don't merge on the hosting tablet server\n      */\n+\n     public static final String MERGE_RANGE = \"merge.range\";\n+\n     public static final String CACHE_METADATA = \"rfile.cache.metdata\";\n+\n     public static final String CACHE_METADATA_EXPIRE_SECONDS = \"rfile.cache.expire.seconds\";\n+\n     public static final String CACHE_RETRIEVE_SIZE = \"rfile.size.compute\";\n+\n     public static final String CACHE_METADATA_SIZE = \"rfile.cache.metdata.size\";\n     private static final String HDFS_BASE = \"hdfs://\";\n     private static final String ACCUMULO_BASE_PATH = \"/accumulo\";\n-    \n     private static final String FS_DEFAULT_NAME = \"fs.default.name\";\n     private static final Logger log = Logger.getLogger(MultiRfileInputformat.class);\n+\n     public static final String tableStr = Path.SEPARATOR + \"tables\" + Path.SEPARATOR;\n-    \n     private static LoadingCache<Range,Set<Tuple2<String,Set<String>>>> locationMap = null;\n-    \n     protected static Map<String,String> dfsUriMap = new ConcurrentHashMap<>();\n     protected static Map<String,String> dfsDirMap = new ConcurrentHashMap<>();\n-    \n+\n     @Override\n     public RecordReader<Key,Value> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {\n         return new RangeRecordReader();\n@@ -77,36 +78,32 @@ public class MultiRfileInputformat extends RFileInputFormat {\n     /**\n      * Return the lists of computed slit points\n      */\n+\n     public List<InputSplit> getSplits(JobContext job) throws IOException {\n-        \n         AccumuloHelper cbHelper = new AccumuloHelper();\n         cbHelper.setup(job.getConfiguration());\n-        \n+\n         String tableName = BulkInputFormat.getTablename(job.getConfiguration());\n         boolean autoAdjust = BulkInputFormat.getAutoAdjustRanges(job.getConfiguration());\n-        List<Range> ranges = autoAdjust ? Range.mergeOverlapping(BulkInputFormat.getRanges(job.getConfiguration())) : BulkInputFormat.getRanges(job\n-                        .getConfiguration());\n-        \n+        List<Range> ranges = autoAdjust ? Range.mergeOverlapping(BulkInputFormat.getRanges(job.getConfiguration())) : BulkInputFormat.getRanges(job.getConfiguration());\n         if (ranges.isEmpty()) {\n             ranges = Lists.newArrayListWithCapacity(1);\n             ranges.add(new Range());\n         }\n-        \n+\n         List<InputSplit> inputSplits = Lists.newArrayList();\n         try {\n             inputSplits = computeSplitPoints(job, tableName, ranges);\n         } catch (TableNotFoundException | AccumuloException | AccumuloSecurityException | InterruptedException e) {\n             throw new IOException(e);\n         }\n-        \n         return inputSplits;\n     }\n-    \n-    List<InputSplit> computeSplitPoints(JobContext job, String tableName, List<Range> ranges) throws TableNotFoundException, AccumuloException,\n-                    AccumuloSecurityException, IOException, InterruptedException {\n+\n+    List<InputSplit> computeSplitPoints(JobContext job, String tableName, List<Range> ranges) throws TableNotFoundException, AccumuloException, AccumuloSecurityException, IOException, InterruptedException {\n         return computeSplitPoints(job.getConfiguration(), tableName, ranges);\n     }\n-    \n+\n     public static void clearMetadataCache() {\n         synchronized (MultiRfileInputformat.class) {\n             if (null == locationMap) {\n@@ -115,94 +112,77 @@ public class MultiRfileInputformat extends RFileInputFormat {\n         }\n         locationMap.invalidateAll();\n     }\n-    \n+\n     public static void clearMetadataCache(Range range) {\n-        \n         synchronized (MultiRfileInputformat.class) {\n             if (null == locationMap) {\n                 return;\n             }\n         }\n-        \n         locationMap.invalidate(range);\n         locationMap.refresh(range);\n     }\n-    \n-    public static List<InputSplit> computeSplitPoints(Configuration conf, String tableName, List<Range> ranges) throws TableNotFoundException,\n-                    AccumuloException, AccumuloSecurityException, IOException, InterruptedException {\n+\n+    public static List<InputSplit> computeSplitPoints(Configuration conf, String tableName, List<Range> ranges) throws TableNotFoundException, AccumuloException, AccumuloSecurityException, IOException, InterruptedException {\n         final Instance instance = BulkInputFormat.getInstance(conf);\n         final PasswordToken token = new PasswordToken(BulkInputFormat.getPassword(conf));\n         return computeSplitPoints(instance.getConnector(BulkInputFormat.getUsername(conf), token), conf, tableName, ranges);\n     }\n-    \n-    public static List<InputSplit> computeSplitPoints(Connector conn, Configuration conf, String tableName, List<Range> ranges) throws TableNotFoundException,\n-                    AccumuloException, AccumuloSecurityException, IOException, InterruptedException {\n-        \n+\n+    public static List<InputSplit> computeSplitPoints(Connector conn, Configuration conf, String tableName, List<Range> ranges) throws TableNotFoundException, AccumuloException, AccumuloSecurityException, IOException, InterruptedException {\n         final Multimap<Range,RfileSplit> binnedRanges = ArrayListMultimap.create();\n-        \n         final Instance instance = conn.getInstance();\n         final PasswordToken token = new PasswordToken(BulkInputFormat.getPassword(conf));\n-        \n         final String tableId = conn.tableOperations().tableIdMap().get(tableName);\n-        \n         final List<InputSplit> inputSplitList = Lists.newArrayList();\n-        \n         Multimap<Text,Range> rowMap = TreeMultimap.create();\n-        \n         String defaultNamespace = null, basePath = null;\n         \n         /**\n          * Attempt the following 1) try to get the default namespace from accumulo 2) Use the custom config option 3) use default name in the hdfs configuration\n          */\n         if (dfsUriMap.get(tableId) == null || dfsDirMap.get(tableId) == null) {\n-            \n             synchronized (MultiRfileInputformat.class) {\n                 final InstanceOperations instOps = conn.instanceOperations();\n                 dfsUriMap.put(tableId, instOps.getSystemConfiguration().get(Property.INSTANCE_DFS_URI.getKey()));\n                 dfsDirMap.put(tableId, instOps.getSystemConfiguration().get(Property.INSTANCE_DFS_DIR.getKey()));\n             }\n         }\n-        \n         defaultNamespace = dfsUriMap.get(tableId);\n-        \n         if (StringUtils.isEmpty(defaultNamespace)) {\n             defaultNamespace = conf.get(FS_DEFAULT_NAMESPACE);\n-            \n             if (StringUtils.isEmpty(defaultNamespace)) {\n                 defaultNamespace = conf.get(FS_DEFAULT_NAME);\n             }\n         }\n-        \n         basePath = dfsDirMap.get(tableId);\n-        \n         if (StringUtils.isEmpty(basePath)) {\n             basePath = ACCUMULO_BASE_PATH;\n         }\n         \n         // ensure we have a separator\n+\n+\n         if (!basePath.startsWith(Path.SEPARATOR)) {\n             basePath = Path.SEPARATOR + basePath;\n         }\n         \n         // must get the default base path since accumulo only stores the full namespace path\n         // when one is not stored on the default.\n+\n         final String defaultBasePath = defaultNamespace + basePath;\n-        \n         if (conf.getBoolean(CACHE_METADATA, false) == true) {\n             synchronized (MultiRfileInputformat.class) {\n                 if (null == locationMap) {\n                     final long size = conf.getLong(CACHE_METADATA_SIZE, 10000);\n                     final long seconds = conf.getInt(CACHE_METADATA_EXPIRE_SECONDS, 7200);\n-                    locationMap = CacheBuilder.newBuilder().maximumSize(size).expireAfterWrite(seconds, TimeUnit.SECONDS)\n-                                    .build(new MetadataCacheLoader(instance.getConnector(BulkInputFormat.getUsername(conf), token), defaultBasePath));\n+                    locationMap = CacheBuilder.newBuilder().maximumSize(size).expireAfterWrite(seconds, TimeUnit.SECONDS).build(new MetadataCacheLoader(instance.getConnector(BulkInputFormat.getUsername(conf), token), defaultBasePath));\n                 }\n             }\n         }\n-        \n         for (Range range : ranges) {\n             // turn this range into a range of rows against the accumulo metadata: (e.g. <tableId>;row)\n             Range metadataRange = MetadataCacheLoader.createMetadataRange(tableId, range);\n-            \n             Set<Tuple2<String,Set<String>>> metadataEntries;\n             try {\n                 if (null == locationMap) {\n@@ -213,34 +193,28 @@ public class MultiRfileInputformat extends RFileInputFormat {\n             } catch (Exception e) {\n                 throw new RuntimeException(\"Unable to get rfile locations from accumulo metadata\", e);\n             }\n-            \n+\n+\n             if (metadataEntries == null || metadataEntries.isEmpty()) {\n                 throw new IOException(\"Unable to find location or files associated with \" + range);\n             }\n-            \n             for (Tuple2<String,Set<String>> entry : metadataEntries) {\n                 String location = entry.first();\n                 Set<String> fileLocations = entry.second();\n                 if (fileLocations != null && !fileLocations.isEmpty()) {\n-                    \n                     if (location == null || location.isEmpty()) {\n                         log.warn(\"Unable to find a location associated with \" + range + \" : ? -> \" + fileLocations);\n                     }\n-                    \n                     for (String fileLocation : fileLocations) {\n-                        \n                         Path path = new Path(fileLocation);\n-                        \n                         boolean pullSize = conf.getBoolean(CACHE_RETRIEVE_SIZE, false);\n                         long length = Long.MAX_VALUE;\n                         if (pullSize) {\n                             length = path.getFileSystem(conf).getFileStatus(path).getLen();\n                         }\n-                        \n+\n                         String[] locations = new String[] {location};\n-                        \n                         binnedRanges.put(range, new RfileSplit(path, 0, length, locations));\n-                        \n                         rowMap.put(range.getStartKey().getRow(), range);\n                     }\n                 } else {\n@@ -248,51 +222,44 @@ public class MultiRfileInputformat extends RFileInputFormat {\n                 }\n             }\n         }\n-        \n+\n         boolean mergeRanges = conf.getBoolean(MERGE_RANGE, true);\n-        \n         if (!mergeRanges) {\n             for (Range range : binnedRanges.keySet()) {\n                 Collection<RfileSplit> rangeSplits = binnedRanges.get(range);\n-                \n                 if (rangeSplits.isEmpty())\n                     continue;\n+\n                 TabletSplitSplit compositeInputSplit = new TabletSplitSplit(rangeSplits.size());\n                 compositeInputSplit.setTable(tableName);\n+\n                 for (RfileSplit split : rangeSplits) {\n                     compositeInputSplit.add(new FileRangeSplit(range, split.path, 0, split.length, split.hosts));\n                 }\n-                \n                 inputSplitList.add(compositeInputSplit);\n             }\n         } else {\n-            \n             for (Text row : rowMap.keySet()) {\n-                \n                 Collection<Range> rangeColl = rowMap.get(row);\n-                \n                 Set<RfileSplit> rfiles = Sets.newHashSet();\n-                \n                 for (Range range : rangeColl) {\n                     Collection<RfileSplit> rangeSplits = binnedRanges.get(range);\n                     rfiles.addAll(rangeSplits);\n                 }\n-                \n+\n                 TabletSplitSplit compositeInputSplit = new TabletSplitSplit(rfiles.size());\n                 compositeInputSplit.setTable(tableName);\n+\n                 for (RfileSplit split : rfiles) {\n-                    \n                     compositeInputSplit.add(new FileRangeSplit(rangeColl, split.path, 0, split.length, split.hosts));\n-                    \n                 }\n-                \n                 inputSplitList.add(compositeInputSplit);\n             }\n         }\n-        \n+\n+\n         if (log.isTraceEnabled())\n             log.trace(\"Size is \" + inputSplitList.size());\n-        \n         return inputSplitList;\n     }\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 84
        },
        {
            "tool": "styler_random",
            "errors": [
                {
                    "line": "25",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        },
        {
            "tool": "styler_three_grams",
            "errors": [
                {
                    "line": "25",
                    "severity": "error",
                    "message": "Accumulo non-public classes imported",
                    "source": "com.puppycrawl.tools.checkstyle.checks.regexp.RegexpSinglelineJavaCheck"
                }
            ],
            "diff": "",
            "diff_size": 0
        }
    ],
    "repaired_by": [],
    "not_repaired_by": [
        "styler",
        "intellij",
        "naturalize",
        "codebuff",
        "styler_random",
        "styler_three_grams"
    ]
}