{
    "project_name": "spark-root-laurelin",
    "error_id": "12",
    "information": {
        "errors": [
            {
                "line": "513",
                "column": "40",
                "severity": "warning",
                "message": "'new' has incorrect indentation level 39, expected level should be 40.",
                "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
            }
        ]
    },
    "source_code": "                                    .maximumSize(100)\n                                    .build(\n                                       new CacheLoader<DataSourceReaderKey,\n                                                       DataSourceReader>() {\n                                            @Override\n                                            public DataSourceReader load(DataSourceReaderKey key) {",
    "results": [
        {
            "tool": "styler",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/errored/1/12/Root.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/styler/12/Root.java\nindex 2d4975cc39e..b87013ff2f4 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/errored/1/12/Root.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/styler/12/Root.java\n@@ -510,7 +510,7 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                                     .softValues()\n                                     .maximumSize(100)\n                                     .build(\n-                                       new CacheLoader<DataSourceReaderKey,\n+                                        new CacheLoader<DataSourceReaderKey,\n                                                        DataSourceReader>() {\n                                             @Override\n                                             public DataSourceReader load(DataSourceReaderKey key) {\n",
            "diff_size": 1
        },
        {
            "tool": "intellij",
            "errors": [
                {
                    "line": "380",
                    "column": "62",
                    "severity": "warning",
                    "message": "'+' should be on a new line.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.OperatorWrapCheck"
                },
                {
                    "line": "381",
                    "column": "69",
                    "severity": "warning",
                    "message": "'+' should be on a new line.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.OperatorWrapCheck"
                },
                {
                    "line": "400",
                    "column": "61",
                    "severity": "warning",
                    "message": "'||' should be on a new line.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.OperatorWrapCheck"
                },
                {
                    "line": "403",
                    "column": "61",
                    "severity": "warning",
                    "message": "'||' should be on a new line.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.OperatorWrapCheck"
                },
                {
                    "line": "406",
                    "column": "62",
                    "severity": "warning",
                    "message": "'||' should be on a new line.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.OperatorWrapCheck"
                },
                {
                    "line": "407",
                    "column": "54",
                    "severity": "warning",
                    "message": "'||' should be on a new line.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.OperatorWrapCheck"
                },
                {
                    "line": "424",
                    "column": "66",
                    "severity": "warning",
                    "message": "'+' should be on a new line.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.OperatorWrapCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/errored/1/12/Root.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/intellij/12/Root.java\nindex 2d4975cc39e..975cb765586 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/errored/1/12/Root.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/intellij/12/Root.java\n@@ -72,7 +72,8 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n      * <p>This is instantiated on the driver, then serialized and transmitted to\n      * the executor\n      */\n-    static class TTreeDataSourceV2Partition implements InputPartition<ColumnarBatch> {\n+    static class TTreeDataSourceV2Partition\n+        implements InputPartition<ColumnarBatch> {\n         private static final long serialVersionUID = -6598704946339913432L;\n         private StructType schema;\n         private long entryStart;\n@@ -83,7 +84,13 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n         private CollectionAccumulator<Storage> profileData;\n         private int pid;\n \n-        public TTreeDataSourceV2Partition(StructType schema, CacheFactory basketCacheFactory, long entryStart, long entryEnd, Map<String, SlimTBranch> slimBranches, int threadCount, CollectionAccumulator<Storage> profileData, int pid) {\n+        public TTreeDataSourceV2Partition(StructType schema,\n+                                          CacheFactory basketCacheFactory,\n+                                          long entryStart, long entryEnd,\n+                                          Map<String, SlimTBranch> slimBranches,\n+                                          int threadCount,\n+                                          CollectionAccumulator<Storage> profileData,\n+                                          int pid) {\n             logger.trace(\"dsv2partition new\");\n             this.schema = schema;\n             this.basketCacheFactory = basketCacheFactory;\n@@ -98,7 +105,9 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n         @Override\n         public InputPartitionReader<ColumnarBatch> createPartitionReader() {\n             logger.trace(\"input partition reader\");\n-            return new TTreeDataSourceV2PartitionReader(basketCacheFactory, schema, entryStart, entryEnd, slimBranches, threadCount, profileData, pid);\n+            return new TTreeDataSourceV2PartitionReader(basketCacheFactory,\n+                schema, entryStart, entryEnd, slimBranches, threadCount,\n+                profileData, pid);\n         }\n \n         public void setPid(int pid) {\n@@ -106,7 +115,8 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n         }\n     }\n \n-    static class TTreeDataSourceV2PartitionReader implements InputPartitionReader<ColumnarBatch> {\n+    static class TTreeDataSourceV2PartitionReader\n+        implements InputPartitionReader<ColumnarBatch> {\n         private Cache basketCache;\n         private StructType schema;\n         private long entryStart;\n@@ -133,16 +143,18 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n          *  see also: https://stackoverflow.com/a/10395700\n          */\n         static {\n-            ThreadFactory factory = new ThreadFactoryBuilder().setNameFormat(\"laurelin-arraybuilder-%d\").build();\n+            ThreadFactory factory = new ThreadFactoryBuilder()\n+                .setNameFormat(\"laurelin-arraybuilder-%d\").build();\n             staticExecutor = new ThreadPoolExecutor(1, 1,\n-                                                    5L, TimeUnit.SECONDS,\n-                                                    new LinkedBlockingQueue<Runnable>(),\n-                                                    factory);\n+                5L, TimeUnit.SECONDS,\n+                new LinkedBlockingQueue<Runnable>(),\n+                factory);\n             staticExecutor.allowCoreThreadTimeOut(true);\n             Runtime.getRuntime().addShutdownHook(new Thread() {\n                 @Override\n                 public void run() {\n-                    TTreeDataSourceV2PartitionReader.staticExecutor.shutdownNow();\n+                    TTreeDataSourceV2PartitionReader.staticExecutor\n+                        .shutdownNow();\n                 }\n             });\n         }\n@@ -155,7 +167,13 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n         private int pid;\n         private static ROOTFileCache fileCache = new ROOTFileCache();\n \n-        public TTreeDataSourceV2PartitionReader(CacheFactory basketCacheFactory, StructType schema, long entryStart, long entryEnd, Map<String, SlimTBranch> slimBranches, int threadCount, CollectionAccumulator<Storage> profileData, int pid) {\n+        public TTreeDataSourceV2PartitionReader(CacheFactory basketCacheFactory,\n+                                                StructType schema,\n+                                                long entryStart, long entryEnd,\n+                                                Map<String, SlimTBranch> slimBranches,\n+                                                int threadCount,\n+                                                CollectionAccumulator<Storage> profileData,\n+                                                int pid) {\n             this.basketCache = basketCacheFactory.getCache();\n             this.schema = schema;\n             this.entryStart = entryStart;\n@@ -210,7 +228,7 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n             // This is miserable\n             ColumnVector[] tmp = new ColumnVector[vecs.size()];\n             int idx = 0;\n-            for (ColumnVector vec: vecs) {\n+            for (ColumnVector vec : vecs) {\n                 tmp[idx] = vec;\n                 idx += 1;\n             }\n@@ -220,28 +238,37 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n             return ret;\n         }\n \n-        private LinkedList<ColumnVector> getBatchRecursive(StructField[] structFields) {\n+        private LinkedList<ColumnVector> getBatchRecursive(\n+            StructField[] structFields) {\n             LinkedList<ColumnVector> vecs = new LinkedList<ColumnVector>();\n-            for (StructField field: structFields)  {\n+            for (StructField field : structFields) {\n                 if (field.dataType() instanceof StructType) {\n-                    LinkedList<ColumnVector> nestedVecs = getBatchRecursive(((StructType)field.dataType()).fields());\n-                    vecs.add(new StructColumnVector(field.dataType(), nestedVecs));\n+                    LinkedList<ColumnVector> nestedVecs = getBatchRecursive(\n+                        ((StructType) field.dataType()).fields());\n+                    vecs.add(\n+                        new StructColumnVector(field.dataType(), nestedVecs));\n                     continue;\n                 }\n-                SlimTBranchInterface slimBranch = slimBranches.get(field.name());\n+                SlimTBranchInterface slimBranch =\n+                    slimBranches.get(field.name());\n                 SimpleType rootType;\n-                rootType = SimpleType.fromString(field.metadata().getString(\"rootType\"));\n-\n-                Dtype dtype = SimpleType.dtypeFromString(field.metadata().getString(\"rootType\"));\n-                vecs.add(new TTreeColumnVector(field.dataType(), rootType, dtype, basketCache, entryStart, entryEnd, slimBranch, executor, fileCache));\n+                rootType = SimpleType\n+                    .fromString(field.metadata().getString(\"rootType\"));\n+\n+                Dtype dtype = SimpleType\n+                    .dtypeFromString(field.metadata().getString(\"rootType\"));\n+                vecs.add(\n+                    new TTreeColumnVector(field.dataType(), rootType, dtype,\n+                        basketCache, entryStart, entryEnd, slimBranch, executor,\n+                        fileCache));\n             }\n             return vecs;\n         }\n     }\n \n     public static class TTreeDataSourceV2Reader implements DataSourceReader,\n-            SupportsScanColumnarBatch,\n-            SupportsPushDownRequiredColumns {\n+        SupportsScanColumnarBatch,\n+        SupportsPushDownRequiredColumns {\n         private LinkedList<String> paths;\n         private String treeName;\n         private TTree currTree;\n@@ -254,16 +281,20 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n         private SparkContext sparkContext;\n         private static ROOTFileCache fileCache = new ROOTFileCache();\n \n-        public TTreeDataSourceV2Reader(DataSourceOptions options, CacheFactory basketCacheFactory, SparkContext sparkContext, CollectionAccumulator<Storage> ioAccum) {\n+        public TTreeDataSourceV2Reader(DataSourceOptions options,\n+                                       CacheFactory basketCacheFactory,\n+                                       SparkContext sparkContext,\n+                                       CollectionAccumulator<Storage> ioAccum) {\n             logger.trace(\"construct ttreedatasourcev2reader\");\n             this.sparkContext = sparkContext;\n             try {\n                 this.paths = new LinkedList<String>();\n-                for (String path: options.paths()) {\n+                for (String path : options.paths()) {\n                     this.paths.addAll(IOFactory.expandPathToList(path));\n                 }\n                 // FIXME - More than one file, please\n-                currFile = TFile.getFromFile(fileCache.getROOTFile(this.paths.get(0)));\n+                currFile =\n+                    TFile.getFromFile(fileCache.getROOTFile(this.paths.get(0)));\n                 treeName = options.get(\"tree\").orElse(\"Events\");\n                 currTree = new TTree(currFile.getProxy(treeName), currFile);\n                 this.basketCacheFactory = basketCacheFactory;\n@@ -299,9 +330,10 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n             return schema;\n         }\n \n-        private List<StructField> readSchemaPart(List<TBranch> branches, String prefix) {\n+        private List<StructField> readSchemaPart(List<TBranch> branches,\n+                                                 String prefix) {\n             List<StructField> fields = new ArrayList<StructField>();\n-            for (TBranch branch: branches) {\n+            for (TBranch branch : branches) {\n                 // The ROOT-given branch name\n                 String name = branch.getName();\n                 try {\n@@ -327,21 +359,32 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                          * We have sub-branches, so we need to recurse.\n                          */\n                         String subname = name.substring(prefix.length());\n-                        List<StructField> subFields = readSchemaPart(branch.getBranches(), name);\n-                        StructField[] subFieldArray = new StructField[subFields.size()];\n+                        List<StructField> subFields =\n+                            readSchemaPart(branch.getBranches(), name);\n+                        StructField[] subFieldArray =\n+                            new StructField[subFields.size()];\n                         subFieldArray = subFields.toArray(subFieldArray);\n                         StructType subStruct = new StructType(subFieldArray);\n                         metadata.putString(\"rootType\", \"nested\");\n-                        fields.add(new StructField(currName, subStruct, false, Metadata.empty()));\n+                        fields.add(new StructField(currName, subStruct, false,\n+                            Metadata.empty()));\n                     } else if ((branchCount == 0) && (leafCount == 1)) {\n-                        DataType sparkType = rootToSparkType(branch.getSimpleType());\n-                        metadata.putString(\"rootType\", branch.getSimpleType().getBaseType().toString());\n-                        fields.add(new StructField(currName, sparkType, false, metadata.build()));\n+                        DataType sparkType =\n+                            rootToSparkType(branch.getSimpleType());\n+                        metadata.putString(\"rootType\",\n+                            branch.getSimpleType().getBaseType().toString());\n+                        fields.add(new StructField(currName, sparkType, false,\n+                            metadata.build()));\n                     } else {\n-                        throw new RuntimeException(\"Unsupported schema for branch \" + branch.getName() + \" branchCount: \" + branchCount + \" leafCount: \" + leafCount);\n+                        throw new RuntimeException(\n+                            \"Unsupported schema for branch \" +\n+                                branch.getName() + \" branchCount: \" +\n+                                branchCount + \" leafCount: \" + leafCount);\n                     }\n                 } catch (UnsupportedBranchTypeException e) {\n-                    logger.error(String.format(\"The branch \\\"%s\\\" is unable to be deserialized and will be skipped\", name));\n+                    logger.error(String.format(\n+                        \"The branch \\\"%s\\\" is unable to be deserialized and will be skipped\",\n+                        name));\n                 }\n             }\n             return fields;\n@@ -354,11 +397,15 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                     ret = DataTypes.BooleanType;\n                 } else if (simpleType == SimpleType.Int8) {\n                     ret = DataTypes.ByteType;\n-                } else if ((simpleType == SimpleType.Int16) || (simpleType == SimpleType.UInt8)) {\n+                } else if ((simpleType == SimpleType.Int16) ||\n+                    (simpleType == SimpleType.UInt8)) {\n                     ret = DataTypes.ShortType;\n-                } else if ((simpleType == SimpleType.Int32) || (simpleType == SimpleType.UInt16)) {\n+                } else if ((simpleType == SimpleType.Int32) ||\n+                    (simpleType == SimpleType.UInt16)) {\n                     ret = DataTypes.IntegerType;\n-                } else if ((simpleType == SimpleType.UInt64) || (simpleType == SimpleType.Int64) || (simpleType == SimpleType.UInt32)) {\n+                } else if ((simpleType == SimpleType.UInt64) ||\n+                    (simpleType == SimpleType.Int64) ||\n+                    (simpleType == SimpleType.UInt32)) {\n                     ret = DataTypes.LongType;\n                 } else if (simpleType == SimpleType.Float32) {\n                     ret = DataTypes.FloatType;\n@@ -368,11 +415,14 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                     ret = DataTypes.LongType;\n                 }\n             } else if (simpleType instanceof SimpleType.ArrayType) {\n-                SimpleType nested = ((SimpleType.ArrayType) simpleType).getChildType();\n+                SimpleType nested =\n+                    ((SimpleType.ArrayType) simpleType).getChildType();\n                 ret = DataTypes.createArrayType(rootToSparkType(nested), false);\n             }\n             if (ret == null) {\n-                throw new RuntimeException(\"Unable to convert ROOT type '\" + simpleType + \"' to Spark\");\n+                throw new RuntimeException(\n+                    \"Unable to convert ROOT type '\" + simpleType +\n+                        \"' to Spark\");\n             }\n             return ret;\n         }\n@@ -388,58 +438,84 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n             int threadCount;\n             CacheFactory basketCacheFactory;\n \n-            public PartitionHelper(String treeName, StructType schema, int threadCount, CacheFactory basketCacheFactory) {\n+            public PartitionHelper(String treeName, StructType schema,\n+                                   int threadCount,\n+                                   CacheFactory basketCacheFactory) {\n                 this.treeName = treeName;\n                 this.schema = schema;\n                 this.threadCount = threadCount;\n                 this.basketCacheFactory = basketCacheFactory;\n             }\n \n-            private static void parseStructFields(TTree inputTree, Map<String, SlimTBranch> slimBranches, StructType struct, String namespace) {\n-                for (StructField field: struct.fields())  {\n+            private static void parseStructFields(TTree inputTree,\n+                                                  Map<String, SlimTBranch> slimBranches,\n+                                                  StructType struct,\n+                                                  String namespace) {\n+                for (StructField field : struct.fields()) {\n                     if (field.dataType() instanceof StructType) {\n-                        parseStructFields(inputTree, slimBranches, (StructType) field.dataType(), namespace + field.name() + \".\");\n+                        parseStructFields(inputTree, slimBranches,\n+                            (StructType) field.dataType(),\n+                            namespace + field.name() + \".\");\n                     }\n-                    ArrayList<TBranch> branchList = inputTree.getBranches(namespace + field.name());\n+                    ArrayList<TBranch> branchList =\n+                        inputTree.getBranches(namespace + field.name());\n                     assert branchList.size() == 1;\n                     TBranch fatBranch = branchList.get(0);\n-                    SlimTBranch slimBranch = SlimTBranch.getFromTBranch(fatBranch);\n+                    SlimTBranch slimBranch =\n+                        SlimTBranch.getFromTBranch(fatBranch);\n                     slimBranches.put(fatBranch.getName(), slimBranch);\n                 }\n             }\n \n-            public static Iterator<InputPartition<ColumnarBatch>> partitionSingleFileImpl(String path, String treeName, StructType schema, int threadCount, CacheFactory basketCacheFactory) {\n-                List<InputPartition<ColumnarBatch>> ret = new ArrayList<InputPartition<ColumnarBatch>>();\n+            public static Iterator<InputPartition<ColumnarBatch>> partitionSingleFileImpl(\n+                String path, String treeName, StructType schema,\n+                int threadCount, CacheFactory basketCacheFactory) {\n+                List<InputPartition<ColumnarBatch>> ret =\n+                    new ArrayList<InputPartition<ColumnarBatch>>();\n                 int pid = 0;\n                 TTree inputTree;\n \n                 try {\n-                    TFile inputFile = TFile.getFromFile(fileCache.getROOTFile(path));\n-                    inputTree = new TTree(inputFile.getProxy(treeName), inputFile);\n+                    TFile inputFile =\n+                        TFile.getFromFile(fileCache.getROOTFile(path));\n+                    inputTree =\n+                        new TTree(inputFile.getProxy(treeName), inputFile);\n \n-                    Map<String, SlimTBranch> slimBranches = new HashMap<String, SlimTBranch>();\n+                    Map<String, SlimTBranch> slimBranches =\n+                        new HashMap<String, SlimTBranch>();\n                     parseStructFields(inputTree, slimBranches, schema, \"\");\n \n                     // TODO We partition based on a fixed number of events per\n                     //      partition, which isn't smart. Redo it with something\n                     //      smarter later\n-                    long[] entryOffset = inputTree.getBranches().get(0).getBasketEntryOffsets();\n+                    long[] entryOffset =\n+                        inputTree.getBranches().get(0).getBasketEntryOffsets();\n                     long lastEntry = entryOffset[entryOffset.length - 1];\n                     for (int i = 0; i < lastEntry; i += PARTITION_SIZE) {\n                         pid += 1;\n                         long partitionStart = i;\n-                        long partitionEnd = Math.min(lastEntry, partitionStart + PARTITION_SIZE);\n-                        Map<String, SlimTBranch> trimmedSlimBranches = new HashMap<String, SlimTBranch>();\n-                        for (Entry<String, SlimTBranch> e: slimBranches.entrySet()) {\n-                            trimmedSlimBranches.put(e.getKey(), e.getValue().copyAndTrim(partitionStart, partitionEnd));\n+                        long partitionEnd = Math.min(lastEntry,\n+                            partitionStart + PARTITION_SIZE);\n+                        Map<String, SlimTBranch> trimmedSlimBranches =\n+                            new HashMap<String, SlimTBranch>();\n+                        for (Entry<String, SlimTBranch> e : slimBranches\n+                            .entrySet()) {\n+                            trimmedSlimBranches.put(e.getKey(), e.getValue()\n+                                .copyAndTrim(partitionStart, partitionEnd));\n                         }\n-                        ret.add(new TTreeDataSourceV2Partition(schema, basketCacheFactory, partitionStart, partitionEnd, trimmedSlimBranches, threadCount, profileData, pid));\n+                        ret.add(new TTreeDataSourceV2Partition(schema,\n+                            basketCacheFactory, partitionStart, partitionEnd,\n+                            trimmedSlimBranches, threadCount, profileData,\n+                            pid));\n                     }\n                     if (ret.size() == 0) {\n                         // Only one basket?\n-                        logger.debug(\"Planned for zero baskets, adding a dummy one\");\n+                        logger.debug(\n+                            \"Planned for zero baskets, adding a dummy one\");\n                         pid += 1;\n-                        ret.add(new TTreeDataSourceV2Partition(schema, basketCacheFactory, 0, inputTree.getEntries(), slimBranches, threadCount, profileData, pid));\n+                        ret.add(new TTreeDataSourceV2Partition(schema,\n+                            basketCacheFactory, 0, inputTree.getEntries(),\n+                            slimBranches, threadCount, profileData, pid));\n                     }\n                     return ret.iterator();\n                 } catch (Exception e) {\n@@ -449,35 +525,46 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n             }\n \n             FlatMapFunction<String, InputPartition<ColumnarBatch>> getLambda() {\n-                return s -> PartitionHelper.partitionSingleFileImpl(s, treeName, schema, threadCount, basketCacheFactory);\n+                return s -> PartitionHelper\n+                    .partitionSingleFileImpl(s, treeName, schema, threadCount,\n+                        basketCacheFactory);\n             }\n         }\n \n         @Override\n         public List<InputPartition<ColumnarBatch>> planBatchInputPartitions() {\n             logger.trace(\"planbatchinputpartitions\");\n-            List<InputPartition<ColumnarBatch>> ret = new ArrayList<InputPartition<ColumnarBatch>>();\n+            List<InputPartition<ColumnarBatch>> ret =\n+                new ArrayList<InputPartition<ColumnarBatch>>();\n             if (sparkContext == null) {\n-                for (String path: paths) {\n-                    partitionSingleFile(path).forEachRemaining(ret::add);;\n+                for (String path : paths) {\n+                    partitionSingleFile(path).forEachRemaining(ret::add);\n+                    ;\n                 }\n             } else {\n-                JavaSparkContext sc = JavaSparkContext.fromSparkContext(sparkContext);\n+                JavaSparkContext sc =\n+                    JavaSparkContext.fromSparkContext(sparkContext);\n                 JavaRDD<String> rdd_paths = sc.parallelize(paths, paths.size());\n-                PartitionHelper helper = new PartitionHelper(treeName, schema, threadCount, basketCacheFactory);\n-                JavaRDD<InputPartition<ColumnarBatch>> partitions = rdd_paths.flatMap(helper.getLambda());\n+                PartitionHelper helper =\n+                    new PartitionHelper(treeName, schema, threadCount,\n+                        basketCacheFactory);\n+                JavaRDD<InputPartition<ColumnarBatch>> partitions =\n+                    rdd_paths.flatMap(helper.getLambda());\n                 ret = partitions.collect();\n             }\n             int pid = 0;\n-            for (InputPartition<ColumnarBatch> x: ret) {\n-                ((TTreeDataSourceV2Partition)x).setPid(pid);\n+            for (InputPartition<ColumnarBatch> x : ret) {\n+                ((TTreeDataSourceV2Partition) x).setPid(pid);\n                 pid += 1;\n             }\n             return ret;\n         }\n \n-        public Iterator<InputPartition<ColumnarBatch>> partitionSingleFile(String path) {\n-            return PartitionHelper.partitionSingleFileImpl(path, treeName, schema, threadCount, basketCacheFactory);\n+        public Iterator<InputPartition<ColumnarBatch>> partitionSingleFile(\n+            String path) {\n+            return PartitionHelper\n+                .partitionSingleFileImpl(path, treeName, schema, threadCount,\n+                    basketCacheFactory);\n         }\n \n         @Override\n@@ -505,29 +592,33 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n      * eagerly garbage collected\n      */\n     private static LoadingCache<DataSourceReaderKey,\n-                                DataSourceReader> dedupDataSource =\n-                                    CacheBuilder.newBuilder()\n-                                    .softValues()\n-                                    .maximumSize(100)\n-                                    .build(\n-                                       new CacheLoader<DataSourceReaderKey,\n-                                                       DataSourceReader>() {\n-                                            @Override\n-                                            public DataSourceReader load(DataSourceReaderKey key) {\n-                                                logger.trace(\"Construct new reader\");\n-                                                DataSourceOptions options = new DataSourceOptions(key.options);\n-                                                boolean traceIO = key.traceIO;\n-                                                SparkContext context = key.context;\n-                                                CacheFactory basketCacheFactory = new CacheFactory();\n-                                                if ((traceIO) && (context != null)) {\n-                                                    synchronized (Root.class) {\n-                                                        ioAccum = new CollectionAccumulator<Event.Storage>();\n-                                                        context.register(ioAccum, \"edu.vanderbilt.accre.laurelin.ioprofile\");\n-                                                    }\n-                                                }\n-                                                return new TTreeDataSourceV2Reader(options, basketCacheFactory, context, ioAccum);\n-                                            }\n-                                            });\n+        DataSourceReader> dedupDataSource =\n+        CacheBuilder.newBuilder()\n+            .softValues()\n+            .maximumSize(100)\n+            .build(\n+                new CacheLoader<DataSourceReaderKey,\n+                    DataSourceReader>() {\n+                    @Override\n+                    public DataSourceReader load(DataSourceReaderKey key) {\n+                        logger.trace(\"Construct new reader\");\n+                        DataSourceOptions options =\n+                            new DataSourceOptions(key.options);\n+                        boolean traceIO = key.traceIO;\n+                        SparkContext context = key.context;\n+                        CacheFactory basketCacheFactory = new CacheFactory();\n+                        if ((traceIO) && (context != null)) {\n+                            synchronized (Root.class) {\n+                                ioAccum =\n+                                    new CollectionAccumulator<Event.Storage>();\n+                                context.register(ioAccum,\n+                                    \"edu.vanderbilt.accre.laurelin.ioprofile\");\n+                            }\n+                        }\n+                        return new TTreeDataSourceV2Reader(options,\n+                            basketCacheFactory, context, ioAccum);\n+                    }\n+                });\n \n     static class DataSourceReaderKey {\n         @Override\n@@ -539,10 +630,11 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n         public boolean equals(Object obj) {\n             DataSourceReaderKey other = (DataSourceReaderKey) obj;\n             return (context == other.context) && options.equals(other.options)\n-                    && traceIO == other.traceIO;\n+                && traceIO == other.traceIO;\n         }\n \n-        public DataSourceReaderKey(DataSourceOptions options, SparkContext context, boolean traceIO) {\n+        public DataSourceReaderKey(DataSourceOptions options,\n+                                   SparkContext context, boolean traceIO) {\n             this.traceIO = traceIO;\n             this.context = context;\n             this.options = options.asMap();\n@@ -557,6 +649,7 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n     /**\n      * This is called by Spark, unlike the following function that accepts a\n      * SparkContext\n+     *\n      * @param options DS options\n      */\n     @Override\n@@ -566,14 +659,18 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n \n     /**\n      * Used for unit-tests when there is no current spark context\n+     *\n      * @param options DS options\n      * @param context spark context to use\n      * @param traceIO whether or not to trace the IO operations\n      * @return new reader\n      */\n-    public DataSourceReader createReader(DataSourceOptions options, SparkContext context, boolean traceIO) {\n+    public DataSourceReader createReader(DataSourceOptions options,\n+                                         SparkContext context,\n+                                         boolean traceIO) {\n         try {\n-            return dedupDataSource.get(new DataSourceReaderKey(options, context, traceIO));\n+            return dedupDataSource\n+                .get(new DataSourceReaderKey(options, context, traceIO));\n         } catch (ExecutionException e) {\n             throw new RuntimeException(\"Could not load DataSourceReader\", e);\n         }\n",
            "diff_size": 194
        },
        {
            "tool": "naturalize",
            "errors": [
                {
                    "line": "139",
                    "column": "1",
                    "severity": "warning",
                    "message": "'static initialization' child has incorrect indentation level 0, expected level should be 12.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "413",
                    "column": "1",
                    "severity": "warning",
                    "message": "'try' has incorrect indentation level 0, expected level should be 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "506",
                    "column": "1",
                    "severity": "warning",
                    "message": "'method def modifier' has incorrect indentation level 0, expected level should be one of the following: 8, 12.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "508",
                    "column": "49",
                    "severity": "warning",
                    "message": "'method def' child has incorrect indentation level 48, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "509",
                    "column": "49",
                    "severity": "warning",
                    "message": "'method def' child has incorrect indentation level 48, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "510",
                    "column": "49",
                    "severity": "warning",
                    "message": "'method def' child has incorrect indentation level 48, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "511",
                    "column": "49",
                    "severity": "warning",
                    "message": "'method def' child has incorrect indentation level 48, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "512",
                    "column": "49",
                    "severity": "warning",
                    "message": "'method def' child has incorrect indentation level 48, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "513",
                    "column": "49",
                    "severity": "warning",
                    "message": "'if' has incorrect indentation level 48, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "514",
                    "column": "53",
                    "severity": "warning",
                    "message": "'synchronized' has incorrect indentation level 52, expected level should be one of the following: 16, 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "515",
                    "column": "57",
                    "severity": "warning",
                    "message": "'synchronized' child has incorrect indentation level 56, expected level should be one of the following: 20, 24.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "516",
                    "column": "57",
                    "severity": "warning",
                    "message": "'synchronized' child has incorrect indentation level 56, expected level should be one of the following: 20, 24.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "517",
                    "column": "53",
                    "severity": "warning",
                    "message": "'synchronized rcurly' has incorrect indentation level 52, expected level should be one of the following: 16, 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "518",
                    "column": "49",
                    "severity": "warning",
                    "message": "'if rcurly' has incorrect indentation level 48, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "519",
                    "column": "49",
                    "severity": "warning",
                    "message": "'method def' child has incorrect indentation level 48, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "520",
                    "column": "45",
                    "severity": "warning",
                    "message": "'method def rcurly' has incorrect indentation level 44, expected level should be one of the following: 8, 12.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "522",
                    "column": "1",
                    "severity": "warning",
                    "message": "'CLASS_DEF' should be separated from previous line.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.EmptyLineSeparatorCheck"
                },
                {
                    "line": "522",
                    "column": "1",
                    "severity": "warning",
                    "message": "'class def modifier' has incorrect indentation level 0, expected level should be 4.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/errored/1/12/Root.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/naturalize/12/Root.java\nindex 2d4975cc39e..effb72660d3 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/errored/1/12/Root.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/naturalize/12/Root.java\n@@ -134,11 +134,9 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n          */\n         static {\n             ThreadFactory factory = new ThreadFactoryBuilder().setNameFormat(\"laurelin-arraybuilder-%d\").build();\n-            staticExecutor = new ThreadPoolExecutor(1, 1,\n-                                                    5L, TimeUnit.SECONDS,\n-                                                    new LinkedBlockingQueue<Runnable>(),\n+            staticExecutor = new ThreadPoolExecutor(1, 1, 5L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(),\n                                                     factory);\n-            staticExecutor.allowCoreThreadTimeOut(true);\n+staticExecutor.allowCoreThreadTimeOut(true);\n             Runtime.getRuntime().addShutdownHook(new Thread() {\n                 @Override\n                 public void run() {\n@@ -412,8 +410,7 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                 List<InputPartition<ColumnarBatch>> ret = new ArrayList<InputPartition<ColumnarBatch>>();\n                 int pid = 0;\n                 TTree inputTree;\n-\n-                try {\n+try {\n                     TFile inputFile = TFile.getFromFile(fileCache.getROOTFile(path));\n                     inputTree = new TTree(inputFile.getProxy(treeName), inputFile);\n \n@@ -504,15 +501,9 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n      * are transient, we need to make these soft references so they're not\n      * eagerly garbage collected\n      */\n-    private static LoadingCache<DataSourceReaderKey,\n-                                DataSourceReader> dedupDataSource =\n-                                    CacheBuilder.newBuilder()\n-                                    .softValues()\n-                                    .maximumSize(100)\n-                                    .build(\n-                                       new CacheLoader<DataSourceReaderKey,\n-                                                       DataSourceReader>() {\n-                                            @Override\n+    private static LoadingCache<DataSourceReaderKey,DataSourceReader> dedupDataSource = CacheBuilder.newBuilder().softValues().maximumSize(100).build(new CacheLoader<DataSourceReaderKey,DataSourceReader>() {\n+\n+@Override\n                                             public DataSourceReader load(DataSourceReaderKey key) {\n                                                 logger.trace(\"Construct new reader\");\n                                                 DataSourceOptions options = new DataSourceOptions(key.options);\n@@ -527,9 +518,8 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                                                 }\n                                                 return new TTreeDataSourceV2Reader(options, basketCacheFactory, context, ioAccum);\n                                             }\n-                                            });\n-\n-    static class DataSourceReaderKey {\n+    });\n+static class DataSourceReaderKey {\n         @Override\n         public int hashCode() {\n             return Objects.hash(context, options, traceIO);\n@@ -538,8 +528,7 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n         @Override\n         public boolean equals(Object obj) {\n             DataSourceReaderKey other = (DataSourceReaderKey) obj;\n-            return (context == other.context) && options.equals(other.options)\n-                    && traceIO == other.traceIO;\n+            return (context == other.context) && options.equals(other.options) && traceIO == other.traceIO;\n         }\n \n         public DataSourceReaderKey(DataSourceOptions options, SparkContext context, boolean traceIO) {\n@@ -584,4 +573,4 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n         return \"root\";\n     }\n \n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 21
        },
        {
            "tool": "codebuff",
            "errors": [
                {
                    "line": "137",
                    "column": "54",
                    "severity": "warning",
                    "message": "'method def modifier' has incorrect indentation level 53, expected level should be one of the following: 16, 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "139",
                    "column": "58",
                    "severity": "warning",
                    "message": "'method def' child has incorrect indentation level 57, expected level should be one of the following: 20, 24.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "140",
                    "column": "54",
                    "severity": "warning",
                    "message": "'method def rcurly' has incorrect indentation level 53, expected level should be one of the following: 16, 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "141",
                    "column": "1",
                    "severity": "warning",
                    "message": "'object def rcurly' has incorrect indentation level 0, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "163",
                    "column": "23",
                    "severity": "warning",
                    "message": "WhitespaceAround: '->' is not preceded with whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.WhitespaceAroundCheck"
                },
                {
                    "line": "164",
                    "column": "26",
                    "severity": "warning",
                    "message": "'block' child has incorrect indentation level 25, expected level should be 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "165",
                    "column": "26",
                    "severity": "warning",
                    "message": "'block' child has incorrect indentation level 25, expected level should be 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "166",
                    "column": "22",
                    "severity": "warning",
                    "message": "'block rcurly' has incorrect indentation level 21, expected level should be 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "235",
                    "column": "1",
                    "severity": "warning",
                    "message": "'SupportsScanColumnarBatch' has incorrect indentation level 0, expected level should be 8.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "236",
                    "column": "1",
                    "severity": "warning",
                    "message": "'SupportsPushDownRequiredColumns' has incorrect indentation level 0, expected level should be 8.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "270",
                    "column": "23",
                    "severity": "warning",
                    "message": "WhitespaceAround: '->' is not preceded with whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.WhitespaceAroundCheck"
                },
                {
                    "line": "271",
                    "column": "26",
                    "severity": "warning",
                    "message": "'block' child has incorrect indentation level 25, expected level should be 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "272",
                    "column": "26",
                    "severity": "warning",
                    "message": "'block' child has incorrect indentation level 25, expected level should be 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "273",
                    "column": "22",
                    "severity": "warning",
                    "message": "'block rcurly' has incorrect indentation level 21, expected level should be 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "334",
                    "column": "32",
                    "severity": "warning",
                    "message": "'else' child has incorrect indentation level 31, expected level should be 24.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "351",
                    "column": "28",
                    "severity": "warning",
                    "message": "'if' child has incorrect indentation level 27, expected level should be 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "352",
                    "column": "24",
                    "severity": "warning",
                    "message": "'if rcurly' has incorrect indentation level 23, expected level should be 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "353",
                    "column": "28",
                    "severity": "warning",
                    "message": "'if' child has incorrect indentation level 27, expected level should be 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "354",
                    "column": "24",
                    "severity": "warning",
                    "message": "'if rcurly' has incorrect indentation level 23, expected level should be 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "355",
                    "column": "35",
                    "severity": "warning",
                    "message": "'if' child has incorrect indentation level 34, expected level should be 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "356",
                    "column": "31",
                    "severity": "warning",
                    "message": "'if rcurly' has incorrect indentation level 30, expected level should be 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "357",
                    "column": "35",
                    "severity": "warning",
                    "message": "'if' child has incorrect indentation level 34, expected level should be 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "358",
                    "column": "31",
                    "severity": "warning",
                    "message": "'if rcurly' has incorrect indentation level 30, expected level should be 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "359",
                    "column": "42",
                    "severity": "warning",
                    "message": "'if' child has incorrect indentation level 41, expected level should be 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "360",
                    "column": "38",
                    "severity": "warning",
                    "message": "'if rcurly' has incorrect indentation level 37, expected level should be 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "361",
                    "column": "49",
                    "severity": "warning",
                    "message": "'if' child has incorrect indentation level 48, expected level should be 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "362",
                    "column": "45",
                    "severity": "warning",
                    "message": "'if rcurly' has incorrect indentation level 44, expected level should be 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "364",
                    "column": "24",
                    "severity": "warning",
                    "message": "'if' child has incorrect indentation level 23, expected level should be 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "365",
                    "column": "24",
                    "severity": "warning",
                    "message": "'if' child has incorrect indentation level 23, expected level should be 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "366",
                    "column": "20",
                    "severity": "warning",
                    "message": "'if rcurly' has incorrect indentation level 19, expected level should be 12.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "429",
                    "column": "1",
                    "severity": "warning",
                    "message": "'basketCacheFactory' has incorrect indentation level 0, expected level should be 28.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "430",
                    "column": "1",
                    "severity": "warning",
                    "message": "'partitionStart' has incorrect indentation level 0, expected level should be 28.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "431",
                    "column": "1",
                    "severity": "warning",
                    "message": "'partitionEnd' has incorrect indentation level 0, expected level should be 28.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "432",
                    "column": "1",
                    "severity": "warning",
                    "message": "'trimmedSlimBranches' has incorrect indentation level 0, expected level should be 28.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "433",
                    "column": "1",
                    "severity": "warning",
                    "message": "'threadCount' has incorrect indentation level 0, expected level should be 28.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "434",
                    "column": "1",
                    "severity": "warning",
                    "message": "'profileData' has incorrect indentation level 0, expected level should be 28.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "435",
                    "column": "1",
                    "severity": "warning",
                    "message": "'pid' has incorrect indentation level 0, expected level should be 28.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "451",
                    "column": "25",
                    "severity": "warning",
                    "message": "WhitespaceAround: '->' is not preceded with whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.WhitespaceAroundCheck"
                },
                {
                    "line": "509",
                    "column": "1",
                    "severity": "warning",
                    "message": "'method def modifier' has incorrect indentation level 0, expected level should be one of the following: 8, 12.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "511",
                    "column": "4",
                    "severity": "warning",
                    "message": "'method def' child has incorrect indentation level 3, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "512",
                    "column": "4",
                    "severity": "warning",
                    "message": "'method def' child has incorrect indentation level 3, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "513",
                    "column": "4",
                    "severity": "warning",
                    "message": "'method def' child has incorrect indentation level 3, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "514",
                    "column": "4",
                    "severity": "warning",
                    "message": "'method def' child has incorrect indentation level 3, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "515",
                    "column": "4",
                    "severity": "warning",
                    "message": "'method def' child has incorrect indentation level 3, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "516",
                    "column": "4",
                    "severity": "warning",
                    "message": "'if' has incorrect indentation level 3, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "517",
                    "column": "8",
                    "severity": "warning",
                    "message": "'synchronized' has incorrect indentation level 7, expected level should be one of the following: 16, 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "517",
                    "column": "8",
                    "severity": "warning",
                    "message": "WhitespaceAround: 'synchronized' is not followed by whitespace. Empty blocks may only be represented as {} when not part of a multi-block statement (4.1.3)",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.WhitespaceAroundCheck"
                },
                {
                    "line": "517",
                    "column": "32",
                    "severity": "warning",
                    "message": "')' is preceded with whitespace.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.ParenPadCheck"
                },
                {
                    "line": "518",
                    "column": "34",
                    "severity": "warning",
                    "message": "'synchronized' child has incorrect indentation level 33, expected level should be one of the following: 20, 24.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "519",
                    "column": "34",
                    "severity": "warning",
                    "message": "'synchronized' child has incorrect indentation level 33, expected level should be one of the following: 20, 24.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "520",
                    "column": "8",
                    "severity": "warning",
                    "message": "'synchronized rcurly' has incorrect indentation level 7, expected level should be one of the following: 16, 20.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "521",
                    "column": "4",
                    "severity": "warning",
                    "message": "'if rcurly' has incorrect indentation level 3, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "522",
                    "column": "4",
                    "severity": "warning",
                    "message": "'method def' child has incorrect indentation level 3, expected level should be one of the following: 12, 16.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "523",
                    "column": "1",
                    "severity": "warning",
                    "message": "'method def rcurly' has incorrect indentation level 0, expected level should be one of the following: 8, 12.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "524",
                    "column": "152",
                    "severity": "warning",
                    "message": "'object def rcurly' has incorrect indentation level 151, expected level should be one of the following: 4, 8.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.indentation.IndentationCheck"
                },
                {
                    "line": "525",
                    "column": "5",
                    "severity": "warning",
                    "message": "'CLASS_DEF' should be separated from previous line.",
                    "source": "com.puppycrawl.tools.checkstyle.checks.whitespace.EmptyLineSeparatorCheck"
                }
            ],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/errored/1/12/Root.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/codebuff/12/Root.java\nindex 2d4975cc39e..f9025d4b24a 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/errored/1/12/Root.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/codebuff/12/Root.java\n@@ -16,7 +16,6 @@ import java.util.concurrent.ThreadFactory;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n import java.util.function.Function;\n-\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.apache.spark.SparkContext;\n@@ -41,12 +40,10 @@ import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.vectorized.ColumnVector;\n import org.apache.spark.sql.vectorized.ColumnarBatch;\n import org.apache.spark.util.CollectionAccumulator;\n-\n import com.google.common.cache.CacheBuilder;\n import com.google.common.cache.CacheLoader;\n import com.google.common.cache.LoadingCache;\n import com.google.common.util.concurrent.ThreadFactoryBuilder;\n-\n import edu.vanderbilt.accre.laurelin.interpretation.AsDtype.Dtype;\n import edu.vanderbilt.accre.laurelin.root_proxy.IOFactory;\n import edu.vanderbilt.accre.laurelin.root_proxy.IOProfile;\n@@ -134,22 +131,20 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n          */\n         static {\n             ThreadFactory factory = new ThreadFactoryBuilder().setNameFormat(\"laurelin-arraybuilder-%d\").build();\n-            staticExecutor = new ThreadPoolExecutor(1, 1,\n-                                                    5L, TimeUnit.SECONDS,\n-                                                    new LinkedBlockingQueue<Runnable>(),\n-                                                    factory);\n+            staticExecutor = new ThreadPoolExecutor(1, 1, 5L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(), factory);\n             staticExecutor.allowCoreThreadTimeOut(true);\n             Runtime.getRuntime().addShutdownHook(new Thread() {\n-                @Override\n-                public void run() {\n-                    TTreeDataSourceV2PartitionReader.staticExecutor.shutdownNow();\n-                }\n-            });\n+                                                     @Override\n+                                                     public void run() {\n+                                                         TTreeDataSourceV2PartitionReader.staticExecutor.shutdownNow();\n+                                                     }\n+});\n         }\n \n         /**\n          * Holds the async threadpool if enabled, null otherwise\n          */\n+\n         private static ThreadPoolExecutor executor;\n         private CollectionAccumulator<Storage> profileData;\n         private int pid;\n@@ -163,16 +158,14 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n             this.slimBranches = slimBranches;\n             this.profileData = profileData;\n             this.pid = pid;\n-\n             Function<Event, Integer> cb = null;\n             if (this.profileData != null) {\n-                cb = e -> {\n-                    this.profileData.add(e.getStorage());\n-                    return 0;\n-                };\n+                cb = e-> {\n+                         this.profileData.add(e.getStorage());\n+                         return 0;\n+                     };\n             }\n             IOProfile.getInstance(pid, cb);\n-\n             if (threadCount >= 1) {\n                 executor = staticExecutor;\n                 executor.setCorePoolSize(threadCount);\n@@ -216,13 +209,13 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n             }\n             // End misery\n             ColumnarBatch ret = new ColumnarBatch(tmp);\n-            ret.setNumRows((int) (entryEnd - entryStart));\n+            ret.setNumRows((int)(entryEnd - entryStart));\n             return ret;\n         }\n \n         private LinkedList<ColumnVector> getBatchRecursive(StructField[] structFields) {\n             LinkedList<ColumnVector> vecs = new LinkedList<ColumnVector>();\n-            for (StructField field: structFields)  {\n+            for (StructField field: structFields) {\n                 if (field.dataType() instanceof StructType) {\n                     LinkedList<ColumnVector> nestedVecs = getBatchRecursive(((StructType)field.dataType()).fields());\n                     vecs.add(new StructColumnVector(field.dataType(), nestedVecs));\n@@ -231,7 +224,6 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                 SlimTBranchInterface slimBranch = slimBranches.get(field.name());\n                 SimpleType rootType;\n                 rootType = SimpleType.fromString(field.metadata().getString(\"rootType\"));\n-\n                 Dtype dtype = SimpleType.dtypeFromString(field.metadata().getString(\"rootType\"));\n                 vecs.add(new TTreeColumnVector(field.dataType(), rootType, dtype, basketCache, entryStart, entryEnd, slimBranch, executor, fileCache));\n             }\n@@ -240,8 +232,8 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n     }\n \n     public static class TTreeDataSourceV2Reader implements DataSourceReader,\n-            SupportsScanColumnarBatch,\n-            SupportsPushDownRequiredColumns {\n+SupportsScanColumnarBatch,\n+SupportsPushDownRequiredColumns {\n         private LinkedList<String> paths;\n         private String treeName;\n         private TTree currTree;\n@@ -272,14 +264,13 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                 throw new RuntimeException(e);\n             }\n             threadCount = options.getInt(\"threadCount\", 16);\n-\n             Function<Event, Integer> cb = null;\n             if (ioAccum != null) {\n                 profileData = ioAccum;\n-                cb = e -> {\n-                    profileData.add(e.getStorage());\n-                    return 0;\n-                };\n+                cb = e-> {\n+                         profileData.add(e.getStorage());\n+                         return 0;\n+                     };\n             }\n             profiler = IOProfile.getInstance(0, cb);\n         }\n@@ -312,9 +303,11 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                         int len = prefix.length();\n                         currName = name.substring(len);\n                     }\n+\n                     if (currName.endsWith(\".\")) {\n                         currName = currName.substring(0, currName.length() - 1);\n                     }\n+\n                     if (currName.startsWith(\".\")) {\n                         currName = currName.substring(1);\n                     }\n@@ -338,7 +331,7 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                         metadata.putString(\"rootType\", branch.getSimpleType().getBaseType().toString());\n                         fields.add(new StructField(currName, sparkType, false, metadata.build()));\n                     } else {\n-                        throw new RuntimeException(\"Unsupported schema for branch \" + branch.getName() + \" branchCount: \" + branchCount + \" leafCount: \" + leafCount);\n+                               throw new RuntimeException(\"Unsupported schema for branch \" + branch.getName() + \" branchCount: \" + branchCount + \" leafCount: \" + leafCount);\n                     }\n                 } catch (UnsupportedBranchTypeException e) {\n                     logger.error(String.format(\"The branch \\\"%s\\\" is unable to be deserialized and will be skipped\", name));\n@@ -355,22 +348,23 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                 } else if (simpleType == SimpleType.Int8) {\n                     ret = DataTypes.ByteType;\n                 } else if ((simpleType == SimpleType.Int16) || (simpleType == SimpleType.UInt8)) {\n-                    ret = DataTypes.ShortType;\n-                } else if ((simpleType == SimpleType.Int32) || (simpleType == SimpleType.UInt16)) {\n-                    ret = DataTypes.IntegerType;\n-                } else if ((simpleType == SimpleType.UInt64) || (simpleType == SimpleType.Int64) || (simpleType == SimpleType.UInt32)) {\n-                    ret = DataTypes.LongType;\n-                } else if (simpleType == SimpleType.Float32) {\n-                    ret = DataTypes.FloatType;\n-                } else if (simpleType == SimpleType.Float64) {\n-                    ret = DataTypes.DoubleType;\n-                } else if (simpleType == SimpleType.Pointer) {\n-                    ret = DataTypes.LongType;\n-                }\n+                           ret = DataTypes.ShortType;\n+                       } else if ((simpleType == SimpleType.Int32) || (simpleType == SimpleType.UInt16)) {\n+                           ret = DataTypes.IntegerType;\n+                       } else if ((simpleType == SimpleType.UInt64) || (simpleType == SimpleType.Int64) || (simpleType == SimpleType.UInt32)) {\n+                                  ret = DataTypes.LongType;\n+                              } else if (simpleType == SimpleType.Float32) {\n+                                  ret = DataTypes.FloatType;\n+                              } else if (simpleType == SimpleType.Float64) {\n+                                         ret = DataTypes.DoubleType;\n+                                     } else if (simpleType == SimpleType.Pointer) {\n+                                                ret = DataTypes.LongType;\n+                                            }\n             } else if (simpleType instanceof SimpleType.ArrayType) {\n-                SimpleType nested = ((SimpleType.ArrayType) simpleType).getChildType();\n-                ret = DataTypes.createArrayType(rootToSparkType(nested), false);\n-            }\n+                       SimpleType nested = ((SimpleType.ArrayType)simpleType).getChildType();\n+                       ret = DataTypes.createArrayType(rootToSparkType(nested), false);\n+                   }\n+\n             if (ret == null) {\n                 throw new RuntimeException(\"Unable to convert ROOT type '\" + simpleType + \"' to Spark\");\n             }\n@@ -396,9 +390,9 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n             }\n \n             private static void parseStructFields(TTree inputTree, Map<String, SlimTBranch> slimBranches, StructType struct, String namespace) {\n-                for (StructField field: struct.fields())  {\n+                for (StructField field: struct.fields()) {\n                     if (field.dataType() instanceof StructType) {\n-                        parseStructFields(inputTree, slimBranches, (StructType) field.dataType(), namespace + field.name() + \".\");\n+                        parseStructFields(inputTree, slimBranches, (StructType)field.dataType(), namespace + field.name() + \".\");\n                     }\n                     ArrayList<TBranch> branchList = inputTree.getBranches(namespace + field.name());\n                     assert branchList.size() == 1;\n@@ -412,11 +406,9 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                 List<InputPartition<ColumnarBatch>> ret = new ArrayList<InputPartition<ColumnarBatch>>();\n                 int pid = 0;\n                 TTree inputTree;\n-\n                 try {\n                     TFile inputFile = TFile.getFromFile(fileCache.getROOTFile(path));\n                     inputTree = new TTree(inputFile.getProxy(treeName), inputFile);\n-\n                     Map<String, SlimTBranch> slimBranches = new HashMap<String, SlimTBranch>();\n                     parseStructFields(inputTree, slimBranches, schema, \"\");\n \n@@ -425,7 +417,7 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                     //      smarter later\n                     long[] entryOffset = inputTree.getBranches().get(0).getBasketEntryOffsets();\n                     long lastEntry = entryOffset[entryOffset.length - 1];\n-                    for (int i = 0; i < lastEntry; i += PARTITION_SIZE) {\n+                    for (int i = 0;  i < lastEntry;  i += PARTITION_SIZE) {\n                         pid += 1;\n                         long partitionStart = i;\n                         long partitionEnd = Math.min(lastEntry, partitionStart + PARTITION_SIZE);\n@@ -433,8 +425,16 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                         for (Entry<String, SlimTBranch> e: slimBranches.entrySet()) {\n                             trimmedSlimBranches.put(e.getKey(), e.getValue().copyAndTrim(partitionStart, partitionEnd));\n                         }\n-                        ret.add(new TTreeDataSourceV2Partition(schema, basketCacheFactory, partitionStart, partitionEnd, trimmedSlimBranches, threadCount, profileData, pid));\n+                        ret.add(new TTreeDataSourceV2Partition(schema,\n+basketCacheFactory,\n+partitionStart,\n+partitionEnd,\n+trimmedSlimBranches,\n+threadCount,\n+profileData,\n+pid));\n                     }\n+\n                     if (ret.size() == 0) {\n                         // Only one basket?\n                         logger.debug(\"Planned for zero baskets, adding a dummy one\");\n@@ -445,11 +445,10 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                 } catch (Exception e) {\n                     throw new RuntimeException(e);\n                 }\n-\n             }\n \n             FlatMapFunction<String, InputPartition<ColumnarBatch>> getLambda() {\n-                return s -> PartitionHelper.partitionSingleFileImpl(s, treeName, schema, threadCount, basketCacheFactory);\n+                return s-> PartitionHelper.partitionSingleFileImpl(s, treeName, schema, threadCount, basketCacheFactory);\n             }\n         }\n \n@@ -459,7 +458,8 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n             List<InputPartition<ColumnarBatch>> ret = new ArrayList<InputPartition<ColumnarBatch>>();\n             if (sparkContext == null) {\n                 for (String path: paths) {\n-                    partitionSingleFile(path).forEachRemaining(ret::add);;\n+                    partitionSingleFile(path).forEachRemaining(ret::add);\n+                    ;\n                 }\n             } else {\n                 JavaSparkContext sc = JavaSparkContext.fromSparkContext(sparkContext);\n@@ -468,6 +468,7 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                 JavaRDD<InputPartition<ColumnarBatch>> partitions = rdd_paths.flatMap(helper.getLambda());\n                 ret = partitions.collect();\n             }\n+\n             int pid = 0;\n             for (InputPartition<ColumnarBatch> x: ret) {\n                 ((TTreeDataSourceV2Partition)x).setPid(pid);\n@@ -485,12 +486,12 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n             logger.trace(\"prunecolumns \");\n             schema = requiredSchema;\n         }\n-\n     }\n \n     /**\n      * Accumulator for IOProfiling information\n      */\n+\n     private static CollectionAccumulator<Event.Storage> ioAccum = null;\n \n     /**\n@@ -504,31 +505,23 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n      * are transient, we need to make these soft references so they're not\n      * eagerly garbage collected\n      */\n-    private static LoadingCache<DataSourceReaderKey,\n-                                DataSourceReader> dedupDataSource =\n-                                    CacheBuilder.newBuilder()\n-                                    .softValues()\n-                                    .maximumSize(100)\n-                                    .build(\n-                                       new CacheLoader<DataSourceReaderKey,\n-                                                       DataSourceReader>() {\n-                                            @Override\n-                                            public DataSourceReader load(DataSourceReaderKey key) {\n-                                                logger.trace(\"Construct new reader\");\n-                                                DataSourceOptions options = new DataSourceOptions(key.options);\n-                                                boolean traceIO = key.traceIO;\n-                                                SparkContext context = key.context;\n-                                                CacheFactory basketCacheFactory = new CacheFactory();\n-                                                if ((traceIO) && (context != null)) {\n-                                                    synchronized (Root.class) {\n-                                                        ioAccum = new CollectionAccumulator<Event.Storage>();\n-                                                        context.register(ioAccum, \"edu.vanderbilt.accre.laurelin.ioprofile\");\n-                                                    }\n-                                                }\n-                                                return new TTreeDataSourceV2Reader(options, basketCacheFactory, context, ioAccum);\n-                                            }\n-                                            });\n-\n+    private static LoadingCache<DataSourceReaderKey, DataSourceReader> dedupDataSource = CacheBuilder.newBuilder().softValues().maximumSize(100).build(new CacheLoader<DataSourceReaderKey, DataSourceReader>() {\n+@Override\n+public DataSourceReader load(DataSourceReaderKey key) {\n+   logger.trace(\"Construct new reader\");\n+   DataSourceOptions options = new DataSourceOptions(key.options);\n+   boolean traceIO = key.traceIO;\n+   SparkContext context = key.context;\n+   CacheFactory basketCacheFactory = new CacheFactory();\n+   if ((traceIO) && (context != null)) {\n+       synchronized(Root.class ) {\n+                                 ioAccum = new CollectionAccumulator<Event.Storage>();\n+                                 context.register(ioAccum, \"edu.vanderbilt.accre.laurelin.ioprofile\");\n+       }\n+   }\n+   return new TTreeDataSourceV2Reader(options, basketCacheFactory, context, ioAccum);\n+}\n+                                                                                                                                                       });\n     static class DataSourceReaderKey {\n         @Override\n         public int hashCode() {\n@@ -537,21 +530,20 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n \n         @Override\n         public boolean equals(Object obj) {\n-            DataSourceReaderKey other = (DataSourceReaderKey) obj;\n-            return (context == other.context) && options.equals(other.options)\n-                    && traceIO == other.traceIO;\n+            DataSourceReaderKey other = (DataSourceReaderKey)obj;\n+            return (context == other.context) && options.equals(other.options) && traceIO == other.traceIO;\n         }\n \n         public DataSourceReaderKey(DataSourceOptions options, SparkContext context, boolean traceIO) {\n             this.traceIO = traceIO;\n             this.context = context;\n             this.options = options.asMap();\n-\n         }\n \n         boolean traceIO;\n         SparkContext context;\n         Map<String, String> options;\n+\n     }\n \n     /**\n@@ -559,6 +551,7 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n      * SparkContext\n      * @param options DS options\n      */\n+\n     @Override\n     public DataSourceReader createReader(DataSourceOptions options) {\n         return createReader(options, SparkContext.getOrCreate(), false);\n@@ -571,6 +564,7 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n      * @param traceIO whether or not to trace the IO operations\n      * @return new reader\n      */\n+\n     public DataSourceReader createReader(DataSourceOptions options, SparkContext context, boolean traceIO) {\n         try {\n             return dedupDataSource.get(new DataSourceReaderKey(options, context, traceIO));\n@@ -583,5 +577,4 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n     public String shortName() {\n         return \"root\";\n     }\n-\n-}\n+}\n\\ No newline at end of file\n",
            "diff_size": 103
        },
        {
            "tool": "styler_random",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/errored/1/12/Root.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/styler_random/12/Root.java\nindex 2d4975cc39e..b87013ff2f4 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/errored/1/12/Root.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/styler_random/12/Root.java\n@@ -510,7 +510,7 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                                     .softValues()\n                                     .maximumSize(100)\n                                     .build(\n-                                       new CacheLoader<DataSourceReaderKey,\n+                                        new CacheLoader<DataSourceReaderKey,\n                                                        DataSourceReader>() {\n                                             @Override\n                                             public DataSourceReader load(DataSourceReaderKey key) {\n",
            "diff_size": 1
        },
        {
            "tool": "styler_three_grams",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/errored/1/12/Root.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/styler_three_grams/12/Root.java\nindex 2d4975cc39e..b87013ff2f4 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/errored/1/12/Root.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/spark-root-laurelin/styler_three_grams/12/Root.java\n@@ -510,7 +510,7 @@ public class Root implements DataSourceV2, ReadSupport, DataSourceRegister {\n                                     .softValues()\n                                     .maximumSize(100)\n                                     .build(\n-                                       new CacheLoader<DataSourceReaderKey,\n+                                        new CacheLoader<DataSourceReaderKey,\n                                                        DataSourceReader>() {\n                                             @Override\n                                             public DataSourceReader load(DataSourceReaderKey key) {\n",
            "diff_size": 1
        }
    ],
    "repaired_by": [
        "styler",
        "styler_random",
        "styler_three_grams"
    ],
    "not_repaired_by": [
        "intellij",
        "naturalize",
        "codebuff"
    ]
}