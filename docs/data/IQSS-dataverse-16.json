{
    "project_name": "IQSS-dataverse",
    "error_id": "16",
    "information": {
        "errors": [
            {
                "line": "362",
                "column": "24",
                "severity": "error",
                "message": "'{' at column 24 should have line break after.",
                "source": "com.puppycrawl.tools.checkstyle.checks.blocks.LeftCurlyCheck"
            }
        ]
    },
    "source_code": "                // temporary - let's not delete the temp metadata file if anything went wrong, for now:\n                if (errMessage == null) {\n                    try{tempFile.delete();}catch(Throwable t){};\n                }\n            }\n        }",
    "results": [
        {
            "tool": "styler",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/16/HarvesterServiceBean.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler/16/HarvesterServiceBean.java\nindex 71cc23e242b..5818d00717b 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/16/HarvesterServiceBean.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler/16/HarvesterServiceBean.java\n@@ -359,7 +359,8 @@ public class HarvesterServiceBean {\n             if (tempFile != null) {\n                 // temporary - let's not delete the temp metadata file if anything went wrong, for now:\n                 if (errMessage == null) {\n-                    try{tempFile.delete();}catch(Throwable t){};\n+                    try{\n+                      tempFile.delete();}catch(Throwable t){};\n                 }\n             }\n         }\n",
            "diff_size": 2
        },
        {
            "tool": "intellij",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/16/HarvesterServiceBean.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/intellij/16/HarvesterServiceBean.java\nindex 71cc23e242b..30c92a009c2 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/16/HarvesterServiceBean.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/intellij/16/HarvesterServiceBean.java\n@@ -3,6 +3,7 @@\n  * To change this template file, choose Tools | Templates\n  * and open the template in the editor.\n  */\n+\n package edu.harvard.iq.dataverse.harvest.client;\n \n import edu.harvard.iq.dataverse.Dataset;\n@@ -48,142 +49,154 @@ import javax.persistence.EntityManager;\n import javax.persistence.PersistenceContext;\n \n /**\n- *\n  * @author Leonid Andreev\n  */\n @Stateless(name = \"harvesterService\")\n @Named\n public class HarvesterServiceBean {\n-    @PersistenceContext(unitName=\"VDCNet-ejbPU\")\n-    private EntityManager em;\n-    \n-    @EJB\n-    DataverseServiceBean dataverseService;\n-    @EJB\n-    DatasetServiceBean datasetService;\n-    @Resource\n-    javax.ejb.TimerService timerService;\n-    @EJB\n-    DataverseTimerServiceBean dataverseTimerService;\n-    @EJB\n-    HarvestingClientServiceBean harvestingClientService;\n-    @EJB\n-    ImportServiceBean importService;\n-    @EJB\n-    EjbDataverseEngine engineService;\n-    @EJB\n-    IndexServiceBean indexService;\n-    \n-    private static final Logger logger = Logger.getLogger(\"edu.harvard.iq.dataverse.harvest.client.HarvesterServiceBean\");\n-    private static final SimpleDateFormat formatter = new SimpleDateFormat(\"yyyy-MM-dd\");\n-    private static final SimpleDateFormat logFormatter = new SimpleDateFormat(\"yyyy-MM-dd'T'HH-mm-ss\");\n-    \n-    public static final String HARVEST_RESULT_SUCCESS=\"success\";\n-    public static final String HARVEST_RESULT_FAILED=\"failed\";\n-    private static final Long  INDEXING_CONTENT_BATCH_SIZE = 10000000L;\n-\n-\n-    public HarvesterServiceBean() {\n-\n+  @PersistenceContext(unitName = \"VDCNet-ejbPU\")\n+  private EntityManager em;\n+\n+  @EJB\n+  DataverseServiceBean dataverseService;\n+  @EJB\n+  DatasetServiceBean datasetService;\n+  @Resource\n+  javax.ejb.TimerService timerService;\n+  @EJB\n+  DataverseTimerServiceBean dataverseTimerService;\n+  @EJB\n+  HarvestingClientServiceBean harvestingClientService;\n+  @EJB\n+  ImportServiceBean importService;\n+  @EJB\n+  EjbDataverseEngine engineService;\n+  @EJB\n+  IndexServiceBean indexService;\n+\n+  private static final Logger logger = Logger.getLogger(\"edu.harvard.iq.dataverse.harvest.client.HarvesterServiceBean\");\n+  private static final SimpleDateFormat formatter = new SimpleDateFormat(\"yyyy-MM-dd\");\n+  private static final SimpleDateFormat logFormatter = new SimpleDateFormat(\"yyyy-MM-dd'T'HH-mm-ss\");\n+\n+  public static final String HARVEST_RESULT_SUCCESS = \"success\";\n+  public static final String HARVEST_RESULT_FAILED = \"failed\";\n+  private static final Long INDEXING_CONTENT_BATCH_SIZE = 10000000L;\n+\n+\n+  public HarvesterServiceBean() {\n+\n+  }\n+\n+  /**\n+   * Called to run an \"On Demand\" harvest.\n+   */\n+  @Asynchronous\n+  public void doAsyncHarvest(DataverseRequest dataverseRequest, HarvestingClient harvestingClient) {\n+\n+    try {\n+      doHarvest(dataverseRequest, harvestingClient.getId());\n+    } catch (Exception e) {\n+      logger\n+        .info(\"Caught exception running an asynchronous harvest (dataverse \\\"\" + harvestingClient.getName() + \"\\\")\");\n     }\n-    \n-    /**\n-     * Called to run an \"On Demand\" harvest.  \n-     */\n-    @Asynchronous\n-    public void doAsyncHarvest(DataverseRequest dataverseRequest, HarvestingClient harvestingClient) {\n-        \n-        try {\n-            doHarvest(dataverseRequest, harvestingClient.getId());\n-        } catch (Exception e) {\n-            logger.info(\"Caught exception running an asynchronous harvest (dataverse \\\"\"+harvestingClient.getName()+\"\\\")\");\n-        }\n+  }\n+\n+  public void createScheduledHarvestTimers() {\n+    logger.log(Level.INFO, \"HarvesterService: going to (re)create Scheduled harvest timers.\");\n+    dataverseTimerService.removeHarvestTimers();\n+\n+    List configuredClients = harvestingClientService.getAllHarvestingClients();\n+    for (Iterator it = configuredClients.iterator(); it.hasNext(); ) {\n+      HarvestingClient harvestingConfig = (HarvestingClient) it.next();\n+      if (harvestingConfig.isScheduled()) {\n+        dataverseTimerService.createHarvestTimer(harvestingConfig);\n+      }\n     }\n+  }\n \n-    public void createScheduledHarvestTimers() {\n-        logger.log(Level.INFO, \"HarvesterService: going to (re)create Scheduled harvest timers.\");\n-        dataverseTimerService.removeHarvestTimers();\n+  public List<HarvestTimerInfo> getHarvestTimers() {\n+    ArrayList<HarvestTimerInfo> timers = new ArrayList<>();\n \n-        List configuredClients = harvestingClientService.getAllHarvestingClients();\n-        for (Iterator it = configuredClients.iterator(); it.hasNext();) {\n-            HarvestingClient harvestingConfig = (HarvestingClient) it.next();\n-            if (harvestingConfig.isScheduled()) {\n-                dataverseTimerService.createHarvestTimer(harvestingConfig);\n-            }\n-        }\n+    for (Iterator it = timerService.getTimers().iterator(); it.hasNext(); ) {\n+      Timer timer = (Timer) it.next();\n+      if (timer.getInfo() instanceof HarvestTimerInfo) {\n+        HarvestTimerInfo info = (HarvestTimerInfo) timer.getInfo();\n+        timers.add(info);\n+      }\n     }\n-  \n-    public List<HarvestTimerInfo> getHarvestTimers() {\n-        ArrayList <HarvestTimerInfo>timers = new ArrayList<>();\n-        \n-        for (Iterator it = timerService.getTimers().iterator(); it.hasNext();) {\n-            Timer timer = (Timer) it.next();\n-            if (timer.getInfo() instanceof HarvestTimerInfo) {\n-                HarvestTimerInfo info = (HarvestTimerInfo) timer.getInfo();\n-                timers.add(info);\n-            }\n-        }    \n-        return timers;\n+    return timers;\n+  }\n+\n+  /**\n+   * Run a harvest for an individual harvesting Dataverse\n+   *\n+   * @param dataverseRequest\n+   * @param harvestingClientId\n+   * @throws IOException\n+   */\n+  public void doHarvest(DataverseRequest dataverseRequest, Long harvestingClientId) throws IOException {\n+    HarvestingClient harvestingClientConfig = harvestingClientService.find(harvestingClientId);\n+\n+    if (harvestingClientConfig == null) {\n+      throw new IOException(\"No such harvesting client: id=\" + harvestingClientId);\n     }\n \n-    /**\n-     * Run a harvest for an individual harvesting Dataverse\n-     * @param dataverseRequest\n-     * @param harvestingClientId\n-     * @throws IOException\n-     */\n-    public void doHarvest(DataverseRequest dataverseRequest, Long harvestingClientId) throws IOException {\n-        HarvestingClient harvestingClientConfig = harvestingClientService.find(harvestingClientId);\n-        \n-        if (harvestingClientConfig == null) {\n-            throw new IOException(\"No such harvesting client: id=\"+harvestingClientId);\n+    Dataverse harvestingDataverse = harvestingClientConfig.getDataverse();\n+\n+    MutableBoolean harvestErrorOccurred = new MutableBoolean(false);\n+    String logTimestamp = logFormatter.format(new Date());\n+    Logger hdLogger = Logger.getLogger(\n+      \"edu.harvard.iq.dataverse.harvest.client.HarvesterServiceBean.\" + harvestingDataverse.getAlias() + logTimestamp);\n+    String logFileName =\n+      \"../logs\" + File.separator + \"harvest_\" + harvestingClientConfig.getName() + \"_\" + logTimestamp + \".log\";\n+    FileHandler fileHandler = new FileHandler(logFileName);\n+    hdLogger.setUseParentHandlers(false);\n+    hdLogger.addHandler(fileHandler);\n+\n+    PrintWriter importCleanupLog = new PrintWriter(\n+      new FileWriter(\"../logs/harvest_cleanup_\" + harvestingClientConfig.getName() + \"_\" + logTimestamp + \".txt\"));\n+\n+\n+    List<Long> harvestedDatasetIds = null;\n+\n+    List<Long> harvestedDatasetIdsThisBatch = new ArrayList<Long>();\n+\n+    List<String> failedIdentifiers = new ArrayList<String>();\n+    List<String> deletedIdentifiers = new ArrayList<String>();\n+\n+    Date harvestStartTime = new Date();\n+\n+    try {\n+      boolean harvestingNow = harvestingClientConfig.isHarvestingNow();\n+\n+      if (harvestingNow) {\n+        harvestErrorOccurred.setValue(true);\n+        hdLogger.log(Level.SEVERE,\n+          \"Cannot begin harvesting, Dataverse \" + harvestingDataverse.getName() + \" is currently being harvested.\");\n+\n+      } else {\n+        harvestingClientService.resetHarvestInProgress(harvestingClientId);\n+        harvestingClientService.setHarvestInProgress(harvestingClientId, harvestStartTime);\n+\n+\n+        if (harvestingClientConfig.isOai()) {\n+          harvestedDatasetIds =\n+            harvestOAI(dataverseRequest, harvestingClientConfig, hdLogger, importCleanupLog, harvestErrorOccurred,\n+              failedIdentifiers, deletedIdentifiers, harvestedDatasetIdsThisBatch);\n+\n+        } else {\n+          throw new IOException(\"Unsupported harvest type\");\n         }\n-        \n-        Dataverse harvestingDataverse = harvestingClientConfig.getDataverse();\n-        \n-        MutableBoolean harvestErrorOccurred = new MutableBoolean(false);\n-        String logTimestamp = logFormatter.format(new Date());\n-        Logger hdLogger = Logger.getLogger(\"edu.harvard.iq.dataverse.harvest.client.HarvesterServiceBean.\" + harvestingDataverse.getAlias() + logTimestamp);\n-        String logFileName = \"../logs\" + File.separator + \"harvest_\" + harvestingClientConfig.getName() + \"_\" + logTimestamp + \".log\";\n-        FileHandler fileHandler = new FileHandler(logFileName);\n-        hdLogger.setUseParentHandlers(false);\n-        hdLogger.addHandler(fileHandler);\n-        \n-        PrintWriter importCleanupLog = new PrintWriter(new FileWriter( \"../logs/harvest_cleanup_\" + harvestingClientConfig.getName() + \"_\" + logTimestamp+\".txt\"));\n-        \n-        \n-        List<Long> harvestedDatasetIds = null;\n-\n-        List<Long> harvestedDatasetIdsThisBatch = new ArrayList<Long>();\n-\n-        List<String> failedIdentifiers = new ArrayList<String>();\n-        List<String> deletedIdentifiers = new ArrayList<String>();\n-        \n-        Date harvestStartTime = new Date();\n-        \n-        try {\n-            boolean harvestingNow = harvestingClientConfig.isHarvestingNow();\n-\n-            if (harvestingNow) {\n-                harvestErrorOccurred.setValue(true);\n-                hdLogger.log(Level.SEVERE, \"Cannot begin harvesting, Dataverse \" + harvestingDataverse.getName() + \" is currently being harvested.\");\n-\n-            } else {\n-                harvestingClientService.resetHarvestInProgress(harvestingClientId);\n-                harvestingClientService.setHarvestInProgress(harvestingClientId, harvestStartTime);\n-\n-               \n-                if (harvestingClientConfig.isOai()) {\n-                    harvestedDatasetIds = harvestOAI(dataverseRequest, harvestingClientConfig, hdLogger, importCleanupLog, harvestErrorOccurred, failedIdentifiers, deletedIdentifiers, harvestedDatasetIdsThisBatch);\n-\n-                } else {\n-                    throw new IOException(\"Unsupported harvest type\");\n-                }\n-               harvestingClientService.setHarvestSuccess(harvestingClientId, new Date(), harvestedDatasetIds.size(), failedIdentifiers.size(), deletedIdentifiers.size());\n-               hdLogger.log(Level.INFO, \"COMPLETED HARVEST, server=\" + harvestingClientConfig.getArchiveUrl() + \", metadataPrefix=\" + harvestingClientConfig.getMetadataPrefix());\n-               hdLogger.log(Level.INFO, \"Datasets created/updated: \" + harvestedDatasetIds.size() + \", datasets deleted: \" + deletedIdentifiers.size() + \", datasets failed: \" + failedIdentifiers.size());\n-\n-                // now index all the datasets we have harvested - created, modified or deleted:\n+        harvestingClientService\n+          .setHarvestSuccess(harvestingClientId, new Date(), harvestedDatasetIds.size(), failedIdentifiers.size(),\n+            deletedIdentifiers.size());\n+        hdLogger.log(Level.INFO,\n+          \"COMPLETED HARVEST, server=\" + harvestingClientConfig.getArchiveUrl() + \", metadataPrefix=\" +\n+            harvestingClientConfig.getMetadataPrefix());\n+        hdLogger.log(Level.INFO, \"Datasets created/updated: \" + harvestedDatasetIds.size() + \", datasets deleted: \" +\n+          deletedIdentifiers.size() + \", datasets failed: \" + failedIdentifiers.size());\n+\n+        // now index all the datasets we have harvested - created, modified or deleted:\n                 /* (TODO: may not be needed at all. In Dataverse4, we may be able to get away with the normal \n                     reindexing after every import. See the rest of the comments about batch indexing throughout \n                     this service bean)\n@@ -199,255 +212,271 @@ public class HarvesterServiceBean {\n                         hdLogger.log(Level.INFO, \"(All harvested content already reindexed)\");\n                     }\n                  */\n-            }\n-            //mailService.sendHarvestNotification(...getSystemEmail(), harvestingDataverse.getName(), logFileName, logTimestamp, harvestErrorOccurred.booleanValue(), harvestedDatasetIds.size(), failedIdentifiers);\n-        } catch (Throwable e) {\n-            harvestErrorOccurred.setValue(true);\n-            String message = \"Exception processing harvest, server= \" + harvestingClientConfig.getHarvestingUrl() + \",format=\" + harvestingClientConfig.getMetadataPrefix() + \" \" + e.getClass().getName() + \" \" + e.getMessage();\n-            hdLogger.log(Level.SEVERE, message);\n-            logException(e, hdLogger);\n-            hdLogger.log(Level.INFO, \"HARVEST NOT COMPLETED DUE TO UNEXPECTED ERROR.\");\n-            // TODO: \n-            // even though this harvesting run failed, we may have had successfully \n-            // processed some number of datasets, by the time the exception was thrown. \n-            // We should record that number too. And the number of the datasets that\n-            // had failed, that we may have counted.  -- L.A. 4.4\n-            harvestingClientService.setHarvestFailure(harvestingClientId, new Date());\n-\n-        } finally {\n-            harvestingClientService.resetHarvestInProgress(harvestingClientId);\n-            fileHandler.close();\n-            hdLogger.removeHandler(fileHandler);\n-            importCleanupLog.close();\n-        }\n+      }\n+      //mailService.sendHarvestNotification(...getSystemEmail(), harvestingDataverse.getName(), logFileName, logTimestamp, harvestErrorOccurred.booleanValue(), harvestedDatasetIds.size(), failedIdentifiers);\n+    } catch (Throwable e) {\n+      harvestErrorOccurred.setValue(true);\n+      String message =\n+        \"Exception processing harvest, server= \" + harvestingClientConfig.getHarvestingUrl() + \",format=\" +\n+          harvestingClientConfig.getMetadataPrefix() + \" \" + e.getClass().getName() + \" \" + e.getMessage();\n+      hdLogger.log(Level.SEVERE, message);\n+      logException(e, hdLogger);\n+      hdLogger.log(Level.INFO, \"HARVEST NOT COMPLETED DUE TO UNEXPECTED ERROR.\");\n+      // TODO:\n+      // even though this harvesting run failed, we may have had successfully\n+      // processed some number of datasets, by the time the exception was thrown.\n+      // We should record that number too. And the number of the datasets that\n+      // had failed, that we may have counted.  -- L.A. 4.4\n+      harvestingClientService.setHarvestFailure(harvestingClientId, new Date());\n+\n+    } finally {\n+      harvestingClientService.resetHarvestInProgress(harvestingClientId);\n+      fileHandler.close();\n+      hdLogger.removeHandler(fileHandler);\n+      importCleanupLog.close();\n+    }\n+  }\n+\n+  /**\n+   * @param harvestingClient     the harvesting client object\n+   * @param hdLogger             custom logger (specific to this harvesting run)\n+   * @param harvestErrorOccurred have we encountered any errors during harvest?\n+   * @param failedIdentifiers    Study Identifiers for failed \"GetRecord\" requests\n+   */\n+  private List<Long> harvestOAI(DataverseRequest dataverseRequest, HarvestingClient harvestingClient, Logger hdLogger,\n+                                PrintWriter importCleanupLog, MutableBoolean harvestErrorOccurred,\n+                                List<String> failedIdentifiers, List<String> deletedIdentifiers,\n+                                List<Long> harvestedDatasetIdsThisBatch)\n+    throws IOException, ParserConfigurationException, SAXException, TransformerException {\n+\n+    logBeginOaiHarvest(hdLogger, harvestingClient);\n+\n+    List<Long> harvestedDatasetIds = new ArrayList<Long>();\n+    MutableLong processedSizeThisBatch = new MutableLong(0L);\n+    OaiHandler oaiHandler;\n+\n+    try {\n+      oaiHandler = new OaiHandler(harvestingClient);\n+    } catch (OaiHandlerException ohe) {\n+      String errorMessage = \"Failed to create OaiHandler for harvesting client \"\n+        + harvestingClient.getName()\n+        + \"; \"\n+        + ohe.getMessage();\n+      hdLogger.log(Level.SEVERE, errorMessage);\n+      throw new IOException(errorMessage);\n     }\n \n-    /**\n-     * \n-     * @param harvestingClient  the harvesting client object\n-     * @param hdLogger          custom logger (specific to this harvesting run)\n-     * @param harvestErrorOccurred  have we encountered any errors during harvest?\n-     * @param failedIdentifiers     Study Identifiers for failed \"GetRecord\" requests\n-     */\n-    private List<Long> harvestOAI(DataverseRequest dataverseRequest, HarvestingClient harvestingClient, Logger hdLogger, PrintWriter importCleanupLog, MutableBoolean harvestErrorOccurred, List<String> failedIdentifiers, List<String> deletedIdentifiers, List<Long> harvestedDatasetIdsThisBatch)\n-            throws IOException, ParserConfigurationException, SAXException, TransformerException {\n-\n-        logBeginOaiHarvest(hdLogger, harvestingClient);\n-        \n-        List<Long> harvestedDatasetIds = new ArrayList<Long>();\n-        MutableLong processedSizeThisBatch = new MutableLong(0L);\n-        OaiHandler oaiHandler;\n-\n-        try {\n-            oaiHandler = new OaiHandler(harvestingClient);\n-        } catch (OaiHandlerException ohe) {\n-            String errorMessage = \"Failed to create OaiHandler for harvesting client \"\n-                    +harvestingClient.getName()\n-                    +\"; \"\n-                    +ohe.getMessage();\n-            hdLogger.log(Level.SEVERE, errorMessage);\n-            throw new IOException(errorMessage);\n+    try {\n+      for (Iterator<Header> idIter = oaiHandler.runListIdentifiers(); idIter.hasNext(); ) {\n+\n+        Header h = idIter.next();\n+        String identifier = h.getIdentifier();\n+        Date dateStamp = h.getDatestamp();\n+\n+        hdLogger.info(\"processing identifier: \" + identifier + \", date: \" + dateStamp);\n+\n+        MutableBoolean getRecordErrorOccurred = new MutableBoolean(false);\n+\n+        // Retrieve and process this record with a separate GetRecord call:\n+        Long datasetId =\n+          processRecord(dataverseRequest, hdLogger, importCleanupLog, oaiHandler, identifier, getRecordErrorOccurred,\n+            processedSizeThisBatch, deletedIdentifiers, dateStamp);\n+\n+        hdLogger.info(\"Total content processed in this batch so far: \" + processedSizeThisBatch);\n+        if (datasetId != null) {\n+          harvestedDatasetIds.add(datasetId);\n+\n+          if (harvestedDatasetIdsThisBatch == null) {\n+            harvestedDatasetIdsThisBatch = new ArrayList<Long>();\n+          }\n+          harvestedDatasetIdsThisBatch.add(datasetId);\n+\n         }\n-                \n-        try {\n-            for (Iterator<Header> idIter = oaiHandler.runListIdentifiers(); idIter.hasNext();) {\n-\n-                Header h = idIter.next();\n-                String identifier = h.getIdentifier();\n-                Date dateStamp = h.getDatestamp();\n-                \n-                hdLogger.info(\"processing identifier: \" + identifier + \", date: \" + dateStamp);\n-\n-                MutableBoolean getRecordErrorOccurred = new MutableBoolean(false);\n-\n-                // Retrieve and process this record with a separate GetRecord call:\n-                Long datasetId = processRecord(dataverseRequest, hdLogger, importCleanupLog, oaiHandler, identifier, getRecordErrorOccurred, processedSizeThisBatch, deletedIdentifiers, dateStamp);\n-                \n-                hdLogger.info(\"Total content processed in this batch so far: \"+processedSizeThisBatch);\n-                if (datasetId != null) {\n-                    harvestedDatasetIds.add(datasetId);\n-                    \n-                    if ( harvestedDatasetIdsThisBatch == null ) {\n-                        harvestedDatasetIdsThisBatch = new ArrayList<Long>();\n-                    }\n-                    harvestedDatasetIdsThisBatch.add(datasetId);\n-                    \n-                }\n-                \n-                if (getRecordErrorOccurred.booleanValue() == true) {\n-                    failedIdentifiers.add(identifier);\n-                    harvestErrorOccurred.setValue(true);\n-                    //temporary:\n-                    //throw new IOException(\"Exception occured, stopping harvest\");\n-                }\n-                \n-                // reindexing in batches? - this is from DVN 3; \n-                // we may not need it anymore. \n-                if ( processedSizeThisBatch.longValue() > INDEXING_CONTENT_BATCH_SIZE ) {\n-\n-                    hdLogger.log(Level.INFO, \"REACHED CONTENT BATCH SIZE LIMIT; calling index (\"+ harvestedDatasetIdsThisBatch.size()+\" datasets in the batch).\");\n-                    //indexService.updateIndexList(this.harvestedDatasetIdsThisBatch);\n-                    hdLogger.log(Level.INFO, \"REINDEX DONE.\");\n-\n-\n-                    processedSizeThisBatch.setValue(0L);\n-                    harvestedDatasetIdsThisBatch = null;\n-                }\n-\n-            }\n-        } catch (OaiHandlerException e) {\n-            throw new IOException(\"Failed to run ListIdentifiers: \" + e.getMessage());\n+\n+        if (getRecordErrorOccurred.booleanValue() == true) {\n+          failedIdentifiers.add(identifier);\n+          harvestErrorOccurred.setValue(true);\n+          //temporary:\n+          //throw new IOException(\"Exception occured, stopping harvest\");\n         }\n \n-        logCompletedOaiHarvest(hdLogger, harvestingClient);\n+        // reindexing in batches? - this is from DVN 3;\n+        // we may not need it anymore.\n+        if (processedSizeThisBatch.longValue() > INDEXING_CONTENT_BATCH_SIZE) {\n \n-        return harvestedDatasetIds;\n+          hdLogger.log(Level.INFO,\n+            \"REACHED CONTENT BATCH SIZE LIMIT; calling index (\" + harvestedDatasetIdsThisBatch.size() +\n+              \" datasets in the batch).\");\n+          //indexService.updateIndexList(this.harvestedDatasetIdsThisBatch);\n+          hdLogger.log(Level.INFO, \"REINDEX DONE.\");\n \n-    }\n-    \n-    \n-    \n-    private Long processRecord(DataverseRequest dataverseRequest, Logger hdLogger, PrintWriter importCleanupLog, OaiHandler oaiHandler, String identifier, MutableBoolean recordErrorOccurred, MutableLong processedSizeThisBatch, List<String> deletedIdentifiers, Date dateStamp) {\n-        String errMessage = null;\n-        Dataset harvestedDataset = null;\n-        logGetRecord(hdLogger, oaiHandler, identifier);\n-        File tempFile = null;\n-        \n-        try {  \n-            FastGetRecord record = oaiHandler.runGetRecord(identifier);\n-            errMessage = record.getErrorMessage();\n-\n-            if (errMessage != null) {\n-                hdLogger.log(Level.SEVERE, \"Error calling GetRecord - \" + errMessage);\n-            } else if (record.isDeleted()) {\n-                hdLogger.info(\"Deleting harvesting dataset for \"+identifier+\", per the OAI server's instructions.\");\n-                \n-                Dataset dataset = datasetService.getDatasetByHarvestInfo(oaiHandler.getHarvestingClient().getDataverse(), identifier);\n-                if (dataset != null) {\n-                    hdLogger.info(\"Deleting dataset \" + dataset.getGlobalIdString());\n-                    datasetService.deleteHarvestedDataset(dataset, dataverseRequest, hdLogger);\n-                    // TODO: \n-                    // check the status of that Delete - see if it actually succeeded\n-                    deletedIdentifiers.add(identifier);\n-                } else {\n-                    hdLogger.info(\"No dataset found for \"+identifier+\", skipping delete. \");\n-                }\n-\n-            } else {\n-                hdLogger.info(\"Successfully retrieved GetRecord response.\");\n-\n-                tempFile = record.getMetadataFile();\n-                PrintWriter cleanupLog;\n-                harvestedDataset = importService.doImportHarvestedDataset(dataverseRequest, \n-                        oaiHandler.getHarvestingClient(),\n-                        identifier,\n-                        oaiHandler.getMetadataPrefix(), \n-                        record.getMetadataFile(),\n-                        dateStamp,\n-                        importCleanupLog);\n-                \n-                hdLogger.fine(\"Harvest Successful for identifier \" + identifier);\n-                hdLogger.fine(\"Size of this record: \" + record.getMetadataFile().length());\n-                processedSizeThisBatch.add(record.getMetadataFile().length());\n-            }\n-        } catch (Throwable e) {\n-            logGetRecordException(hdLogger, oaiHandler, identifier, e);\n-            errMessage = \"Caught exception while executing GetRecord on \"+identifier;\n-            //logException(e, hdLogger);\n-                \n-        } finally {\n-            if (tempFile != null) {\n-                // temporary - let's not delete the temp metadata file if anything went wrong, for now:\n-                if (errMessage == null) {\n-                    try{tempFile.delete();}catch(Throwable t){};\n-                }\n-            }\n-        }\n \n-        // TODO: the message below is taken from DVN3; - figure out what it means...\n-        // \n-        // If we got an Error from the OAI server or an exception happened during import, then\n-        // set recordErrorOccurred to true (if recordErrorOccurred is being used)\n-        // otherwise throw an exception (if recordErrorOccurred is not used, i.e null)\n-        \n-        if (errMessage != null) {\n-            if (recordErrorOccurred  != null) {\n-                recordErrorOccurred.setValue(true);\n-            } else {\n-                throw new EJBException(errMessage);\n-            }\n+          processedSizeThisBatch.setValue(0L);\n+          harvestedDatasetIdsThisBatch = null;\n         }\n \n-        return harvestedDataset != null ? harvestedDataset.getId() : null;\n-    }\n-       \n-    private void logBeginOaiHarvest(Logger hdLogger, HarvestingClient harvestingClient) {\n-        hdLogger.log(Level.INFO, \"BEGIN HARVEST, oaiUrl=\" \n-                +harvestingClient.getHarvestingUrl() \n-                +\",set=\" \n-                +harvestingClient.getHarvestingSet() \n-                +\", metadataPrefix=\" \n-                +harvestingClient.getMetadataPrefix()\n-                + harvestingClient.getLastNonEmptyHarvestTime() == null ? \"\" : \"from=\" + harvestingClient.getLastNonEmptyHarvestTime());\n-    }\n-    \n-    private void logCompletedOaiHarvest(Logger hdLogger, HarvestingClient harvestingClient) {\n-        hdLogger.log(Level.INFO, \"COMPLETED HARVEST, oaiUrl=\" \n-                +harvestingClient.getHarvestingUrl() \n-                +\",set=\" \n-                +harvestingClient.getHarvestingSet() \n-                +\", metadataPrefix=\" \n-                +harvestingClient.getMetadataPrefix()\n-                + harvestingClient.getLastNonEmptyHarvestTime() == null ? \"\" : \"from=\" + harvestingClient.getLastNonEmptyHarvestTime());\n+      }\n+    } catch (OaiHandlerException e) {\n+      throw new IOException(\"Failed to run ListIdentifiers: \" + e.getMessage());\n     }\n-    \n-    public void logGetRecord(Logger hdLogger, OaiHandler oaiHandler, String identifier) {\n-        hdLogger.log(Level.FINE, \"Calling GetRecord: oaiUrl =\" \n-                +oaiHandler.getBaseOaiUrl()\n-                +\"?verb=GetRecord&identifier=\" \n-                +identifier \n-                +\"&metadataPrefix=\" + oaiHandler.getMetadataPrefix());\n-    }\n-    \n-    public void logGetRecordException(Logger hdLogger, OaiHandler oaiHandler, String identifier, Throwable e) {\n-        String errMessage = \"Exception processing getRecord(), oaiUrl=\" \n-                +oaiHandler.getBaseOaiUrl() \n-                +\", identifier=\"\n-                +identifier \n-                +\", \"\n-                +e.getClass().getName() \n-                //+\" (exception message suppressed)\";\n-                +\", \"\n-                +e.getMessage();\n-        \n-            hdLogger.log(Level.SEVERE, errMessage);\n-            \n-            // temporary:\n-            e.printStackTrace();\n+\n+    logCompletedOaiHarvest(hdLogger, harvestingClient);\n+\n+    return harvestedDatasetIds;\n+\n+  }\n+\n+\n+  private Long processRecord(DataverseRequest dataverseRequest, Logger hdLogger, PrintWriter importCleanupLog,\n+                             OaiHandler oaiHandler, String identifier, MutableBoolean recordErrorOccurred,\n+                             MutableLong processedSizeThisBatch, List<String> deletedIdentifiers, Date dateStamp) {\n+    String errMessage = null;\n+    Dataset harvestedDataset = null;\n+    logGetRecord(hdLogger, oaiHandler, identifier);\n+    File tempFile = null;\n+\n+    try {\n+      FastGetRecord record = oaiHandler.runGetRecord(identifier);\n+      errMessage = record.getErrorMessage();\n+\n+      if (errMessage != null) {\n+        hdLogger.log(Level.SEVERE, \"Error calling GetRecord - \" + errMessage);\n+      } else if (record.isDeleted()) {\n+        hdLogger.info(\"Deleting harvesting dataset for \" + identifier + \", per the OAI server's instructions.\");\n+\n+        Dataset dataset =\n+          datasetService.getDatasetByHarvestInfo(oaiHandler.getHarvestingClient().getDataverse(), identifier);\n+        if (dataset != null) {\n+          hdLogger.info(\"Deleting dataset \" + dataset.getGlobalIdString());\n+          datasetService.deleteHarvestedDataset(dataset, dataverseRequest, hdLogger);\n+          // TODO:\n+          // check the status of that Delete - see if it actually succeeded\n+          deletedIdentifiers.add(identifier);\n+        } else {\n+          hdLogger.info(\"No dataset found for \" + identifier + \", skipping delete. \");\n+        }\n+\n+      } else {\n+        hdLogger.info(\"Successfully retrieved GetRecord response.\");\n+\n+        tempFile = record.getMetadataFile();\n+        PrintWriter cleanupLog;\n+        harvestedDataset = importService.doImportHarvestedDataset(dataverseRequest,\n+          oaiHandler.getHarvestingClient(),\n+          identifier,\n+          oaiHandler.getMetadataPrefix(),\n+          record.getMetadataFile(),\n+          dateStamp,\n+          importCleanupLog);\n+\n+        hdLogger.fine(\"Harvest Successful for identifier \" + identifier);\n+        hdLogger.fine(\"Size of this record: \" + record.getMetadataFile().length());\n+        processedSizeThisBatch.add(record.getMetadataFile().length());\n+      }\n+    } catch (Throwable e) {\n+      logGetRecordException(hdLogger, oaiHandler, identifier, e);\n+      errMessage = \"Caught exception while executing GetRecord on \" + identifier;\n+      //logException(e, hdLogger);\n+\n+    } finally {\n+      if (tempFile != null) {\n+        // temporary - let's not delete the temp metadata file if anything went wrong, for now:\n+        if (errMessage == null) {\n+          try {\n+            tempFile.delete();\n+          } catch (Throwable t) {\n+          }\n+          ;\n+        }\n+      }\n     }\n-    \n-    \n-    // TODO: I doubt we need a full stacktrace in the harvest log - ??\n-    // -- L.A. 4.5 May 2016\n-    private void logException(Throwable e, Logger logger) {\n-\n-        boolean cause = false;\n-        String fullMessage = \"\";\n-        do {\n-            String message = e.getClass().getName() + \" \" + e.getMessage();\n-            if (cause) {\n-                message = \"\\nCaused By Exception.................... \" + e.getClass().getName() + \" \" + e.getMessage();\n-            }\n-            StackTraceElement[] ste = e.getStackTrace();\n-            message += \"\\nStackTrace: \\n\";\n-            for (int m = 0; m < ste.length; m++) {\n-                message += ste[m].toString() + \"\\n\";\n-            }\n-            fullMessage += message;\n-            cause = true;\n-        } while ((e = e.getCause()) != null);\n-        logger.severe(fullMessage);\n+\n+    // TODO: the message below is taken from DVN3; - figure out what it means...\n+    //\n+    // If we got an Error from the OAI server or an exception happened during import, then\n+    // set recordErrorOccurred to true (if recordErrorOccurred is being used)\n+    // otherwise throw an exception (if recordErrorOccurred is not used, i.e null)\n+\n+    if (errMessage != null) {\n+      if (recordErrorOccurred != null) {\n+        recordErrorOccurred.setValue(true);\n+      } else {\n+        throw new EJBException(errMessage);\n+      }\n     }\n+\n+    return harvestedDataset != null ? harvestedDataset.getId() : null;\n+  }\n+\n+  private void logBeginOaiHarvest(Logger hdLogger, HarvestingClient harvestingClient) {\n+    hdLogger.log(Level.INFO, \"BEGIN HARVEST, oaiUrl=\"\n+      + harvestingClient.getHarvestingUrl()\n+      + \",set=\"\n+      + harvestingClient.getHarvestingSet()\n+      + \", metadataPrefix=\"\n+      + harvestingClient.getMetadataPrefix()\n+      + harvestingClient.getLastNonEmptyHarvestTime() == null ? \"\" :\n+      \"from=\" + harvestingClient.getLastNonEmptyHarvestTime());\n+  }\n+\n+  private void logCompletedOaiHarvest(Logger hdLogger, HarvestingClient harvestingClient) {\n+    hdLogger.log(Level.INFO, \"COMPLETED HARVEST, oaiUrl=\"\n+      + harvestingClient.getHarvestingUrl()\n+      + \",set=\"\n+      + harvestingClient.getHarvestingSet()\n+      + \", metadataPrefix=\"\n+      + harvestingClient.getMetadataPrefix()\n+      + harvestingClient.getLastNonEmptyHarvestTime() == null ? \"\" :\n+      \"from=\" + harvestingClient.getLastNonEmptyHarvestTime());\n+  }\n+\n+  public void logGetRecord(Logger hdLogger, OaiHandler oaiHandler, String identifier) {\n+    hdLogger.log(Level.FINE, \"Calling GetRecord: oaiUrl =\"\n+      + oaiHandler.getBaseOaiUrl()\n+      + \"?verb=GetRecord&identifier=\"\n+      + identifier\n+      + \"&metadataPrefix=\" + oaiHandler.getMetadataPrefix());\n+  }\n+\n+  public void logGetRecordException(Logger hdLogger, OaiHandler oaiHandler, String identifier, Throwable e) {\n+    String errMessage = \"Exception processing getRecord(), oaiUrl=\"\n+      + oaiHandler.getBaseOaiUrl()\n+      + \", identifier=\"\n+      + identifier\n+      + \", \"\n+      + e.getClass().getName()\n+      //+\" (exception message suppressed)\";\n+      + \", \"\n+      + e.getMessage();\n+\n+    hdLogger.log(Level.SEVERE, errMessage);\n+\n+    // temporary:\n+    e.printStackTrace();\n+  }\n+\n+\n+  // TODO: I doubt we need a full stacktrace in the harvest log - ??\n+  // -- L.A. 4.5 May 2016\n+  private void logException(Throwable e, Logger logger) {\n+\n+    boolean cause = false;\n+    String fullMessage = \"\";\n+    do {\n+      String message = e.getClass().getName() + \" \" + e.getMessage();\n+      if (cause) {\n+        message = \"\\nCaused By Exception.................... \" + e.getClass().getName() + \" \" + e.getMessage();\n+      }\n+      StackTraceElement[] ste = e.getStackTrace();\n+      message += \"\\nStackTrace: \\n\";\n+      for (int m = 0; m < ste.length; m++) {\n+        message += ste[m].toString() + \"\\n\";\n+      }\n+      fullMessage += message;\n+      cause = true;\n+    } while ((e = e.getCause()) != null);\n+    logger.severe(fullMessage);\n+  }\n     \n     /*\n      some dead code below: \n@@ -465,15 +494,15 @@ public class HarvesterServiceBean {\n         return serviceProvider;\n     }\n     */\n-    \n-    /**\n-     * Creates an XOAI parameters object for the ListIdentifiers call\n-     *\n-     * @param metadataPrefix\n-     * @param set\n-     * @param from\n-     * @return ListIdentifiersParameters\n-     */\n+\n+  /**\n+   * Creates an XOAI parameters object for the ListIdentifiers call\n+   *\n+   * @param metadataPrefix\n+   * @param set\n+   * @param from\n+   * @return ListIdentifiersParameters\n+   */\n     /*\n     private ListIdentifiersParameters buildParams(String metadataPrefix, String set, Date from) {\n         ListIdentifiersParameters mip = ListIdentifiersParameters.request();\n@@ -489,6 +518,6 @@ public class HarvesterServiceBean {\n         return mip;\n     }\n     */\n-    \n-    \n+\n+\n }\n",
            "diff_size": 578
        },
        {
            "tool": "naturalize",
            "errors": null,
            "diff": null
        },
        {
            "tool": "codebuff",
            "errors": null,
            "diff": null
        },
        {
            "tool": "styler_random",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/16/HarvesterServiceBean.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_random/16/HarvesterServiceBean.java\nindex 71cc23e242b..5818d00717b 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/16/HarvesterServiceBean.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_random/16/HarvesterServiceBean.java\n@@ -359,7 +359,8 @@ public class HarvesterServiceBean {\n             if (tempFile != null) {\n                 // temporary - let's not delete the temp metadata file if anything went wrong, for now:\n                 if (errMessage == null) {\n-                    try{tempFile.delete();}catch(Throwable t){};\n+                    try{\n+                      tempFile.delete();}catch(Throwable t){};\n                 }\n             }\n         }\n",
            "diff_size": 2
        },
        {
            "tool": "styler_three_grams",
            "errors": [],
            "diff": "diff --git a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/16/HarvesterServiceBean.java b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_three_grams/16/HarvesterServiceBean.java\nindex 71cc23e242b..6c21e1b324d 100644\n--- a/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/errored/1/16/HarvesterServiceBean.java\n+++ b/home/fernanda/mnt/fernanda/git-styler/styler/python/experiments/results/IQSS-dataverse/styler_three_grams/16/HarvesterServiceBean.java\n@@ -359,7 +359,8 @@ public class HarvesterServiceBean {\n             if (tempFile != null) {\n                 // temporary - let's not delete the temp metadata file if anything went wrong, for now:\n                 if (errMessage == null) {\n-                    try{tempFile.delete();}catch(Throwable t){};\n+                    try {\n+                        tempFile.delete();}catch(Throwable t){};\n                 }\n             }\n         }\n",
            "diff_size": 2
        }
    ],
    "repaired_by": [
        "styler",
        "intellij",
        "styler_random",
        "styler_three_grams"
    ],
    "not_repaired_by": [
        "naturalize",
        "codebuff"
    ]
}